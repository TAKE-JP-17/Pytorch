{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxPjuXnMPxP9mviaE5GlVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAKE-JP-17/Pytorch/blob/main/1DHeat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IGCVS5so1fiP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pdsF0QCa1g5s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We consider Net as our solution u_theta(x,t)\n",
        "\n",
        "\"\"\"\n",
        "When forming the network, we have to keep in mind the number of inputs and outputs\n",
        "In ur case: #inputs = 2 (x,t)\n",
        "and #outputs = 1\n",
        "\n",
        "You can add ass many hidden layers as you want with as many neurons.\n",
        "More complex the network, the more prepared it is to find complex solutions, but it also requires more data.\n",
        "\n",
        "Let us create this network:\n",
        "min 5 hidden layer with 5 neurons each.\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_layer1 = nn.Linear(2,5)\n",
        "        self.hidden_layer2 = nn.Linear(5,5)\n",
        "        self.hidden_layer3 = nn.Linear(5,5)\n",
        "        self.hidden_layer4 = nn.Linear(5,5)\n",
        "        self.hidden_layer5 = nn.Linear(5,5)\n",
        "        self.output_layer = nn.Linear(5,1)\n",
        "\n",
        "    def forward(self, x,t):\n",
        "        inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
        "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
        "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
        "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
        "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
        "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
        "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
        "        return output"
      ],
      "metadata": {
        "id": "02XTD6W11g42"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (2) Model\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
        "optimizer = torch.optim.Adam(net.parameters())"
      ],
      "metadata": {
        "id": "Pjr3SrdO1mVV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PDE as loss function. Thus would use the network which we call as u_theta\n",
        "def f(x,t, net):\n",
        "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
        "    ## Based on our f = du/dx - 2du/dt - u, we need du/dx and du/dt\n",
        "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
        "    pde = u_x - 2*u_t - u\n",
        "    return pde"
      ],
      "metadata": {
        "id": "2dVC4p2H1rre"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data from Boundary Conditions\n",
        "# u(x,0)=6e^(-3x)\n",
        "## BC just gives us datapoints for training\n",
        "\n",
        "# BC tells us that for any x in range[0,2] and time=0, the value of u is given by 6e^(-3x)\n",
        "# Take say 500 random numbers of x\n",
        "x_bc = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "t_bc = np.zeros((500,1))\n",
        "# compute u based on BC\n",
        "u_bc = 6*np.exp(-3*x_bc)"
      ],
      "metadata": {
        "id": "TBBxRIP81tlE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (3) Training / Fitting\n",
        "iterations = 20000\n",
        "previous_validation_loss = 99999999.0\n",
        "for epoch in range(iterations):\n",
        "    optimizer.zero_grad() # to make the gradients zero\n",
        "\n",
        "    # Loss based on boundary conditions\n",
        "    pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
        "    pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
        "    pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
        "\n",
        "    net_bc_out = net(pt_x_bc, pt_t_bc) # output of u(x,t)\n",
        "    mse_u = mse_cost_function(net_bc_out, pt_u_bc)\n",
        "\n",
        "    # Loss based on PDE\n",
        "    x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
        "    all_zeros = np.zeros((500,1))\n",
        "\n",
        "\n",
        "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
        "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
        "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
        "\n",
        "    f_out = f(pt_x_collocation, pt_t_collocation, net) # output of f(x,t)\n",
        "    mse_f = mse_cost_function(f_out, pt_all_zeros)\n",
        "\n",
        "    # Combining the loss functions\n",
        "    loss = mse_u + mse_f\n",
        "\n",
        "\n",
        "    loss.backward() # This is for computing gradients using backward propagation\n",
        "    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
        "\n",
        "    with torch.autograd.no_grad():\n",
        "    \tprint(epoch,\"Traning Loss:\",loss.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBjNbHtw1uKD",
        "outputId": "85e796e5-be48-4764-b4f3-97a875d3bdeb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "15000 Traning Loss: tensor(9.8117e-05)\n",
            "15001 Traning Loss: tensor(0.0001)\n",
            "15002 Traning Loss: tensor(0.0001)\n",
            "15003 Traning Loss: tensor(0.0002)\n",
            "15004 Traning Loss: tensor(0.0001)\n",
            "15005 Traning Loss: tensor(0.0001)\n",
            "15006 Traning Loss: tensor(9.6871e-05)\n",
            "15007 Traning Loss: tensor(9.5534e-05)\n",
            "15008 Traning Loss: tensor(0.0001)\n",
            "15009 Traning Loss: tensor(0.0001)\n",
            "15010 Traning Loss: tensor(0.0001)\n",
            "15011 Traning Loss: tensor(9.4258e-05)\n",
            "15012 Traning Loss: tensor(9.4452e-05)\n",
            "15013 Traning Loss: tensor(9.8516e-05)\n",
            "15014 Traning Loss: tensor(9.9895e-05)\n",
            "15015 Traning Loss: tensor(0.0002)\n",
            "15016 Traning Loss: tensor(0.0001)\n",
            "15017 Traning Loss: tensor(0.0002)\n",
            "15018 Traning Loss: tensor(0.0001)\n",
            "15019 Traning Loss: tensor(0.0001)\n",
            "15020 Traning Loss: tensor(0.0001)\n",
            "15021 Traning Loss: tensor(0.0001)\n",
            "15022 Traning Loss: tensor(9.4135e-05)\n",
            "15023 Traning Loss: tensor(0.0001)\n",
            "15024 Traning Loss: tensor(9.6196e-05)\n",
            "15025 Traning Loss: tensor(0.0001)\n",
            "15026 Traning Loss: tensor(0.0001)\n",
            "15027 Traning Loss: tensor(9.9863e-05)\n",
            "15028 Traning Loss: tensor(0.0001)\n",
            "15029 Traning Loss: tensor(9.3965e-05)\n",
            "15030 Traning Loss: tensor(0.0001)\n",
            "15031 Traning Loss: tensor(0.0001)\n",
            "15032 Traning Loss: tensor(0.0002)\n",
            "15033 Traning Loss: tensor(0.0001)\n",
            "15034 Traning Loss: tensor(0.0001)\n",
            "15035 Traning Loss: tensor(0.0001)\n",
            "15036 Traning Loss: tensor(0.0001)\n",
            "15037 Traning Loss: tensor(0.0001)\n",
            "15038 Traning Loss: tensor(0.0001)\n",
            "15039 Traning Loss: tensor(0.0001)\n",
            "15040 Traning Loss: tensor(0.0002)\n",
            "15041 Traning Loss: tensor(0.0001)\n",
            "15042 Traning Loss: tensor(9.2730e-05)\n",
            "15043 Traning Loss: tensor(0.0001)\n",
            "15044 Traning Loss: tensor(0.0001)\n",
            "15045 Traning Loss: tensor(0.0001)\n",
            "15046 Traning Loss: tensor(9.8528e-05)\n",
            "15047 Traning Loss: tensor(0.0001)\n",
            "15048 Traning Loss: tensor(9.5625e-05)\n",
            "15049 Traning Loss: tensor(0.0001)\n",
            "15050 Traning Loss: tensor(0.0001)\n",
            "15051 Traning Loss: tensor(0.0001)\n",
            "15052 Traning Loss: tensor(0.0001)\n",
            "15053 Traning Loss: tensor(0.0001)\n",
            "15054 Traning Loss: tensor(0.0001)\n",
            "15055 Traning Loss: tensor(0.0001)\n",
            "15056 Traning Loss: tensor(9.3808e-05)\n",
            "15057 Traning Loss: tensor(0.0001)\n",
            "15058 Traning Loss: tensor(9.7978e-05)\n",
            "15059 Traning Loss: tensor(0.0001)\n",
            "15060 Traning Loss: tensor(0.0002)\n",
            "15061 Traning Loss: tensor(0.0001)\n",
            "15062 Traning Loss: tensor(0.0001)\n",
            "15063 Traning Loss: tensor(0.0001)\n",
            "15064 Traning Loss: tensor(0.0001)\n",
            "15065 Traning Loss: tensor(0.0002)\n",
            "15066 Traning Loss: tensor(9.4942e-05)\n",
            "15067 Traning Loss: tensor(0.0001)\n",
            "15068 Traning Loss: tensor(0.0001)\n",
            "15069 Traning Loss: tensor(9.9392e-05)\n",
            "15070 Traning Loss: tensor(0.0002)\n",
            "15071 Traning Loss: tensor(0.0001)\n",
            "15072 Traning Loss: tensor(9.4957e-05)\n",
            "15073 Traning Loss: tensor(0.0001)\n",
            "15074 Traning Loss: tensor(9.7310e-05)\n",
            "15075 Traning Loss: tensor(0.0001)\n",
            "15076 Traning Loss: tensor(0.0001)\n",
            "15077 Traning Loss: tensor(0.0001)\n",
            "15078 Traning Loss: tensor(0.0001)\n",
            "15079 Traning Loss: tensor(0.0001)\n",
            "15080 Traning Loss: tensor(0.0001)\n",
            "15081 Traning Loss: tensor(0.0001)\n",
            "15082 Traning Loss: tensor(9.6021e-05)\n",
            "15083 Traning Loss: tensor(0.0001)\n",
            "15084 Traning Loss: tensor(9.3528e-05)\n",
            "15085 Traning Loss: tensor(9.7709e-05)\n",
            "15086 Traning Loss: tensor(9.4699e-05)\n",
            "15087 Traning Loss: tensor(9.7225e-05)\n",
            "15088 Traning Loss: tensor(0.0001)\n",
            "15089 Traning Loss: tensor(0.0001)\n",
            "15090 Traning Loss: tensor(0.0001)\n",
            "15091 Traning Loss: tensor(9.9779e-05)\n",
            "15092 Traning Loss: tensor(9.5298e-05)\n",
            "15093 Traning Loss: tensor(8.9899e-05)\n",
            "15094 Traning Loss: tensor(0.0001)\n",
            "15095 Traning Loss: tensor(9.8862e-05)\n",
            "15096 Traning Loss: tensor(9.6975e-05)\n",
            "15097 Traning Loss: tensor(8.9647e-05)\n",
            "15098 Traning Loss: tensor(9.4844e-05)\n",
            "15099 Traning Loss: tensor(9.5375e-05)\n",
            "15100 Traning Loss: tensor(8.9923e-05)\n",
            "15101 Traning Loss: tensor(9.7559e-05)\n",
            "15102 Traning Loss: tensor(0.0001)\n",
            "15103 Traning Loss: tensor(9.8969e-05)\n",
            "15104 Traning Loss: tensor(9.0043e-05)\n",
            "15105 Traning Loss: tensor(9.2766e-05)\n",
            "15106 Traning Loss: tensor(0.0001)\n",
            "15107 Traning Loss: tensor(0.0001)\n",
            "15108 Traning Loss: tensor(9.5477e-05)\n",
            "15109 Traning Loss: tensor(0.0001)\n",
            "15110 Traning Loss: tensor(8.9511e-05)\n",
            "15111 Traning Loss: tensor(0.0002)\n",
            "15112 Traning Loss: tensor(8.6893e-05)\n",
            "15113 Traning Loss: tensor(0.0001)\n",
            "15114 Traning Loss: tensor(9.9210e-05)\n",
            "15115 Traning Loss: tensor(9.3718e-05)\n",
            "15116 Traning Loss: tensor(8.9768e-05)\n",
            "15117 Traning Loss: tensor(0.0001)\n",
            "15118 Traning Loss: tensor(9.2365e-05)\n",
            "15119 Traning Loss: tensor(0.0001)\n",
            "15120 Traning Loss: tensor(0.0002)\n",
            "15121 Traning Loss: tensor(0.0001)\n",
            "15122 Traning Loss: tensor(0.0001)\n",
            "15123 Traning Loss: tensor(9.4181e-05)\n",
            "15124 Traning Loss: tensor(9.7853e-05)\n",
            "15125 Traning Loss: tensor(0.0001)\n",
            "15126 Traning Loss: tensor(0.0001)\n",
            "15127 Traning Loss: tensor(9.2432e-05)\n",
            "15128 Traning Loss: tensor(9.8590e-05)\n",
            "15129 Traning Loss: tensor(9.5452e-05)\n",
            "15130 Traning Loss: tensor(0.0001)\n",
            "15131 Traning Loss: tensor(9.2829e-05)\n",
            "15132 Traning Loss: tensor(8.7357e-05)\n",
            "15133 Traning Loss: tensor(9.6789e-05)\n",
            "15134 Traning Loss: tensor(9.0629e-05)\n",
            "15135 Traning Loss: tensor(0.0001)\n",
            "15136 Traning Loss: tensor(0.0001)\n",
            "15137 Traning Loss: tensor(9.5691e-05)\n",
            "15138 Traning Loss: tensor(9.3112e-05)\n",
            "15139 Traning Loss: tensor(9.2214e-05)\n",
            "15140 Traning Loss: tensor(9.2111e-05)\n",
            "15141 Traning Loss: tensor(9.4216e-05)\n",
            "15142 Traning Loss: tensor(0.0001)\n",
            "15143 Traning Loss: tensor(0.0002)\n",
            "15144 Traning Loss: tensor(0.0001)\n",
            "15145 Traning Loss: tensor(9.2101e-05)\n",
            "15146 Traning Loss: tensor(9.5525e-05)\n",
            "15147 Traning Loss: tensor(9.4162e-05)\n",
            "15148 Traning Loss: tensor(9.6836e-05)\n",
            "15149 Traning Loss: tensor(0.0003)\n",
            "15150 Traning Loss: tensor(9.8394e-05)\n",
            "15151 Traning Loss: tensor(0.0001)\n",
            "15152 Traning Loss: tensor(0.0001)\n",
            "15153 Traning Loss: tensor(0.0001)\n",
            "15154 Traning Loss: tensor(0.0002)\n",
            "15155 Traning Loss: tensor(9.3929e-05)\n",
            "15156 Traning Loss: tensor(0.0001)\n",
            "15157 Traning Loss: tensor(0.0002)\n",
            "15158 Traning Loss: tensor(0.0002)\n",
            "15159 Traning Loss: tensor(0.0001)\n",
            "15160 Traning Loss: tensor(9.5759e-05)\n",
            "15161 Traning Loss: tensor(9.6753e-05)\n",
            "15162 Traning Loss: tensor(9.5313e-05)\n",
            "15163 Traning Loss: tensor(0.0001)\n",
            "15164 Traning Loss: tensor(0.0001)\n",
            "15165 Traning Loss: tensor(9.7815e-05)\n",
            "15166 Traning Loss: tensor(9.6346e-05)\n",
            "15167 Traning Loss: tensor(9.0950e-05)\n",
            "15168 Traning Loss: tensor(0.0002)\n",
            "15169 Traning Loss: tensor(9.2013e-05)\n",
            "15170 Traning Loss: tensor(9.9871e-05)\n",
            "15171 Traning Loss: tensor(0.0001)\n",
            "15172 Traning Loss: tensor(0.0001)\n",
            "15173 Traning Loss: tensor(8.9242e-05)\n",
            "15174 Traning Loss: tensor(9.8228e-05)\n",
            "15175 Traning Loss: tensor(0.0001)\n",
            "15176 Traning Loss: tensor(9.5509e-05)\n",
            "15177 Traning Loss: tensor(9.7963e-05)\n",
            "15178 Traning Loss: tensor(0.0001)\n",
            "15179 Traning Loss: tensor(0.0001)\n",
            "15180 Traning Loss: tensor(9.6533e-05)\n",
            "15181 Traning Loss: tensor(9.3032e-05)\n",
            "15182 Traning Loss: tensor(0.0002)\n",
            "15183 Traning Loss: tensor(0.0002)\n",
            "15184 Traning Loss: tensor(0.0001)\n",
            "15185 Traning Loss: tensor(8.9803e-05)\n",
            "15186 Traning Loss: tensor(0.0001)\n",
            "15187 Traning Loss: tensor(0.0001)\n",
            "15188 Traning Loss: tensor(0.0001)\n",
            "15189 Traning Loss: tensor(0.0001)\n",
            "15190 Traning Loss: tensor(0.0002)\n",
            "15191 Traning Loss: tensor(0.0002)\n",
            "15192 Traning Loss: tensor(0.0003)\n",
            "15193 Traning Loss: tensor(0.0001)\n",
            "15194 Traning Loss: tensor(0.0001)\n",
            "15195 Traning Loss: tensor(0.0001)\n",
            "15196 Traning Loss: tensor(0.0002)\n",
            "15197 Traning Loss: tensor(0.0002)\n",
            "15198 Traning Loss: tensor(0.0001)\n",
            "15199 Traning Loss: tensor(0.0001)\n",
            "15200 Traning Loss: tensor(9.9878e-05)\n",
            "15201 Traning Loss: tensor(0.0001)\n",
            "15202 Traning Loss: tensor(0.0001)\n",
            "15203 Traning Loss: tensor(0.0001)\n",
            "15204 Traning Loss: tensor(9.8721e-05)\n",
            "15205 Traning Loss: tensor(9.1478e-05)\n",
            "15206 Traning Loss: tensor(9.8643e-05)\n",
            "15207 Traning Loss: tensor(0.0002)\n",
            "15208 Traning Loss: tensor(9.6888e-05)\n",
            "15209 Traning Loss: tensor(9.5991e-05)\n",
            "15210 Traning Loss: tensor(9.4754e-05)\n",
            "15211 Traning Loss: tensor(9.2309e-05)\n",
            "15212 Traning Loss: tensor(0.0001)\n",
            "15213 Traning Loss: tensor(0.0001)\n",
            "15214 Traning Loss: tensor(8.5202e-05)\n",
            "15215 Traning Loss: tensor(9.7117e-05)\n",
            "15216 Traning Loss: tensor(8.7227e-05)\n",
            "15217 Traning Loss: tensor(0.0001)\n",
            "15218 Traning Loss: tensor(9.9226e-05)\n",
            "15219 Traning Loss: tensor(9.4282e-05)\n",
            "15220 Traning Loss: tensor(8.7050e-05)\n",
            "15221 Traning Loss: tensor(0.0002)\n",
            "15222 Traning Loss: tensor(9.7988e-05)\n",
            "15223 Traning Loss: tensor(9.9545e-05)\n",
            "15224 Traning Loss: tensor(9.1217e-05)\n",
            "15225 Traning Loss: tensor(0.0001)\n",
            "15226 Traning Loss: tensor(0.0001)\n",
            "15227 Traning Loss: tensor(9.5658e-05)\n",
            "15228 Traning Loss: tensor(0.0001)\n",
            "15229 Traning Loss: tensor(9.2238e-05)\n",
            "15230 Traning Loss: tensor(9.3282e-05)\n",
            "15231 Traning Loss: tensor(0.0001)\n",
            "15232 Traning Loss: tensor(0.0001)\n",
            "15233 Traning Loss: tensor(8.6334e-05)\n",
            "15234 Traning Loss: tensor(0.0002)\n",
            "15235 Traning Loss: tensor(9.3223e-05)\n",
            "15236 Traning Loss: tensor(8.7770e-05)\n",
            "15237 Traning Loss: tensor(0.0001)\n",
            "15238 Traning Loss: tensor(9.3469e-05)\n",
            "15239 Traning Loss: tensor(8.7983e-05)\n",
            "15240 Traning Loss: tensor(0.0001)\n",
            "15241 Traning Loss: tensor(9.0070e-05)\n",
            "15242 Traning Loss: tensor(9.3039e-05)\n",
            "15243 Traning Loss: tensor(9.2294e-05)\n",
            "15244 Traning Loss: tensor(8.9840e-05)\n",
            "15245 Traning Loss: tensor(9.3899e-05)\n",
            "15246 Traning Loss: tensor(9.8218e-05)\n",
            "15247 Traning Loss: tensor(8.9609e-05)\n",
            "15248 Traning Loss: tensor(9.4086e-05)\n",
            "15249 Traning Loss: tensor(8.6531e-05)\n",
            "15250 Traning Loss: tensor(9.1815e-05)\n",
            "15251 Traning Loss: tensor(8.6856e-05)\n",
            "15252 Traning Loss: tensor(8.6806e-05)\n",
            "15253 Traning Loss: tensor(9.8090e-05)\n",
            "15254 Traning Loss: tensor(8.7004e-05)\n",
            "15255 Traning Loss: tensor(8.7473e-05)\n",
            "15256 Traning Loss: tensor(8.5427e-05)\n",
            "15257 Traning Loss: tensor(8.4269e-05)\n",
            "15258 Traning Loss: tensor(8.8727e-05)\n",
            "15259 Traning Loss: tensor(0.0001)\n",
            "15260 Traning Loss: tensor(9.0622e-05)\n",
            "15261 Traning Loss: tensor(8.9323e-05)\n",
            "15262 Traning Loss: tensor(8.7055e-05)\n",
            "15263 Traning Loss: tensor(9.1270e-05)\n",
            "15264 Traning Loss: tensor(9.5514e-05)\n",
            "15265 Traning Loss: tensor(9.1207e-05)\n",
            "15266 Traning Loss: tensor(9.3183e-05)\n",
            "15267 Traning Loss: tensor(8.8525e-05)\n",
            "15268 Traning Loss: tensor(8.8729e-05)\n",
            "15269 Traning Loss: tensor(9.5068e-05)\n",
            "15270 Traning Loss: tensor(8.9583e-05)\n",
            "15271 Traning Loss: tensor(8.8931e-05)\n",
            "15272 Traning Loss: tensor(7.6265e-05)\n",
            "15273 Traning Loss: tensor(8.4957e-05)\n",
            "15274 Traning Loss: tensor(9.9890e-05)\n",
            "15275 Traning Loss: tensor(8.7766e-05)\n",
            "15276 Traning Loss: tensor(9.2217e-05)\n",
            "15277 Traning Loss: tensor(8.8283e-05)\n",
            "15278 Traning Loss: tensor(9.5404e-05)\n",
            "15279 Traning Loss: tensor(8.8294e-05)\n",
            "15280 Traning Loss: tensor(9.4775e-05)\n",
            "15281 Traning Loss: tensor(8.9963e-05)\n",
            "15282 Traning Loss: tensor(8.6891e-05)\n",
            "15283 Traning Loss: tensor(8.9223e-05)\n",
            "15284 Traning Loss: tensor(8.9900e-05)\n",
            "15285 Traning Loss: tensor(0.0002)\n",
            "15286 Traning Loss: tensor(0.0001)\n",
            "15287 Traning Loss: tensor(9.3321e-05)\n",
            "15288 Traning Loss: tensor(9.2448e-05)\n",
            "15289 Traning Loss: tensor(9.3168e-05)\n",
            "15290 Traning Loss: tensor(0.0001)\n",
            "15291 Traning Loss: tensor(9.7063e-05)\n",
            "15292 Traning Loss: tensor(7.8246e-05)\n",
            "15293 Traning Loss: tensor(0.0001)\n",
            "15294 Traning Loss: tensor(8.8563e-05)\n",
            "15295 Traning Loss: tensor(8.7287e-05)\n",
            "15296 Traning Loss: tensor(8.9380e-05)\n",
            "15297 Traning Loss: tensor(0.0001)\n",
            "15298 Traning Loss: tensor(9.0935e-05)\n",
            "15299 Traning Loss: tensor(8.1332e-05)\n",
            "15300 Traning Loss: tensor(8.8695e-05)\n",
            "15301 Traning Loss: tensor(9.8130e-05)\n",
            "15302 Traning Loss: tensor(8.6159e-05)\n",
            "15303 Traning Loss: tensor(0.0001)\n",
            "15304 Traning Loss: tensor(9.4814e-05)\n",
            "15305 Traning Loss: tensor(0.0001)\n",
            "15306 Traning Loss: tensor(0.0002)\n",
            "15307 Traning Loss: tensor(0.0002)\n",
            "15308 Traning Loss: tensor(0.0002)\n",
            "15309 Traning Loss: tensor(9.1952e-05)\n",
            "15310 Traning Loss: tensor(9.2980e-05)\n",
            "15311 Traning Loss: tensor(0.0001)\n",
            "15312 Traning Loss: tensor(0.0001)\n",
            "15313 Traning Loss: tensor(8.6136e-05)\n",
            "15314 Traning Loss: tensor(9.1989e-05)\n",
            "15315 Traning Loss: tensor(9.9119e-05)\n",
            "15316 Traning Loss: tensor(9.0350e-05)\n",
            "15317 Traning Loss: tensor(9.2221e-05)\n",
            "15318 Traning Loss: tensor(9.0437e-05)\n",
            "15319 Traning Loss: tensor(8.9837e-05)\n",
            "15320 Traning Loss: tensor(8.6927e-05)\n",
            "15321 Traning Loss: tensor(9.0474e-05)\n",
            "15322 Traning Loss: tensor(8.9267e-05)\n",
            "15323 Traning Loss: tensor(9.7914e-05)\n",
            "15324 Traning Loss: tensor(9.6714e-05)\n",
            "15325 Traning Loss: tensor(0.0001)\n",
            "15326 Traning Loss: tensor(9.1250e-05)\n",
            "15327 Traning Loss: tensor(9.7863e-05)\n",
            "15328 Traning Loss: tensor(0.0001)\n",
            "15329 Traning Loss: tensor(9.6817e-05)\n",
            "15330 Traning Loss: tensor(8.9486e-05)\n",
            "15331 Traning Loss: tensor(8.0662e-05)\n",
            "15332 Traning Loss: tensor(0.0001)\n",
            "15333 Traning Loss: tensor(8.9973e-05)\n",
            "15334 Traning Loss: tensor(9.1864e-05)\n",
            "15335 Traning Loss: tensor(8.6452e-05)\n",
            "15336 Traning Loss: tensor(8.4182e-05)\n",
            "15337 Traning Loss: tensor(8.3592e-05)\n",
            "15338 Traning Loss: tensor(9.2079e-05)\n",
            "15339 Traning Loss: tensor(8.3746e-05)\n",
            "15340 Traning Loss: tensor(8.7753e-05)\n",
            "15341 Traning Loss: tensor(0.0001)\n",
            "15342 Traning Loss: tensor(9.5871e-05)\n",
            "15343 Traning Loss: tensor(9.5084e-05)\n",
            "15344 Traning Loss: tensor(8.8426e-05)\n",
            "15345 Traning Loss: tensor(9.1167e-05)\n",
            "15346 Traning Loss: tensor(7.9183e-05)\n",
            "15347 Traning Loss: tensor(8.4374e-05)\n",
            "15348 Traning Loss: tensor(9.0524e-05)\n",
            "15349 Traning Loss: tensor(8.7788e-05)\n",
            "15350 Traning Loss: tensor(8.9025e-05)\n",
            "15351 Traning Loss: tensor(9.4320e-05)\n",
            "15352 Traning Loss: tensor(7.9596e-05)\n",
            "15353 Traning Loss: tensor(8.8953e-05)\n",
            "15354 Traning Loss: tensor(8.8161e-05)\n",
            "15355 Traning Loss: tensor(8.8667e-05)\n",
            "15356 Traning Loss: tensor(0.0002)\n",
            "15357 Traning Loss: tensor(8.9883e-05)\n",
            "15358 Traning Loss: tensor(8.1738e-05)\n",
            "15359 Traning Loss: tensor(9.3022e-05)\n",
            "15360 Traning Loss: tensor(8.6066e-05)\n",
            "15361 Traning Loss: tensor(9.0996e-05)\n",
            "15362 Traning Loss: tensor(0.0001)\n",
            "15363 Traning Loss: tensor(8.9044e-05)\n",
            "15364 Traning Loss: tensor(8.4341e-05)\n",
            "15365 Traning Loss: tensor(9.0591e-05)\n",
            "15366 Traning Loss: tensor(8.5042e-05)\n",
            "15367 Traning Loss: tensor(8.5457e-05)\n",
            "15368 Traning Loss: tensor(9.5207e-05)\n",
            "15369 Traning Loss: tensor(8.6434e-05)\n",
            "15370 Traning Loss: tensor(8.9252e-05)\n",
            "15371 Traning Loss: tensor(8.8392e-05)\n",
            "15372 Traning Loss: tensor(8.3160e-05)\n",
            "15373 Traning Loss: tensor(8.4914e-05)\n",
            "15374 Traning Loss: tensor(0.0001)\n",
            "15375 Traning Loss: tensor(9.0023e-05)\n",
            "15376 Traning Loss: tensor(0.0002)\n",
            "15377 Traning Loss: tensor(0.0002)\n",
            "15378 Traning Loss: tensor(8.5222e-05)\n",
            "15379 Traning Loss: tensor(9.1274e-05)\n",
            "15380 Traning Loss: tensor(0.0001)\n",
            "15381 Traning Loss: tensor(9.1132e-05)\n",
            "15382 Traning Loss: tensor(9.7699e-05)\n",
            "15383 Traning Loss: tensor(0.0002)\n",
            "15384 Traning Loss: tensor(9.1500e-05)\n",
            "15385 Traning Loss: tensor(8.3616e-05)\n",
            "15386 Traning Loss: tensor(8.9900e-05)\n",
            "15387 Traning Loss: tensor(8.5239e-05)\n",
            "15388 Traning Loss: tensor(9.3523e-05)\n",
            "15389 Traning Loss: tensor(9.1003e-05)\n",
            "15390 Traning Loss: tensor(8.6965e-05)\n",
            "15391 Traning Loss: tensor(9.6443e-05)\n",
            "15392 Traning Loss: tensor(0.0001)\n",
            "15393 Traning Loss: tensor(0.0001)\n",
            "15394 Traning Loss: tensor(0.0002)\n",
            "15395 Traning Loss: tensor(9.7566e-05)\n",
            "15396 Traning Loss: tensor(0.0001)\n",
            "15397 Traning Loss: tensor(0.0001)\n",
            "15398 Traning Loss: tensor(9.0476e-05)\n",
            "15399 Traning Loss: tensor(8.3747e-05)\n",
            "15400 Traning Loss: tensor(0.0001)\n",
            "15401 Traning Loss: tensor(0.0001)\n",
            "15402 Traning Loss: tensor(8.6959e-05)\n",
            "15403 Traning Loss: tensor(9.8745e-05)\n",
            "15404 Traning Loss: tensor(0.0001)\n",
            "15405 Traning Loss: tensor(9.7489e-05)\n",
            "15406 Traning Loss: tensor(9.0774e-05)\n",
            "15407 Traning Loss: tensor(8.8137e-05)\n",
            "15408 Traning Loss: tensor(8.2436e-05)\n",
            "15409 Traning Loss: tensor(9.4677e-05)\n",
            "15410 Traning Loss: tensor(8.8520e-05)\n",
            "15411 Traning Loss: tensor(9.0809e-05)\n",
            "15412 Traning Loss: tensor(9.3588e-05)\n",
            "15413 Traning Loss: tensor(8.4054e-05)\n",
            "15414 Traning Loss: tensor(9.3177e-05)\n",
            "15415 Traning Loss: tensor(8.7454e-05)\n",
            "15416 Traning Loss: tensor(8.3370e-05)\n",
            "15417 Traning Loss: tensor(8.7700e-05)\n",
            "15418 Traning Loss: tensor(0.0001)\n",
            "15419 Traning Loss: tensor(8.3658e-05)\n",
            "15420 Traning Loss: tensor(9.6881e-05)\n",
            "15421 Traning Loss: tensor(8.1149e-05)\n",
            "15422 Traning Loss: tensor(8.1569e-05)\n",
            "15423 Traning Loss: tensor(0.0002)\n",
            "15424 Traning Loss: tensor(8.4802e-05)\n",
            "15425 Traning Loss: tensor(0.0002)\n",
            "15426 Traning Loss: tensor(8.8218e-05)\n",
            "15427 Traning Loss: tensor(0.0001)\n",
            "15428 Traning Loss: tensor(9.1917e-05)\n",
            "15429 Traning Loss: tensor(9.3403e-05)\n",
            "15430 Traning Loss: tensor(9.1910e-05)\n",
            "15431 Traning Loss: tensor(0.0002)\n",
            "15432 Traning Loss: tensor(8.0857e-05)\n",
            "15433 Traning Loss: tensor(9.3735e-05)\n",
            "15434 Traning Loss: tensor(9.4469e-05)\n",
            "15435 Traning Loss: tensor(8.2877e-05)\n",
            "15436 Traning Loss: tensor(9.1734e-05)\n",
            "15437 Traning Loss: tensor(9.0210e-05)\n",
            "15438 Traning Loss: tensor(9.5075e-05)\n",
            "15439 Traning Loss: tensor(9.2221e-05)\n",
            "15440 Traning Loss: tensor(0.0001)\n",
            "15441 Traning Loss: tensor(9.1338e-05)\n",
            "15442 Traning Loss: tensor(9.2992e-05)\n",
            "15443 Traning Loss: tensor(9.3365e-05)\n",
            "15444 Traning Loss: tensor(9.1912e-05)\n",
            "15445 Traning Loss: tensor(9.5963e-05)\n",
            "15446 Traning Loss: tensor(9.5503e-05)\n",
            "15447 Traning Loss: tensor(8.9874e-05)\n",
            "15448 Traning Loss: tensor(0.0001)\n",
            "15449 Traning Loss: tensor(8.5362e-05)\n",
            "15450 Traning Loss: tensor(0.0001)\n",
            "15451 Traning Loss: tensor(9.0147e-05)\n",
            "15452 Traning Loss: tensor(8.1882e-05)\n",
            "15453 Traning Loss: tensor(8.5511e-05)\n",
            "15454 Traning Loss: tensor(9.0478e-05)\n",
            "15455 Traning Loss: tensor(9.9769e-05)\n",
            "15456 Traning Loss: tensor(8.2052e-05)\n",
            "15457 Traning Loss: tensor(8.7431e-05)\n",
            "15458 Traning Loss: tensor(8.0078e-05)\n",
            "15459 Traning Loss: tensor(8.4246e-05)\n",
            "15460 Traning Loss: tensor(8.6043e-05)\n",
            "15461 Traning Loss: tensor(8.5116e-05)\n",
            "15462 Traning Loss: tensor(8.1511e-05)\n",
            "15463 Traning Loss: tensor(9.1681e-05)\n",
            "15464 Traning Loss: tensor(9.4487e-05)\n",
            "15465 Traning Loss: tensor(7.9606e-05)\n",
            "15466 Traning Loss: tensor(7.9250e-05)\n",
            "15467 Traning Loss: tensor(0.0001)\n",
            "15468 Traning Loss: tensor(7.9403e-05)\n",
            "15469 Traning Loss: tensor(0.0001)\n",
            "15470 Traning Loss: tensor(8.9013e-05)\n",
            "15471 Traning Loss: tensor(8.8923e-05)\n",
            "15472 Traning Loss: tensor(8.8312e-05)\n",
            "15473 Traning Loss: tensor(9.0238e-05)\n",
            "15474 Traning Loss: tensor(8.1912e-05)\n",
            "15475 Traning Loss: tensor(8.1514e-05)\n",
            "15476 Traning Loss: tensor(8.4051e-05)\n",
            "15477 Traning Loss: tensor(8.8996e-05)\n",
            "15478 Traning Loss: tensor(7.5567e-05)\n",
            "15479 Traning Loss: tensor(8.7634e-05)\n",
            "15480 Traning Loss: tensor(8.7328e-05)\n",
            "15481 Traning Loss: tensor(8.2420e-05)\n",
            "15482 Traning Loss: tensor(8.2242e-05)\n",
            "15483 Traning Loss: tensor(9.5230e-05)\n",
            "15484 Traning Loss: tensor(7.8246e-05)\n",
            "15485 Traning Loss: tensor(0.0002)\n",
            "15486 Traning Loss: tensor(9.7882e-05)\n",
            "15487 Traning Loss: tensor(9.5980e-05)\n",
            "15488 Traning Loss: tensor(9.2918e-05)\n",
            "15489 Traning Loss: tensor(8.8404e-05)\n",
            "15490 Traning Loss: tensor(8.5425e-05)\n",
            "15491 Traning Loss: tensor(8.1061e-05)\n",
            "15492 Traning Loss: tensor(0.0001)\n",
            "15493 Traning Loss: tensor(9.5890e-05)\n",
            "15494 Traning Loss: tensor(0.0002)\n",
            "15495 Traning Loss: tensor(8.5713e-05)\n",
            "15496 Traning Loss: tensor(9.0982e-05)\n",
            "15497 Traning Loss: tensor(8.9418e-05)\n",
            "15498 Traning Loss: tensor(8.4150e-05)\n",
            "15499 Traning Loss: tensor(0.0001)\n",
            "15500 Traning Loss: tensor(8.5471e-05)\n",
            "15501 Traning Loss: tensor(0.0001)\n",
            "15502 Traning Loss: tensor(8.0013e-05)\n",
            "15503 Traning Loss: tensor(8.0344e-05)\n",
            "15504 Traning Loss: tensor(8.4331e-05)\n",
            "15505 Traning Loss: tensor(8.0648e-05)\n",
            "15506 Traning Loss: tensor(9.2822e-05)\n",
            "15507 Traning Loss: tensor(0.0002)\n",
            "15508 Traning Loss: tensor(8.6565e-05)\n",
            "15509 Traning Loss: tensor(8.6404e-05)\n",
            "15510 Traning Loss: tensor(9.0329e-05)\n",
            "15511 Traning Loss: tensor(9.2154e-05)\n",
            "15512 Traning Loss: tensor(9.2022e-05)\n",
            "15513 Traning Loss: tensor(7.9029e-05)\n",
            "15514 Traning Loss: tensor(9.1208e-05)\n",
            "15515 Traning Loss: tensor(9.3109e-05)\n",
            "15516 Traning Loss: tensor(8.4021e-05)\n",
            "15517 Traning Loss: tensor(9.2607e-05)\n",
            "15518 Traning Loss: tensor(8.4173e-05)\n",
            "15519 Traning Loss: tensor(9.3262e-05)\n",
            "15520 Traning Loss: tensor(8.9435e-05)\n",
            "15521 Traning Loss: tensor(0.0001)\n",
            "15522 Traning Loss: tensor(8.8954e-05)\n",
            "15523 Traning Loss: tensor(8.0641e-05)\n",
            "15524 Traning Loss: tensor(8.4745e-05)\n",
            "15525 Traning Loss: tensor(0.0001)\n",
            "15526 Traning Loss: tensor(8.8798e-05)\n",
            "15527 Traning Loss: tensor(8.5406e-05)\n",
            "15528 Traning Loss: tensor(0.0002)\n",
            "15529 Traning Loss: tensor(8.0397e-05)\n",
            "15530 Traning Loss: tensor(0.0001)\n",
            "15531 Traning Loss: tensor(8.2880e-05)\n",
            "15532 Traning Loss: tensor(9.4490e-05)\n",
            "15533 Traning Loss: tensor(8.4197e-05)\n",
            "15534 Traning Loss: tensor(8.5030e-05)\n",
            "15535 Traning Loss: tensor(8.1898e-05)\n",
            "15536 Traning Loss: tensor(8.8651e-05)\n",
            "15537 Traning Loss: tensor(7.7398e-05)\n",
            "15538 Traning Loss: tensor(8.4960e-05)\n",
            "15539 Traning Loss: tensor(9.6674e-05)\n",
            "15540 Traning Loss: tensor(8.6756e-05)\n",
            "15541 Traning Loss: tensor(7.7978e-05)\n",
            "15542 Traning Loss: tensor(9.2168e-05)\n",
            "15543 Traning Loss: tensor(8.8338e-05)\n",
            "15544 Traning Loss: tensor(7.6826e-05)\n",
            "15545 Traning Loss: tensor(7.9846e-05)\n",
            "15546 Traning Loss: tensor(8.0726e-05)\n",
            "15547 Traning Loss: tensor(8.0554e-05)\n",
            "15548 Traning Loss: tensor(7.8634e-05)\n",
            "15549 Traning Loss: tensor(7.6962e-05)\n",
            "15550 Traning Loss: tensor(7.8398e-05)\n",
            "15551 Traning Loss: tensor(8.4546e-05)\n",
            "15552 Traning Loss: tensor(8.0154e-05)\n",
            "15553 Traning Loss: tensor(8.1757e-05)\n",
            "15554 Traning Loss: tensor(7.9234e-05)\n",
            "15555 Traning Loss: tensor(8.3448e-05)\n",
            "15556 Traning Loss: tensor(8.4146e-05)\n",
            "15557 Traning Loss: tensor(9.4559e-05)\n",
            "15558 Traning Loss: tensor(8.8153e-05)\n",
            "15559 Traning Loss: tensor(0.0002)\n",
            "15560 Traning Loss: tensor(0.0001)\n",
            "15561 Traning Loss: tensor(9.2985e-05)\n",
            "15562 Traning Loss: tensor(9.1628e-05)\n",
            "15563 Traning Loss: tensor(9.5656e-05)\n",
            "15564 Traning Loss: tensor(8.3121e-05)\n",
            "15565 Traning Loss: tensor(8.6272e-05)\n",
            "15566 Traning Loss: tensor(8.7928e-05)\n",
            "15567 Traning Loss: tensor(8.9906e-05)\n",
            "15568 Traning Loss: tensor(7.8737e-05)\n",
            "15569 Traning Loss: tensor(0.0001)\n",
            "15570 Traning Loss: tensor(8.2171e-05)\n",
            "15571 Traning Loss: tensor(8.0065e-05)\n",
            "15572 Traning Loss: tensor(8.6324e-05)\n",
            "15573 Traning Loss: tensor(7.7627e-05)\n",
            "15574 Traning Loss: tensor(0.0001)\n",
            "15575 Traning Loss: tensor(8.3402e-05)\n",
            "15576 Traning Loss: tensor(8.3838e-05)\n",
            "15577 Traning Loss: tensor(8.6802e-05)\n",
            "15578 Traning Loss: tensor(8.3180e-05)\n",
            "15579 Traning Loss: tensor(8.7642e-05)\n",
            "15580 Traning Loss: tensor(8.2677e-05)\n",
            "15581 Traning Loss: tensor(0.0001)\n",
            "15582 Traning Loss: tensor(8.2899e-05)\n",
            "15583 Traning Loss: tensor(8.0485e-05)\n",
            "15584 Traning Loss: tensor(8.4585e-05)\n",
            "15585 Traning Loss: tensor(9.2735e-05)\n",
            "15586 Traning Loss: tensor(7.9392e-05)\n",
            "15587 Traning Loss: tensor(8.4685e-05)\n",
            "15588 Traning Loss: tensor(0.0001)\n",
            "15589 Traning Loss: tensor(8.7906e-05)\n",
            "15590 Traning Loss: tensor(9.4851e-05)\n",
            "15591 Traning Loss: tensor(8.4518e-05)\n",
            "15592 Traning Loss: tensor(8.4285e-05)\n",
            "15593 Traning Loss: tensor(8.1966e-05)\n",
            "15594 Traning Loss: tensor(0.0001)\n",
            "15595 Traning Loss: tensor(8.4109e-05)\n",
            "15596 Traning Loss: tensor(8.9037e-05)\n",
            "15597 Traning Loss: tensor(7.6391e-05)\n",
            "15598 Traning Loss: tensor(7.2768e-05)\n",
            "15599 Traning Loss: tensor(8.0507e-05)\n",
            "15600 Traning Loss: tensor(8.1290e-05)\n",
            "15601 Traning Loss: tensor(0.0001)\n",
            "15602 Traning Loss: tensor(8.1131e-05)\n",
            "15603 Traning Loss: tensor(0.0001)\n",
            "15604 Traning Loss: tensor(0.0001)\n",
            "15605 Traning Loss: tensor(9.4310e-05)\n",
            "15606 Traning Loss: tensor(0.0001)\n",
            "15607 Traning Loss: tensor(7.8619e-05)\n",
            "15608 Traning Loss: tensor(7.9160e-05)\n",
            "15609 Traning Loss: tensor(0.0001)\n",
            "15610 Traning Loss: tensor(8.4075e-05)\n",
            "15611 Traning Loss: tensor(7.5927e-05)\n",
            "15612 Traning Loss: tensor(0.0001)\n",
            "15613 Traning Loss: tensor(8.8364e-05)\n",
            "15614 Traning Loss: tensor(8.4488e-05)\n",
            "15615 Traning Loss: tensor(8.1899e-05)\n",
            "15616 Traning Loss: tensor(7.5848e-05)\n",
            "15617 Traning Loss: tensor(8.0193e-05)\n",
            "15618 Traning Loss: tensor(8.0183e-05)\n",
            "15619 Traning Loss: tensor(8.0993e-05)\n",
            "15620 Traning Loss: tensor(8.5621e-05)\n",
            "15621 Traning Loss: tensor(7.8627e-05)\n",
            "15622 Traning Loss: tensor(0.0002)\n",
            "15623 Traning Loss: tensor(8.4444e-05)\n",
            "15624 Traning Loss: tensor(8.4540e-05)\n",
            "15625 Traning Loss: tensor(7.7323e-05)\n",
            "15626 Traning Loss: tensor(8.2713e-05)\n",
            "15627 Traning Loss: tensor(8.7158e-05)\n",
            "15628 Traning Loss: tensor(7.6605e-05)\n",
            "15629 Traning Loss: tensor(8.1851e-05)\n",
            "15630 Traning Loss: tensor(7.8198e-05)\n",
            "15631 Traning Loss: tensor(7.8523e-05)\n",
            "15632 Traning Loss: tensor(8.0779e-05)\n",
            "15633 Traning Loss: tensor(8.2516e-05)\n",
            "15634 Traning Loss: tensor(8.0018e-05)\n",
            "15635 Traning Loss: tensor(7.6279e-05)\n",
            "15636 Traning Loss: tensor(7.4058e-05)\n",
            "15637 Traning Loss: tensor(0.0002)\n",
            "15638 Traning Loss: tensor(8.7221e-05)\n",
            "15639 Traning Loss: tensor(7.5468e-05)\n",
            "15640 Traning Loss: tensor(0.0002)\n",
            "15641 Traning Loss: tensor(8.9887e-05)\n",
            "15642 Traning Loss: tensor(8.6279e-05)\n",
            "15643 Traning Loss: tensor(9.6712e-05)\n",
            "15644 Traning Loss: tensor(9.0109e-05)\n",
            "15645 Traning Loss: tensor(0.0001)\n",
            "15646 Traning Loss: tensor(8.9342e-05)\n",
            "15647 Traning Loss: tensor(8.4649e-05)\n",
            "15648 Traning Loss: tensor(7.7724e-05)\n",
            "15649 Traning Loss: tensor(9.0892e-05)\n",
            "15650 Traning Loss: tensor(8.3258e-05)\n",
            "15651 Traning Loss: tensor(7.7641e-05)\n",
            "15652 Traning Loss: tensor(9.4531e-05)\n",
            "15653 Traning Loss: tensor(7.6438e-05)\n",
            "15654 Traning Loss: tensor(8.7477e-05)\n",
            "15655 Traning Loss: tensor(9.0965e-05)\n",
            "15656 Traning Loss: tensor(8.4089e-05)\n",
            "15657 Traning Loss: tensor(8.0997e-05)\n",
            "15658 Traning Loss: tensor(8.2736e-05)\n",
            "15659 Traning Loss: tensor(7.4514e-05)\n",
            "15660 Traning Loss: tensor(7.4675e-05)\n",
            "15661 Traning Loss: tensor(0.0001)\n",
            "15662 Traning Loss: tensor(8.9284e-05)\n",
            "15663 Traning Loss: tensor(8.2839e-05)\n",
            "15664 Traning Loss: tensor(7.6216e-05)\n",
            "15665 Traning Loss: tensor(7.7580e-05)\n",
            "15666 Traning Loss: tensor(8.0098e-05)\n",
            "15667 Traning Loss: tensor(7.7249e-05)\n",
            "15668 Traning Loss: tensor(7.5971e-05)\n",
            "15669 Traning Loss: tensor(7.7109e-05)\n",
            "15670 Traning Loss: tensor(7.1785e-05)\n",
            "15671 Traning Loss: tensor(7.5032e-05)\n",
            "15672 Traning Loss: tensor(7.0545e-05)\n",
            "15673 Traning Loss: tensor(7.5106e-05)\n",
            "15674 Traning Loss: tensor(8.7234e-05)\n",
            "15675 Traning Loss: tensor(9.8178e-05)\n",
            "15676 Traning Loss: tensor(8.9783e-05)\n",
            "15677 Traning Loss: tensor(8.4577e-05)\n",
            "15678 Traning Loss: tensor(7.9692e-05)\n",
            "15679 Traning Loss: tensor(8.0175e-05)\n",
            "15680 Traning Loss: tensor(9.6861e-05)\n",
            "15681 Traning Loss: tensor(7.4701e-05)\n",
            "15682 Traning Loss: tensor(0.0001)\n",
            "15683 Traning Loss: tensor(8.5904e-05)\n",
            "15684 Traning Loss: tensor(0.0002)\n",
            "15685 Traning Loss: tensor(7.8540e-05)\n",
            "15686 Traning Loss: tensor(8.3662e-05)\n",
            "15687 Traning Loss: tensor(7.7828e-05)\n",
            "15688 Traning Loss: tensor(8.0150e-05)\n",
            "15689 Traning Loss: tensor(8.0346e-05)\n",
            "15690 Traning Loss: tensor(8.4883e-05)\n",
            "15691 Traning Loss: tensor(7.7798e-05)\n",
            "15692 Traning Loss: tensor(8.9957e-05)\n",
            "15693 Traning Loss: tensor(8.3124e-05)\n",
            "15694 Traning Loss: tensor(7.7237e-05)\n",
            "15695 Traning Loss: tensor(0.0001)\n",
            "15696 Traning Loss: tensor(9.1909e-05)\n",
            "15697 Traning Loss: tensor(8.4870e-05)\n",
            "15698 Traning Loss: tensor(0.0002)\n",
            "15699 Traning Loss: tensor(8.0787e-05)\n",
            "15700 Traning Loss: tensor(8.2035e-05)\n",
            "15701 Traning Loss: tensor(9.6904e-05)\n",
            "15702 Traning Loss: tensor(7.8642e-05)\n",
            "15703 Traning Loss: tensor(7.7331e-05)\n",
            "15704 Traning Loss: tensor(7.7913e-05)\n",
            "15705 Traning Loss: tensor(9.7628e-05)\n",
            "15706 Traning Loss: tensor(8.2522e-05)\n",
            "15707 Traning Loss: tensor(8.0169e-05)\n",
            "15708 Traning Loss: tensor(8.0691e-05)\n",
            "15709 Traning Loss: tensor(0.0002)\n",
            "15710 Traning Loss: tensor(8.6835e-05)\n",
            "15711 Traning Loss: tensor(7.2180e-05)\n",
            "15712 Traning Loss: tensor(9.3118e-05)\n",
            "15713 Traning Loss: tensor(0.0001)\n",
            "15714 Traning Loss: tensor(8.9329e-05)\n",
            "15715 Traning Loss: tensor(8.6633e-05)\n",
            "15716 Traning Loss: tensor(8.3677e-05)\n",
            "15717 Traning Loss: tensor(7.6507e-05)\n",
            "15718 Traning Loss: tensor(0.0001)\n",
            "15719 Traning Loss: tensor(8.4364e-05)\n",
            "15720 Traning Loss: tensor(8.1335e-05)\n",
            "15721 Traning Loss: tensor(7.8080e-05)\n",
            "15722 Traning Loss: tensor(8.1494e-05)\n",
            "15723 Traning Loss: tensor(8.1960e-05)\n",
            "15724 Traning Loss: tensor(8.0205e-05)\n",
            "15725 Traning Loss: tensor(7.7457e-05)\n",
            "15726 Traning Loss: tensor(7.4039e-05)\n",
            "15727 Traning Loss: tensor(8.4089e-05)\n",
            "15728 Traning Loss: tensor(9.7857e-05)\n",
            "15729 Traning Loss: tensor(8.1751e-05)\n",
            "15730 Traning Loss: tensor(8.5475e-05)\n",
            "15731 Traning Loss: tensor(0.0002)\n",
            "15732 Traning Loss: tensor(8.6456e-05)\n",
            "15733 Traning Loss: tensor(8.5635e-05)\n",
            "15734 Traning Loss: tensor(7.6363e-05)\n",
            "15735 Traning Loss: tensor(7.6946e-05)\n",
            "15736 Traning Loss: tensor(8.7046e-05)\n",
            "15737 Traning Loss: tensor(7.7260e-05)\n",
            "15738 Traning Loss: tensor(7.8775e-05)\n",
            "15739 Traning Loss: tensor(7.5728e-05)\n",
            "15740 Traning Loss: tensor(8.1692e-05)\n",
            "15741 Traning Loss: tensor(0.0001)\n",
            "15742 Traning Loss: tensor(7.9176e-05)\n",
            "15743 Traning Loss: tensor(0.0001)\n",
            "15744 Traning Loss: tensor(7.7631e-05)\n",
            "15745 Traning Loss: tensor(8.2179e-05)\n",
            "15746 Traning Loss: tensor(8.0389e-05)\n",
            "15747 Traning Loss: tensor(8.1926e-05)\n",
            "15748 Traning Loss: tensor(9.7878e-05)\n",
            "15749 Traning Loss: tensor(7.8772e-05)\n",
            "15750 Traning Loss: tensor(7.6422e-05)\n",
            "15751 Traning Loss: tensor(7.6751e-05)\n",
            "15752 Traning Loss: tensor(7.8783e-05)\n",
            "15753 Traning Loss: tensor(7.8278e-05)\n",
            "15754 Traning Loss: tensor(7.6361e-05)\n",
            "15755 Traning Loss: tensor(7.8713e-05)\n",
            "15756 Traning Loss: tensor(0.0002)\n",
            "15757 Traning Loss: tensor(7.7859e-05)\n",
            "15758 Traning Loss: tensor(8.6820e-05)\n",
            "15759 Traning Loss: tensor(0.0001)\n",
            "15760 Traning Loss: tensor(8.3954e-05)\n",
            "15761 Traning Loss: tensor(7.9043e-05)\n",
            "15762 Traning Loss: tensor(8.0169e-05)\n",
            "15763 Traning Loss: tensor(0.0001)\n",
            "15764 Traning Loss: tensor(9.2053e-05)\n",
            "15765 Traning Loss: tensor(8.2379e-05)\n",
            "15766 Traning Loss: tensor(7.7294e-05)\n",
            "15767 Traning Loss: tensor(7.8684e-05)\n",
            "15768 Traning Loss: tensor(7.8341e-05)\n",
            "15769 Traning Loss: tensor(7.6559e-05)\n",
            "15770 Traning Loss: tensor(7.3595e-05)\n",
            "15771 Traning Loss: tensor(8.2248e-05)\n",
            "15772 Traning Loss: tensor(8.2754e-05)\n",
            "15773 Traning Loss: tensor(7.7279e-05)\n",
            "15774 Traning Loss: tensor(8.2451e-05)\n",
            "15775 Traning Loss: tensor(8.0162e-05)\n",
            "15776 Traning Loss: tensor(7.5862e-05)\n",
            "15777 Traning Loss: tensor(8.0773e-05)\n",
            "15778 Traning Loss: tensor(8.2236e-05)\n",
            "15779 Traning Loss: tensor(7.4775e-05)\n",
            "15780 Traning Loss: tensor(8.6300e-05)\n",
            "15781 Traning Loss: tensor(7.9291e-05)\n",
            "15782 Traning Loss: tensor(7.7922e-05)\n",
            "15783 Traning Loss: tensor(7.1046e-05)\n",
            "15784 Traning Loss: tensor(6.9087e-05)\n",
            "15785 Traning Loss: tensor(7.7021e-05)\n",
            "15786 Traning Loss: tensor(0.0002)\n",
            "15787 Traning Loss: tensor(9.7143e-05)\n",
            "15788 Traning Loss: tensor(7.6239e-05)\n",
            "15789 Traning Loss: tensor(8.1223e-05)\n",
            "15790 Traning Loss: tensor(7.6939e-05)\n",
            "15791 Traning Loss: tensor(7.9413e-05)\n",
            "15792 Traning Loss: tensor(7.3095e-05)\n",
            "15793 Traning Loss: tensor(8.1128e-05)\n",
            "15794 Traning Loss: tensor(8.0614e-05)\n",
            "15795 Traning Loss: tensor(7.0876e-05)\n",
            "15796 Traning Loss: tensor(7.5293e-05)\n",
            "15797 Traning Loss: tensor(7.9046e-05)\n",
            "15798 Traning Loss: tensor(7.4419e-05)\n",
            "15799 Traning Loss: tensor(7.2046e-05)\n",
            "15800 Traning Loss: tensor(7.6548e-05)\n",
            "15801 Traning Loss: tensor(7.3210e-05)\n",
            "15802 Traning Loss: tensor(7.5608e-05)\n",
            "15803 Traning Loss: tensor(0.0001)\n",
            "15804 Traning Loss: tensor(8.4664e-05)\n",
            "15805 Traning Loss: tensor(7.4232e-05)\n",
            "15806 Traning Loss: tensor(7.6231e-05)\n",
            "15807 Traning Loss: tensor(7.0542e-05)\n",
            "15808 Traning Loss: tensor(7.2539e-05)\n",
            "15809 Traning Loss: tensor(7.6700e-05)\n",
            "15810 Traning Loss: tensor(7.6021e-05)\n",
            "15811 Traning Loss: tensor(7.5957e-05)\n",
            "15812 Traning Loss: tensor(8.6687e-05)\n",
            "15813 Traning Loss: tensor(7.7549e-05)\n",
            "15814 Traning Loss: tensor(0.0001)\n",
            "15815 Traning Loss: tensor(8.0953e-05)\n",
            "15816 Traning Loss: tensor(7.2459e-05)\n",
            "15817 Traning Loss: tensor(7.8832e-05)\n",
            "15818 Traning Loss: tensor(7.5884e-05)\n",
            "15819 Traning Loss: tensor(7.1527e-05)\n",
            "15820 Traning Loss: tensor(7.9767e-05)\n",
            "15821 Traning Loss: tensor(0.0001)\n",
            "15822 Traning Loss: tensor(6.9966e-05)\n",
            "15823 Traning Loss: tensor(9.0766e-05)\n",
            "15824 Traning Loss: tensor(7.1160e-05)\n",
            "15825 Traning Loss: tensor(7.5911e-05)\n",
            "15826 Traning Loss: tensor(7.7603e-05)\n",
            "15827 Traning Loss: tensor(7.2947e-05)\n",
            "15828 Traning Loss: tensor(7.7544e-05)\n",
            "15829 Traning Loss: tensor(7.7347e-05)\n",
            "15830 Traning Loss: tensor(7.3413e-05)\n",
            "15831 Traning Loss: tensor(9.9696e-05)\n",
            "15832 Traning Loss: tensor(6.8349e-05)\n",
            "15833 Traning Loss: tensor(7.1573e-05)\n",
            "15834 Traning Loss: tensor(7.0854e-05)\n",
            "15835 Traning Loss: tensor(0.0001)\n",
            "15836 Traning Loss: tensor(7.1240e-05)\n",
            "15837 Traning Loss: tensor(7.1536e-05)\n",
            "15838 Traning Loss: tensor(7.3703e-05)\n",
            "15839 Traning Loss: tensor(6.9962e-05)\n",
            "15840 Traning Loss: tensor(6.9522e-05)\n",
            "15841 Traning Loss: tensor(7.0475e-05)\n",
            "15842 Traning Loss: tensor(7.0793e-05)\n",
            "15843 Traning Loss: tensor(0.0001)\n",
            "15844 Traning Loss: tensor(7.5923e-05)\n",
            "15845 Traning Loss: tensor(6.9920e-05)\n",
            "15846 Traning Loss: tensor(7.8269e-05)\n",
            "15847 Traning Loss: tensor(7.2460e-05)\n",
            "15848 Traning Loss: tensor(7.0079e-05)\n",
            "15849 Traning Loss: tensor(7.5648e-05)\n",
            "15850 Traning Loss: tensor(7.3959e-05)\n",
            "15851 Traning Loss: tensor(8.7968e-05)\n",
            "15852 Traning Loss: tensor(7.3552e-05)\n",
            "15853 Traning Loss: tensor(7.9922e-05)\n",
            "15854 Traning Loss: tensor(7.5826e-05)\n",
            "15855 Traning Loss: tensor(0.0002)\n",
            "15856 Traning Loss: tensor(9.2593e-05)\n",
            "15857 Traning Loss: tensor(8.6345e-05)\n",
            "15858 Traning Loss: tensor(7.7493e-05)\n",
            "15859 Traning Loss: tensor(8.9472e-05)\n",
            "15860 Traning Loss: tensor(7.4286e-05)\n",
            "15861 Traning Loss: tensor(8.5266e-05)\n",
            "15862 Traning Loss: tensor(8.4919e-05)\n",
            "15863 Traning Loss: tensor(8.1852e-05)\n",
            "15864 Traning Loss: tensor(7.8893e-05)\n",
            "15865 Traning Loss: tensor(8.0617e-05)\n",
            "15866 Traning Loss: tensor(7.1415e-05)\n",
            "15867 Traning Loss: tensor(8.7240e-05)\n",
            "15868 Traning Loss: tensor(7.6295e-05)\n",
            "15869 Traning Loss: tensor(7.4912e-05)\n",
            "15870 Traning Loss: tensor(8.7689e-05)\n",
            "15871 Traning Loss: tensor(6.9367e-05)\n",
            "15872 Traning Loss: tensor(7.6676e-05)\n",
            "15873 Traning Loss: tensor(6.8655e-05)\n",
            "15874 Traning Loss: tensor(8.0150e-05)\n",
            "15875 Traning Loss: tensor(7.0922e-05)\n",
            "15876 Traning Loss: tensor(7.7809e-05)\n",
            "15877 Traning Loss: tensor(7.6112e-05)\n",
            "15878 Traning Loss: tensor(6.8895e-05)\n",
            "15879 Traning Loss: tensor(7.2840e-05)\n",
            "15880 Traning Loss: tensor(7.1680e-05)\n",
            "15881 Traning Loss: tensor(6.8851e-05)\n",
            "15882 Traning Loss: tensor(7.3258e-05)\n",
            "15883 Traning Loss: tensor(7.3468e-05)\n",
            "15884 Traning Loss: tensor(8.0190e-05)\n",
            "15885 Traning Loss: tensor(7.0641e-05)\n",
            "15886 Traning Loss: tensor(6.8461e-05)\n",
            "15887 Traning Loss: tensor(7.2399e-05)\n",
            "15888 Traning Loss: tensor(7.1379e-05)\n",
            "15889 Traning Loss: tensor(6.7768e-05)\n",
            "15890 Traning Loss: tensor(6.8928e-05)\n",
            "15891 Traning Loss: tensor(7.2397e-05)\n",
            "15892 Traning Loss: tensor(7.1230e-05)\n",
            "15893 Traning Loss: tensor(8.3708e-05)\n",
            "15894 Traning Loss: tensor(0.0001)\n",
            "15895 Traning Loss: tensor(0.0001)\n",
            "15896 Traning Loss: tensor(7.9944e-05)\n",
            "15897 Traning Loss: tensor(7.0714e-05)\n",
            "15898 Traning Loss: tensor(7.8405e-05)\n",
            "15899 Traning Loss: tensor(7.9662e-05)\n",
            "15900 Traning Loss: tensor(8.2502e-05)\n",
            "15901 Traning Loss: tensor(7.4186e-05)\n",
            "15902 Traning Loss: tensor(7.1113e-05)\n",
            "15903 Traning Loss: tensor(7.5496e-05)\n",
            "15904 Traning Loss: tensor(0.0002)\n",
            "15905 Traning Loss: tensor(8.0438e-05)\n",
            "15906 Traning Loss: tensor(6.9064e-05)\n",
            "15907 Traning Loss: tensor(6.9779e-05)\n",
            "15908 Traning Loss: tensor(6.7116e-05)\n",
            "15909 Traning Loss: tensor(7.6846e-05)\n",
            "15910 Traning Loss: tensor(7.3435e-05)\n",
            "15911 Traning Loss: tensor(7.3455e-05)\n",
            "15912 Traning Loss: tensor(7.2538e-05)\n",
            "15913 Traning Loss: tensor(7.2148e-05)\n",
            "15914 Traning Loss: tensor(7.5428e-05)\n",
            "15915 Traning Loss: tensor(8.0841e-05)\n",
            "15916 Traning Loss: tensor(7.8688e-05)\n",
            "15917 Traning Loss: tensor(8.8404e-05)\n",
            "15918 Traning Loss: tensor(7.7202e-05)\n",
            "15919 Traning Loss: tensor(7.3037e-05)\n",
            "15920 Traning Loss: tensor(6.6824e-05)\n",
            "15921 Traning Loss: tensor(7.3489e-05)\n",
            "15922 Traning Loss: tensor(7.3229e-05)\n",
            "15923 Traning Loss: tensor(6.9516e-05)\n",
            "15924 Traning Loss: tensor(6.7535e-05)\n",
            "15925 Traning Loss: tensor(6.9238e-05)\n",
            "15926 Traning Loss: tensor(7.3808e-05)\n",
            "15927 Traning Loss: tensor(6.7085e-05)\n",
            "15928 Traning Loss: tensor(7.6043e-05)\n",
            "15929 Traning Loss: tensor(9.4856e-05)\n",
            "15930 Traning Loss: tensor(6.9083e-05)\n",
            "15931 Traning Loss: tensor(9.8565e-05)\n",
            "15932 Traning Loss: tensor(7.2515e-05)\n",
            "15933 Traning Loss: tensor(6.9907e-05)\n",
            "15934 Traning Loss: tensor(6.7010e-05)\n",
            "15935 Traning Loss: tensor(7.5565e-05)\n",
            "15936 Traning Loss: tensor(7.1769e-05)\n",
            "15937 Traning Loss: tensor(7.3799e-05)\n",
            "15938 Traning Loss: tensor(9.6153e-05)\n",
            "15939 Traning Loss: tensor(7.2697e-05)\n",
            "15940 Traning Loss: tensor(7.7041e-05)\n",
            "15941 Traning Loss: tensor(8.7212e-05)\n",
            "15942 Traning Loss: tensor(6.8941e-05)\n",
            "15943 Traning Loss: tensor(7.1252e-05)\n",
            "15944 Traning Loss: tensor(6.9184e-05)\n",
            "15945 Traning Loss: tensor(0.0001)\n",
            "15946 Traning Loss: tensor(7.3899e-05)\n",
            "15947 Traning Loss: tensor(7.6273e-05)\n",
            "15948 Traning Loss: tensor(7.2600e-05)\n",
            "15949 Traning Loss: tensor(6.9136e-05)\n",
            "15950 Traning Loss: tensor(7.6490e-05)\n",
            "15951 Traning Loss: tensor(7.0763e-05)\n",
            "15952 Traning Loss: tensor(7.6048e-05)\n",
            "15953 Traning Loss: tensor(6.5632e-05)\n",
            "15954 Traning Loss: tensor(7.5946e-05)\n",
            "15955 Traning Loss: tensor(7.1446e-05)\n",
            "15956 Traning Loss: tensor(6.8504e-05)\n",
            "15957 Traning Loss: tensor(6.8431e-05)\n",
            "15958 Traning Loss: tensor(0.0001)\n",
            "15959 Traning Loss: tensor(6.7942e-05)\n",
            "15960 Traning Loss: tensor(6.8380e-05)\n",
            "15961 Traning Loss: tensor(6.8412e-05)\n",
            "15962 Traning Loss: tensor(7.5730e-05)\n",
            "15963 Traning Loss: tensor(0.0001)\n",
            "15964 Traning Loss: tensor(7.2658e-05)\n",
            "15965 Traning Loss: tensor(6.6832e-05)\n",
            "15966 Traning Loss: tensor(7.0133e-05)\n",
            "15967 Traning Loss: tensor(6.8949e-05)\n",
            "15968 Traning Loss: tensor(7.2462e-05)\n",
            "15969 Traning Loss: tensor(6.8797e-05)\n",
            "15970 Traning Loss: tensor(6.6596e-05)\n",
            "15971 Traning Loss: tensor(6.7858e-05)\n",
            "15972 Traning Loss: tensor(8.0134e-05)\n",
            "15973 Traning Loss: tensor(7.7621e-05)\n",
            "15974 Traning Loss: tensor(7.8043e-05)\n",
            "15975 Traning Loss: tensor(6.8309e-05)\n",
            "15976 Traning Loss: tensor(6.4561e-05)\n",
            "15977 Traning Loss: tensor(7.3239e-05)\n",
            "15978 Traning Loss: tensor(6.8949e-05)\n",
            "15979 Traning Loss: tensor(7.0922e-05)\n",
            "15980 Traning Loss: tensor(6.9261e-05)\n",
            "15981 Traning Loss: tensor(7.0641e-05)\n",
            "15982 Traning Loss: tensor(0.0001)\n",
            "15983 Traning Loss: tensor(7.2233e-05)\n",
            "15984 Traning Loss: tensor(6.8760e-05)\n",
            "15985 Traning Loss: tensor(0.0001)\n",
            "15986 Traning Loss: tensor(6.8444e-05)\n",
            "15987 Traning Loss: tensor(7.1077e-05)\n",
            "15988 Traning Loss: tensor(7.4831e-05)\n",
            "15989 Traning Loss: tensor(7.2276e-05)\n",
            "15990 Traning Loss: tensor(6.5552e-05)\n",
            "15991 Traning Loss: tensor(6.8517e-05)\n",
            "15992 Traning Loss: tensor(6.7504e-05)\n",
            "15993 Traning Loss: tensor(0.0001)\n",
            "15994 Traning Loss: tensor(8.5940e-05)\n",
            "15995 Traning Loss: tensor(6.5114e-05)\n",
            "15996 Traning Loss: tensor(7.1404e-05)\n",
            "15997 Traning Loss: tensor(6.6260e-05)\n",
            "15998 Traning Loss: tensor(6.6506e-05)\n",
            "15999 Traning Loss: tensor(7.4107e-05)\n",
            "16000 Traning Loss: tensor(8.5105e-05)\n",
            "16001 Traning Loss: tensor(8.4547e-05)\n",
            "16002 Traning Loss: tensor(6.8527e-05)\n",
            "16003 Traning Loss: tensor(6.9553e-05)\n",
            "16004 Traning Loss: tensor(6.8490e-05)\n",
            "16005 Traning Loss: tensor(9.9101e-05)\n",
            "16006 Traning Loss: tensor(6.8591e-05)\n",
            "16007 Traning Loss: tensor(7.2442e-05)\n",
            "16008 Traning Loss: tensor(6.7523e-05)\n",
            "16009 Traning Loss: tensor(7.7081e-05)\n",
            "16010 Traning Loss: tensor(6.6146e-05)\n",
            "16011 Traning Loss: tensor(6.7722e-05)\n",
            "16012 Traning Loss: tensor(8.9074e-05)\n",
            "16013 Traning Loss: tensor(0.0001)\n",
            "16014 Traning Loss: tensor(0.0001)\n",
            "16015 Traning Loss: tensor(7.0941e-05)\n",
            "16016 Traning Loss: tensor(7.9181e-05)\n",
            "16017 Traning Loss: tensor(7.1146e-05)\n",
            "16018 Traning Loss: tensor(7.4176e-05)\n",
            "16019 Traning Loss: tensor(0.0001)\n",
            "16020 Traning Loss: tensor(7.2808e-05)\n",
            "16021 Traning Loss: tensor(6.4526e-05)\n",
            "16022 Traning Loss: tensor(0.0001)\n",
            "16023 Traning Loss: tensor(6.7644e-05)\n",
            "16024 Traning Loss: tensor(9.5339e-05)\n",
            "16025 Traning Loss: tensor(6.0146e-05)\n",
            "16026 Traning Loss: tensor(0.0001)\n",
            "16027 Traning Loss: tensor(7.7468e-05)\n",
            "16028 Traning Loss: tensor(7.0323e-05)\n",
            "16029 Traning Loss: tensor(7.5701e-05)\n",
            "16030 Traning Loss: tensor(7.4445e-05)\n",
            "16031 Traning Loss: tensor(7.1952e-05)\n",
            "16032 Traning Loss: tensor(7.0427e-05)\n",
            "16033 Traning Loss: tensor(7.1955e-05)\n",
            "16034 Traning Loss: tensor(7.0685e-05)\n",
            "16035 Traning Loss: tensor(7.2029e-05)\n",
            "16036 Traning Loss: tensor(7.2519e-05)\n",
            "16037 Traning Loss: tensor(7.9812e-05)\n",
            "16038 Traning Loss: tensor(7.1411e-05)\n",
            "16039 Traning Loss: tensor(7.7825e-05)\n",
            "16040 Traning Loss: tensor(6.9861e-05)\n",
            "16041 Traning Loss: tensor(8.6507e-05)\n",
            "16042 Traning Loss: tensor(8.1795e-05)\n",
            "16043 Traning Loss: tensor(7.9701e-05)\n",
            "16044 Traning Loss: tensor(7.3332e-05)\n",
            "16045 Traning Loss: tensor(6.8239e-05)\n",
            "16046 Traning Loss: tensor(7.0025e-05)\n",
            "16047 Traning Loss: tensor(6.7555e-05)\n",
            "16048 Traning Loss: tensor(7.5652e-05)\n",
            "16049 Traning Loss: tensor(0.0002)\n",
            "16050 Traning Loss: tensor(9.0779e-05)\n",
            "16051 Traning Loss: tensor(8.4438e-05)\n",
            "16052 Traning Loss: tensor(7.2008e-05)\n",
            "16053 Traning Loss: tensor(8.1244e-05)\n",
            "16054 Traning Loss: tensor(8.2741e-05)\n",
            "16055 Traning Loss: tensor(8.7833e-05)\n",
            "16056 Traning Loss: tensor(7.0112e-05)\n",
            "16057 Traning Loss: tensor(7.4747e-05)\n",
            "16058 Traning Loss: tensor(7.6102e-05)\n",
            "16059 Traning Loss: tensor(6.9859e-05)\n",
            "16060 Traning Loss: tensor(7.0099e-05)\n",
            "16061 Traning Loss: tensor(7.4179e-05)\n",
            "16062 Traning Loss: tensor(7.5369e-05)\n",
            "16063 Traning Loss: tensor(6.2758e-05)\n",
            "16064 Traning Loss: tensor(7.0219e-05)\n",
            "16065 Traning Loss: tensor(0.0002)\n",
            "16066 Traning Loss: tensor(8.5060e-05)\n",
            "16067 Traning Loss: tensor(8.0106e-05)\n",
            "16068 Traning Loss: tensor(0.0001)\n",
            "16069 Traning Loss: tensor(7.2156e-05)\n",
            "16070 Traning Loss: tensor(7.2241e-05)\n",
            "16071 Traning Loss: tensor(0.0001)\n",
            "16072 Traning Loss: tensor(7.3207e-05)\n",
            "16073 Traning Loss: tensor(6.8540e-05)\n",
            "16074 Traning Loss: tensor(7.3380e-05)\n",
            "16075 Traning Loss: tensor(7.1009e-05)\n",
            "16076 Traning Loss: tensor(7.0103e-05)\n",
            "16077 Traning Loss: tensor(8.9207e-05)\n",
            "16078 Traning Loss: tensor(7.3605e-05)\n",
            "16079 Traning Loss: tensor(6.7712e-05)\n",
            "16080 Traning Loss: tensor(6.4280e-05)\n",
            "16081 Traning Loss: tensor(6.9956e-05)\n",
            "16082 Traning Loss: tensor(7.9013e-05)\n",
            "16083 Traning Loss: tensor(7.2207e-05)\n",
            "16084 Traning Loss: tensor(7.7308e-05)\n",
            "16085 Traning Loss: tensor(6.5087e-05)\n",
            "16086 Traning Loss: tensor(6.8763e-05)\n",
            "16087 Traning Loss: tensor(7.2096e-05)\n",
            "16088 Traning Loss: tensor(7.2415e-05)\n",
            "16089 Traning Loss: tensor(6.5862e-05)\n",
            "16090 Traning Loss: tensor(6.6823e-05)\n",
            "16091 Traning Loss: tensor(6.6442e-05)\n",
            "16092 Traning Loss: tensor(0.0001)\n",
            "16093 Traning Loss: tensor(7.6075e-05)\n",
            "16094 Traning Loss: tensor(8.3083e-05)\n",
            "16095 Traning Loss: tensor(7.3724e-05)\n",
            "16096 Traning Loss: tensor(8.3157e-05)\n",
            "16097 Traning Loss: tensor(7.3659e-05)\n",
            "16098 Traning Loss: tensor(0.0001)\n",
            "16099 Traning Loss: tensor(6.9555e-05)\n",
            "16100 Traning Loss: tensor(8.9035e-05)\n",
            "16101 Traning Loss: tensor(6.7740e-05)\n",
            "16102 Traning Loss: tensor(6.5954e-05)\n",
            "16103 Traning Loss: tensor(6.5858e-05)\n",
            "16104 Traning Loss: tensor(6.6332e-05)\n",
            "16105 Traning Loss: tensor(6.5458e-05)\n",
            "16106 Traning Loss: tensor(6.1146e-05)\n",
            "16107 Traning Loss: tensor(6.1614e-05)\n",
            "16108 Traning Loss: tensor(6.9441e-05)\n",
            "16109 Traning Loss: tensor(6.4406e-05)\n",
            "16110 Traning Loss: tensor(7.0117e-05)\n",
            "16111 Traning Loss: tensor(7.1927e-05)\n",
            "16112 Traning Loss: tensor(6.4361e-05)\n",
            "16113 Traning Loss: tensor(6.5942e-05)\n",
            "16114 Traning Loss: tensor(6.6440e-05)\n",
            "16115 Traning Loss: tensor(6.6196e-05)\n",
            "16116 Traning Loss: tensor(6.7226e-05)\n",
            "16117 Traning Loss: tensor(6.2686e-05)\n",
            "16118 Traning Loss: tensor(6.4798e-05)\n",
            "16119 Traning Loss: tensor(6.6224e-05)\n",
            "16120 Traning Loss: tensor(6.5922e-05)\n",
            "16121 Traning Loss: tensor(6.3864e-05)\n",
            "16122 Traning Loss: tensor(6.8203e-05)\n",
            "16123 Traning Loss: tensor(9.2454e-05)\n",
            "16124 Traning Loss: tensor(6.3652e-05)\n",
            "16125 Traning Loss: tensor(9.2637e-05)\n",
            "16126 Traning Loss: tensor(7.1947e-05)\n",
            "16127 Traning Loss: tensor(7.1542e-05)\n",
            "16128 Traning Loss: tensor(0.0001)\n",
            "16129 Traning Loss: tensor(6.9519e-05)\n",
            "16130 Traning Loss: tensor(6.5893e-05)\n",
            "16131 Traning Loss: tensor(6.4825e-05)\n",
            "16132 Traning Loss: tensor(6.3136e-05)\n",
            "16133 Traning Loss: tensor(6.8813e-05)\n",
            "16134 Traning Loss: tensor(0.0001)\n",
            "16135 Traning Loss: tensor(7.0378e-05)\n",
            "16136 Traning Loss: tensor(6.2982e-05)\n",
            "16137 Traning Loss: tensor(6.4066e-05)\n",
            "16138 Traning Loss: tensor(6.7618e-05)\n",
            "16139 Traning Loss: tensor(6.8345e-05)\n",
            "16140 Traning Loss: tensor(6.6434e-05)\n",
            "16141 Traning Loss: tensor(6.5345e-05)\n",
            "16142 Traning Loss: tensor(6.3439e-05)\n",
            "16143 Traning Loss: tensor(6.7449e-05)\n",
            "16144 Traning Loss: tensor(6.4570e-05)\n",
            "16145 Traning Loss: tensor(8.7403e-05)\n",
            "16146 Traning Loss: tensor(6.3985e-05)\n",
            "16147 Traning Loss: tensor(8.9178e-05)\n",
            "16148 Traning Loss: tensor(7.0503e-05)\n",
            "16149 Traning Loss: tensor(7.1994e-05)\n",
            "16150 Traning Loss: tensor(8.7379e-05)\n",
            "16151 Traning Loss: tensor(0.0001)\n",
            "16152 Traning Loss: tensor(8.2539e-05)\n",
            "16153 Traning Loss: tensor(7.3156e-05)\n",
            "16154 Traning Loss: tensor(6.5480e-05)\n",
            "16155 Traning Loss: tensor(6.7678e-05)\n",
            "16156 Traning Loss: tensor(6.9697e-05)\n",
            "16157 Traning Loss: tensor(6.0643e-05)\n",
            "16158 Traning Loss: tensor(6.2675e-05)\n",
            "16159 Traning Loss: tensor(6.1876e-05)\n",
            "16160 Traning Loss: tensor(6.9185e-05)\n",
            "16161 Traning Loss: tensor(7.1294e-05)\n",
            "16162 Traning Loss: tensor(6.3047e-05)\n",
            "16163 Traning Loss: tensor(8.8190e-05)\n",
            "16164 Traning Loss: tensor(6.6699e-05)\n",
            "16165 Traning Loss: tensor(9.2802e-05)\n",
            "16166 Traning Loss: tensor(6.3305e-05)\n",
            "16167 Traning Loss: tensor(6.7149e-05)\n",
            "16168 Traning Loss: tensor(6.9918e-05)\n",
            "16169 Traning Loss: tensor(7.0126e-05)\n",
            "16170 Traning Loss: tensor(6.2444e-05)\n",
            "16171 Traning Loss: tensor(6.3718e-05)\n",
            "16172 Traning Loss: tensor(6.8311e-05)\n",
            "16173 Traning Loss: tensor(6.4274e-05)\n",
            "16174 Traning Loss: tensor(6.3661e-05)\n",
            "16175 Traning Loss: tensor(5.9379e-05)\n",
            "16176 Traning Loss: tensor(6.2498e-05)\n",
            "16177 Traning Loss: tensor(6.8529e-05)\n",
            "16178 Traning Loss: tensor(6.4458e-05)\n",
            "16179 Traning Loss: tensor(6.0628e-05)\n",
            "16180 Traning Loss: tensor(6.3322e-05)\n",
            "16181 Traning Loss: tensor(6.7734e-05)\n",
            "16182 Traning Loss: tensor(0.0001)\n",
            "16183 Traning Loss: tensor(6.0499e-05)\n",
            "16184 Traning Loss: tensor(6.7574e-05)\n",
            "16185 Traning Loss: tensor(6.4529e-05)\n",
            "16186 Traning Loss: tensor(6.5548e-05)\n",
            "16187 Traning Loss: tensor(6.9300e-05)\n",
            "16188 Traning Loss: tensor(7.0345e-05)\n",
            "16189 Traning Loss: tensor(6.4933e-05)\n",
            "16190 Traning Loss: tensor(6.6781e-05)\n",
            "16191 Traning Loss: tensor(6.6179e-05)\n",
            "16192 Traning Loss: tensor(6.5390e-05)\n",
            "16193 Traning Loss: tensor(6.9532e-05)\n",
            "16194 Traning Loss: tensor(6.6718e-05)\n",
            "16195 Traning Loss: tensor(6.1458e-05)\n",
            "16196 Traning Loss: tensor(7.6554e-05)\n",
            "16197 Traning Loss: tensor(6.3620e-05)\n",
            "16198 Traning Loss: tensor(0.0001)\n",
            "16199 Traning Loss: tensor(8.5783e-05)\n",
            "16200 Traning Loss: tensor(7.5353e-05)\n",
            "16201 Traning Loss: tensor(6.8835e-05)\n",
            "16202 Traning Loss: tensor(7.0133e-05)\n",
            "16203 Traning Loss: tensor(6.8984e-05)\n",
            "16204 Traning Loss: tensor(0.0001)\n",
            "16205 Traning Loss: tensor(7.0808e-05)\n",
            "16206 Traning Loss: tensor(6.9634e-05)\n",
            "16207 Traning Loss: tensor(6.2202e-05)\n",
            "16208 Traning Loss: tensor(7.6546e-05)\n",
            "16209 Traning Loss: tensor(6.8617e-05)\n",
            "16210 Traning Loss: tensor(7.4011e-05)\n",
            "16211 Traning Loss: tensor(6.9188e-05)\n",
            "16212 Traning Loss: tensor(6.7808e-05)\n",
            "16213 Traning Loss: tensor(6.7179e-05)\n",
            "16214 Traning Loss: tensor(6.6819e-05)\n",
            "16215 Traning Loss: tensor(7.5558e-05)\n",
            "16216 Traning Loss: tensor(6.4194e-05)\n",
            "16217 Traning Loss: tensor(6.5970e-05)\n",
            "16218 Traning Loss: tensor(7.2149e-05)\n",
            "16219 Traning Loss: tensor(6.1308e-05)\n",
            "16220 Traning Loss: tensor(7.2058e-05)\n",
            "16221 Traning Loss: tensor(6.2306e-05)\n",
            "16222 Traning Loss: tensor(6.5662e-05)\n",
            "16223 Traning Loss: tensor(6.5335e-05)\n",
            "16224 Traning Loss: tensor(6.9627e-05)\n",
            "16225 Traning Loss: tensor(6.4712e-05)\n",
            "16226 Traning Loss: tensor(6.4737e-05)\n",
            "16227 Traning Loss: tensor(7.4079e-05)\n",
            "16228 Traning Loss: tensor(6.2049e-05)\n",
            "16229 Traning Loss: tensor(6.8229e-05)\n",
            "16230 Traning Loss: tensor(6.6451e-05)\n",
            "16231 Traning Loss: tensor(6.3779e-05)\n",
            "16232 Traning Loss: tensor(6.1834e-05)\n",
            "16233 Traning Loss: tensor(6.3066e-05)\n",
            "16234 Traning Loss: tensor(6.2261e-05)\n",
            "16235 Traning Loss: tensor(6.6756e-05)\n",
            "16236 Traning Loss: tensor(6.8096e-05)\n",
            "16237 Traning Loss: tensor(6.6032e-05)\n",
            "16238 Traning Loss: tensor(6.8244e-05)\n",
            "16239 Traning Loss: tensor(6.3107e-05)\n",
            "16240 Traning Loss: tensor(7.2647e-05)\n",
            "16241 Traning Loss: tensor(6.7871e-05)\n",
            "16242 Traning Loss: tensor(6.4152e-05)\n",
            "16243 Traning Loss: tensor(6.5332e-05)\n",
            "16244 Traning Loss: tensor(6.6517e-05)\n",
            "16245 Traning Loss: tensor(0.0001)\n",
            "16246 Traning Loss: tensor(7.6458e-05)\n",
            "16247 Traning Loss: tensor(7.1912e-05)\n",
            "16248 Traning Loss: tensor(6.1751e-05)\n",
            "16249 Traning Loss: tensor(7.2871e-05)\n",
            "16250 Traning Loss: tensor(7.0969e-05)\n",
            "16251 Traning Loss: tensor(6.4823e-05)\n",
            "16252 Traning Loss: tensor(6.5939e-05)\n",
            "16253 Traning Loss: tensor(7.3063e-05)\n",
            "16254 Traning Loss: tensor(6.7455e-05)\n",
            "16255 Traning Loss: tensor(6.8456e-05)\n",
            "16256 Traning Loss: tensor(8.5496e-05)\n",
            "16257 Traning Loss: tensor(6.2486e-05)\n",
            "16258 Traning Loss: tensor(6.5424e-05)\n",
            "16259 Traning Loss: tensor(6.4804e-05)\n",
            "16260 Traning Loss: tensor(6.4159e-05)\n",
            "16261 Traning Loss: tensor(8.1882e-05)\n",
            "16262 Traning Loss: tensor(6.6254e-05)\n",
            "16263 Traning Loss: tensor(6.4130e-05)\n",
            "16264 Traning Loss: tensor(6.2323e-05)\n",
            "16265 Traning Loss: tensor(6.1573e-05)\n",
            "16266 Traning Loss: tensor(7.9659e-05)\n",
            "16267 Traning Loss: tensor(0.0001)\n",
            "16268 Traning Loss: tensor(6.5380e-05)\n",
            "16269 Traning Loss: tensor(6.2783e-05)\n",
            "16270 Traning Loss: tensor(6.1846e-05)\n",
            "16271 Traning Loss: tensor(6.1382e-05)\n",
            "16272 Traning Loss: tensor(6.2082e-05)\n",
            "16273 Traning Loss: tensor(6.2108e-05)\n",
            "16274 Traning Loss: tensor(5.9177e-05)\n",
            "16275 Traning Loss: tensor(5.7889e-05)\n",
            "16276 Traning Loss: tensor(9.4505e-05)\n",
            "16277 Traning Loss: tensor(5.7412e-05)\n",
            "16278 Traning Loss: tensor(6.5277e-05)\n",
            "16279 Traning Loss: tensor(6.5632e-05)\n",
            "16280 Traning Loss: tensor(6.5195e-05)\n",
            "16281 Traning Loss: tensor(5.8809e-05)\n",
            "16282 Traning Loss: tensor(6.2227e-05)\n",
            "16283 Traning Loss: tensor(6.0430e-05)\n",
            "16284 Traning Loss: tensor(5.7990e-05)\n",
            "16285 Traning Loss: tensor(5.6587e-05)\n",
            "16286 Traning Loss: tensor(7.2559e-05)\n",
            "16287 Traning Loss: tensor(7.5779e-05)\n",
            "16288 Traning Loss: tensor(6.7657e-05)\n",
            "16289 Traning Loss: tensor(6.9813e-05)\n",
            "16290 Traning Loss: tensor(5.8882e-05)\n",
            "16291 Traning Loss: tensor(6.3958e-05)\n",
            "16292 Traning Loss: tensor(7.3377e-05)\n",
            "16293 Traning Loss: tensor(0.0001)\n",
            "16294 Traning Loss: tensor(6.3772e-05)\n",
            "16295 Traning Loss: tensor(6.5366e-05)\n",
            "16296 Traning Loss: tensor(6.6065e-05)\n",
            "16297 Traning Loss: tensor(5.9229e-05)\n",
            "16298 Traning Loss: tensor(6.1552e-05)\n",
            "16299 Traning Loss: tensor(6.4985e-05)\n",
            "16300 Traning Loss: tensor(6.5355e-05)\n",
            "16301 Traning Loss: tensor(7.8794e-05)\n",
            "16302 Traning Loss: tensor(6.3596e-05)\n",
            "16303 Traning Loss: tensor(6.2618e-05)\n",
            "16304 Traning Loss: tensor(5.9084e-05)\n",
            "16305 Traning Loss: tensor(6.9669e-05)\n",
            "16306 Traning Loss: tensor(6.7891e-05)\n",
            "16307 Traning Loss: tensor(6.1239e-05)\n",
            "16308 Traning Loss: tensor(9.1330e-05)\n",
            "16309 Traning Loss: tensor(6.4920e-05)\n",
            "16310 Traning Loss: tensor(7.2470e-05)\n",
            "16311 Traning Loss: tensor(6.0654e-05)\n",
            "16312 Traning Loss: tensor(6.7129e-05)\n",
            "16313 Traning Loss: tensor(6.7664e-05)\n",
            "16314 Traning Loss: tensor(5.9062e-05)\n",
            "16315 Traning Loss: tensor(6.2155e-05)\n",
            "16316 Traning Loss: tensor(6.4440e-05)\n",
            "16317 Traning Loss: tensor(6.5390e-05)\n",
            "16318 Traning Loss: tensor(6.1710e-05)\n",
            "16319 Traning Loss: tensor(6.1845e-05)\n",
            "16320 Traning Loss: tensor(6.2217e-05)\n",
            "16321 Traning Loss: tensor(5.8891e-05)\n",
            "16322 Traning Loss: tensor(6.8193e-05)\n",
            "16323 Traning Loss: tensor(7.0730e-05)\n",
            "16324 Traning Loss: tensor(7.2062e-05)\n",
            "16325 Traning Loss: tensor(7.7559e-05)\n",
            "16326 Traning Loss: tensor(6.1632e-05)\n",
            "16327 Traning Loss: tensor(5.7424e-05)\n",
            "16328 Traning Loss: tensor(6.6178e-05)\n",
            "16329 Traning Loss: tensor(5.7575e-05)\n",
            "16330 Traning Loss: tensor(6.7897e-05)\n",
            "16331 Traning Loss: tensor(6.1267e-05)\n",
            "16332 Traning Loss: tensor(6.0186e-05)\n",
            "16333 Traning Loss: tensor(7.2067e-05)\n",
            "16334 Traning Loss: tensor(6.6146e-05)\n",
            "16335 Traning Loss: tensor(6.0542e-05)\n",
            "16336 Traning Loss: tensor(6.0224e-05)\n",
            "16337 Traning Loss: tensor(8.7169e-05)\n",
            "16338 Traning Loss: tensor(7.1146e-05)\n",
            "16339 Traning Loss: tensor(7.0003e-05)\n",
            "16340 Traning Loss: tensor(6.3968e-05)\n",
            "16341 Traning Loss: tensor(6.6111e-05)\n",
            "16342 Traning Loss: tensor(6.4218e-05)\n",
            "16343 Traning Loss: tensor(6.2092e-05)\n",
            "16344 Traning Loss: tensor(6.5120e-05)\n",
            "16345 Traning Loss: tensor(6.2630e-05)\n",
            "16346 Traning Loss: tensor(0.0001)\n",
            "16347 Traning Loss: tensor(0.0001)\n",
            "16348 Traning Loss: tensor(6.1902e-05)\n",
            "16349 Traning Loss: tensor(6.2505e-05)\n",
            "16350 Traning Loss: tensor(6.2746e-05)\n",
            "16351 Traning Loss: tensor(7.0690e-05)\n",
            "16352 Traning Loss: tensor(7.6045e-05)\n",
            "16353 Traning Loss: tensor(5.9632e-05)\n",
            "16354 Traning Loss: tensor(6.0662e-05)\n",
            "16355 Traning Loss: tensor(6.1465e-05)\n",
            "16356 Traning Loss: tensor(5.5570e-05)\n",
            "16357 Traning Loss: tensor(5.7762e-05)\n",
            "16358 Traning Loss: tensor(6.0402e-05)\n",
            "16359 Traning Loss: tensor(6.0095e-05)\n",
            "16360 Traning Loss: tensor(5.3504e-05)\n",
            "16361 Traning Loss: tensor(5.9373e-05)\n",
            "16362 Traning Loss: tensor(5.8911e-05)\n",
            "16363 Traning Loss: tensor(5.9080e-05)\n",
            "16364 Traning Loss: tensor(5.7501e-05)\n",
            "16365 Traning Loss: tensor(5.5999e-05)\n",
            "16366 Traning Loss: tensor(5.7158e-05)\n",
            "16367 Traning Loss: tensor(7.5430e-05)\n",
            "16368 Traning Loss: tensor(6.1642e-05)\n",
            "16369 Traning Loss: tensor(5.9265e-05)\n",
            "16370 Traning Loss: tensor(6.2773e-05)\n",
            "16371 Traning Loss: tensor(5.4169e-05)\n",
            "16372 Traning Loss: tensor(6.1888e-05)\n",
            "16373 Traning Loss: tensor(9.5176e-05)\n",
            "16374 Traning Loss: tensor(6.2539e-05)\n",
            "16375 Traning Loss: tensor(6.1349e-05)\n",
            "16376 Traning Loss: tensor(8.7753e-05)\n",
            "16377 Traning Loss: tensor(5.8948e-05)\n",
            "16378 Traning Loss: tensor(6.0882e-05)\n",
            "16379 Traning Loss: tensor(6.1609e-05)\n",
            "16380 Traning Loss: tensor(5.7092e-05)\n",
            "16381 Traning Loss: tensor(6.2911e-05)\n",
            "16382 Traning Loss: tensor(6.4783e-05)\n",
            "16383 Traning Loss: tensor(6.2376e-05)\n",
            "16384 Traning Loss: tensor(5.3765e-05)\n",
            "16385 Traning Loss: tensor(5.9067e-05)\n",
            "16386 Traning Loss: tensor(6.1935e-05)\n",
            "16387 Traning Loss: tensor(6.0648e-05)\n",
            "16388 Traning Loss: tensor(5.7822e-05)\n",
            "16389 Traning Loss: tensor(6.0014e-05)\n",
            "16390 Traning Loss: tensor(6.2021e-05)\n",
            "16391 Traning Loss: tensor(6.0600e-05)\n",
            "16392 Traning Loss: tensor(5.8486e-05)\n",
            "16393 Traning Loss: tensor(6.2866e-05)\n",
            "16394 Traning Loss: tensor(5.5693e-05)\n",
            "16395 Traning Loss: tensor(6.0180e-05)\n",
            "16396 Traning Loss: tensor(0.0001)\n",
            "16397 Traning Loss: tensor(5.7066e-05)\n",
            "16398 Traning Loss: tensor(5.3064e-05)\n",
            "16399 Traning Loss: tensor(6.1846e-05)\n",
            "16400 Traning Loss: tensor(5.9285e-05)\n",
            "16401 Traning Loss: tensor(5.9647e-05)\n",
            "16402 Traning Loss: tensor(9.9754e-05)\n",
            "16403 Traning Loss: tensor(9.7469e-05)\n",
            "16404 Traning Loss: tensor(6.1306e-05)\n",
            "16405 Traning Loss: tensor(0.0001)\n",
            "16406 Traning Loss: tensor(6.1624e-05)\n",
            "16407 Traning Loss: tensor(5.7980e-05)\n",
            "16408 Traning Loss: tensor(5.8794e-05)\n",
            "16409 Traning Loss: tensor(5.4889e-05)\n",
            "16410 Traning Loss: tensor(5.7255e-05)\n",
            "16411 Traning Loss: tensor(6.3606e-05)\n",
            "16412 Traning Loss: tensor(6.2670e-05)\n",
            "16413 Traning Loss: tensor(5.8084e-05)\n",
            "16414 Traning Loss: tensor(6.5921e-05)\n",
            "16415 Traning Loss: tensor(5.9606e-05)\n",
            "16416 Traning Loss: tensor(5.6978e-05)\n",
            "16417 Traning Loss: tensor(8.7265e-05)\n",
            "16418 Traning Loss: tensor(5.6693e-05)\n",
            "16419 Traning Loss: tensor(6.1449e-05)\n",
            "16420 Traning Loss: tensor(5.5427e-05)\n",
            "16421 Traning Loss: tensor(6.0512e-05)\n",
            "16422 Traning Loss: tensor(5.9601e-05)\n",
            "16423 Traning Loss: tensor(5.7062e-05)\n",
            "16424 Traning Loss: tensor(6.0826e-05)\n",
            "16425 Traning Loss: tensor(5.9519e-05)\n",
            "16426 Traning Loss: tensor(5.8756e-05)\n",
            "16427 Traning Loss: tensor(8.3600e-05)\n",
            "16428 Traning Loss: tensor(5.5849e-05)\n",
            "16429 Traning Loss: tensor(6.1123e-05)\n",
            "16430 Traning Loss: tensor(5.7233e-05)\n",
            "16431 Traning Loss: tensor(6.8052e-05)\n",
            "16432 Traning Loss: tensor(6.2107e-05)\n",
            "16433 Traning Loss: tensor(6.6071e-05)\n",
            "16434 Traning Loss: tensor(5.5535e-05)\n",
            "16435 Traning Loss: tensor(6.2769e-05)\n",
            "16436 Traning Loss: tensor(5.3564e-05)\n",
            "16437 Traning Loss: tensor(6.2933e-05)\n",
            "16438 Traning Loss: tensor(6.0206e-05)\n",
            "16439 Traning Loss: tensor(5.8679e-05)\n",
            "16440 Traning Loss: tensor(6.0271e-05)\n",
            "16441 Traning Loss: tensor(5.3892e-05)\n",
            "16442 Traning Loss: tensor(5.9124e-05)\n",
            "16443 Traning Loss: tensor(5.8961e-05)\n",
            "16444 Traning Loss: tensor(6.1911e-05)\n",
            "16445 Traning Loss: tensor(5.9302e-05)\n",
            "16446 Traning Loss: tensor(5.9306e-05)\n",
            "16447 Traning Loss: tensor(5.9807e-05)\n",
            "16448 Traning Loss: tensor(5.8498e-05)\n",
            "16449 Traning Loss: tensor(6.0786e-05)\n",
            "16450 Traning Loss: tensor(5.9799e-05)\n",
            "16451 Traning Loss: tensor(5.8607e-05)\n",
            "16452 Traning Loss: tensor(5.6771e-05)\n",
            "16453 Traning Loss: tensor(6.3960e-05)\n",
            "16454 Traning Loss: tensor(6.2963e-05)\n",
            "16455 Traning Loss: tensor(6.1333e-05)\n",
            "16456 Traning Loss: tensor(6.0215e-05)\n",
            "16457 Traning Loss: tensor(5.6392e-05)\n",
            "16458 Traning Loss: tensor(6.8634e-05)\n",
            "16459 Traning Loss: tensor(6.0175e-05)\n",
            "16460 Traning Loss: tensor(0.0001)\n",
            "16461 Traning Loss: tensor(6.5342e-05)\n",
            "16462 Traning Loss: tensor(5.4079e-05)\n",
            "16463 Traning Loss: tensor(7.5722e-05)\n",
            "16464 Traning Loss: tensor(5.6149e-05)\n",
            "16465 Traning Loss: tensor(5.7409e-05)\n",
            "16466 Traning Loss: tensor(5.5886e-05)\n",
            "16467 Traning Loss: tensor(5.4683e-05)\n",
            "16468 Traning Loss: tensor(5.5731e-05)\n",
            "16469 Traning Loss: tensor(5.8471e-05)\n",
            "16470 Traning Loss: tensor(5.3997e-05)\n",
            "16471 Traning Loss: tensor(5.4654e-05)\n",
            "16472 Traning Loss: tensor(5.7075e-05)\n",
            "16473 Traning Loss: tensor(5.5437e-05)\n",
            "16474 Traning Loss: tensor(5.4948e-05)\n",
            "16475 Traning Loss: tensor(8.8365e-05)\n",
            "16476 Traning Loss: tensor(5.9648e-05)\n",
            "16477 Traning Loss: tensor(5.8038e-05)\n",
            "16478 Traning Loss: tensor(9.2010e-05)\n",
            "16479 Traning Loss: tensor(5.6170e-05)\n",
            "16480 Traning Loss: tensor(5.2043e-05)\n",
            "16481 Traning Loss: tensor(5.8732e-05)\n",
            "16482 Traning Loss: tensor(5.2199e-05)\n",
            "16483 Traning Loss: tensor(6.4686e-05)\n",
            "16484 Traning Loss: tensor(6.0888e-05)\n",
            "16485 Traning Loss: tensor(5.5919e-05)\n",
            "16486 Traning Loss: tensor(5.8651e-05)\n",
            "16487 Traning Loss: tensor(0.0001)\n",
            "16488 Traning Loss: tensor(7.7172e-05)\n",
            "16489 Traning Loss: tensor(5.4135e-05)\n",
            "16490 Traning Loss: tensor(6.9627e-05)\n",
            "16491 Traning Loss: tensor(6.2565e-05)\n",
            "16492 Traning Loss: tensor(6.2769e-05)\n",
            "16493 Traning Loss: tensor(6.0840e-05)\n",
            "16494 Traning Loss: tensor(6.0632e-05)\n",
            "16495 Traning Loss: tensor(5.8296e-05)\n",
            "16496 Traning Loss: tensor(5.8082e-05)\n",
            "16497 Traning Loss: tensor(5.5113e-05)\n",
            "16498 Traning Loss: tensor(5.5649e-05)\n",
            "16499 Traning Loss: tensor(5.9744e-05)\n",
            "16500 Traning Loss: tensor(5.4208e-05)\n",
            "16501 Traning Loss: tensor(5.8317e-05)\n",
            "16502 Traning Loss: tensor(7.0020e-05)\n",
            "16503 Traning Loss: tensor(5.5262e-05)\n",
            "16504 Traning Loss: tensor(5.4418e-05)\n",
            "16505 Traning Loss: tensor(5.5785e-05)\n",
            "16506 Traning Loss: tensor(6.3153e-05)\n",
            "16507 Traning Loss: tensor(6.3420e-05)\n",
            "16508 Traning Loss: tensor(6.0063e-05)\n",
            "16509 Traning Loss: tensor(5.6871e-05)\n",
            "16510 Traning Loss: tensor(5.5760e-05)\n",
            "16511 Traning Loss: tensor(6.1730e-05)\n",
            "16512 Traning Loss: tensor(8.0511e-05)\n",
            "16513 Traning Loss: tensor(5.7854e-05)\n",
            "16514 Traning Loss: tensor(5.8166e-05)\n",
            "16515 Traning Loss: tensor(5.8149e-05)\n",
            "16516 Traning Loss: tensor(5.9141e-05)\n",
            "16517 Traning Loss: tensor(6.6605e-05)\n",
            "16518 Traning Loss: tensor(5.3313e-05)\n",
            "16519 Traning Loss: tensor(5.2605e-05)\n",
            "16520 Traning Loss: tensor(5.1434e-05)\n",
            "16521 Traning Loss: tensor(6.0877e-05)\n",
            "16522 Traning Loss: tensor(5.2563e-05)\n",
            "16523 Traning Loss: tensor(5.7731e-05)\n",
            "16524 Traning Loss: tensor(7.5708e-05)\n",
            "16525 Traning Loss: tensor(5.6186e-05)\n",
            "16526 Traning Loss: tensor(5.9049e-05)\n",
            "16527 Traning Loss: tensor(5.4806e-05)\n",
            "16528 Traning Loss: tensor(5.4208e-05)\n",
            "16529 Traning Loss: tensor(5.2814e-05)\n",
            "16530 Traning Loss: tensor(5.1582e-05)\n",
            "16531 Traning Loss: tensor(5.2586e-05)\n",
            "16532 Traning Loss: tensor(5.4793e-05)\n",
            "16533 Traning Loss: tensor(5.5171e-05)\n",
            "16534 Traning Loss: tensor(5.5049e-05)\n",
            "16535 Traning Loss: tensor(5.3665e-05)\n",
            "16536 Traning Loss: tensor(6.0388e-05)\n",
            "16537 Traning Loss: tensor(5.7239e-05)\n",
            "16538 Traning Loss: tensor(5.8275e-05)\n",
            "16539 Traning Loss: tensor(5.5148e-05)\n",
            "16540 Traning Loss: tensor(7.7409e-05)\n",
            "16541 Traning Loss: tensor(5.8114e-05)\n",
            "16542 Traning Loss: tensor(5.3284e-05)\n",
            "16543 Traning Loss: tensor(5.6073e-05)\n",
            "16544 Traning Loss: tensor(5.9606e-05)\n",
            "16545 Traning Loss: tensor(5.4257e-05)\n",
            "16546 Traning Loss: tensor(5.7799e-05)\n",
            "16547 Traning Loss: tensor(5.8543e-05)\n",
            "16548 Traning Loss: tensor(5.7243e-05)\n",
            "16549 Traning Loss: tensor(6.1349e-05)\n",
            "16550 Traning Loss: tensor(5.7205e-05)\n",
            "16551 Traning Loss: tensor(5.4689e-05)\n",
            "16552 Traning Loss: tensor(5.2051e-05)\n",
            "16553 Traning Loss: tensor(5.4593e-05)\n",
            "16554 Traning Loss: tensor(5.2777e-05)\n",
            "16555 Traning Loss: tensor(5.6280e-05)\n",
            "16556 Traning Loss: tensor(6.3931e-05)\n",
            "16557 Traning Loss: tensor(8.4794e-05)\n",
            "16558 Traning Loss: tensor(5.4342e-05)\n",
            "16559 Traning Loss: tensor(5.4134e-05)\n",
            "16560 Traning Loss: tensor(5.4710e-05)\n",
            "16561 Traning Loss: tensor(9.6512e-05)\n",
            "16562 Traning Loss: tensor(5.7267e-05)\n",
            "16563 Traning Loss: tensor(5.7214e-05)\n",
            "16564 Traning Loss: tensor(5.7840e-05)\n",
            "16565 Traning Loss: tensor(5.5294e-05)\n",
            "16566 Traning Loss: tensor(5.4090e-05)\n",
            "16567 Traning Loss: tensor(0.0002)\n",
            "16568 Traning Loss: tensor(6.0791e-05)\n",
            "16569 Traning Loss: tensor(5.1602e-05)\n",
            "16570 Traning Loss: tensor(8.4686e-05)\n",
            "16571 Traning Loss: tensor(5.7416e-05)\n",
            "16572 Traning Loss: tensor(6.0051e-05)\n",
            "16573 Traning Loss: tensor(9.8886e-05)\n",
            "16574 Traning Loss: tensor(5.7691e-05)\n",
            "16575 Traning Loss: tensor(6.2542e-05)\n",
            "16576 Traning Loss: tensor(6.1527e-05)\n",
            "16577 Traning Loss: tensor(5.9311e-05)\n",
            "16578 Traning Loss: tensor(5.9756e-05)\n",
            "16579 Traning Loss: tensor(9.5457e-05)\n",
            "16580 Traning Loss: tensor(5.7672e-05)\n",
            "16581 Traning Loss: tensor(6.0679e-05)\n",
            "16582 Traning Loss: tensor(5.1122e-05)\n",
            "16583 Traning Loss: tensor(5.9618e-05)\n",
            "16584 Traning Loss: tensor(6.4011e-05)\n",
            "16585 Traning Loss: tensor(5.7448e-05)\n",
            "16586 Traning Loss: tensor(5.7817e-05)\n",
            "16587 Traning Loss: tensor(6.0272e-05)\n",
            "16588 Traning Loss: tensor(5.5973e-05)\n",
            "16589 Traning Loss: tensor(5.5010e-05)\n",
            "16590 Traning Loss: tensor(5.9665e-05)\n",
            "16591 Traning Loss: tensor(5.3285e-05)\n",
            "16592 Traning Loss: tensor(6.0339e-05)\n",
            "16593 Traning Loss: tensor(5.9123e-05)\n",
            "16594 Traning Loss: tensor(5.7362e-05)\n",
            "16595 Traning Loss: tensor(5.4955e-05)\n",
            "16596 Traning Loss: tensor(5.8325e-05)\n",
            "16597 Traning Loss: tensor(5.9641e-05)\n",
            "16598 Traning Loss: tensor(5.4439e-05)\n",
            "16599 Traning Loss: tensor(5.2283e-05)\n",
            "16600 Traning Loss: tensor(6.0113e-05)\n",
            "16601 Traning Loss: tensor(5.2758e-05)\n",
            "16602 Traning Loss: tensor(6.2705e-05)\n",
            "16603 Traning Loss: tensor(5.4233e-05)\n",
            "16604 Traning Loss: tensor(5.2904e-05)\n",
            "16605 Traning Loss: tensor(5.9378e-05)\n",
            "16606 Traning Loss: tensor(5.2637e-05)\n",
            "16607 Traning Loss: tensor(5.5417e-05)\n",
            "16608 Traning Loss: tensor(5.3849e-05)\n",
            "16609 Traning Loss: tensor(5.8072e-05)\n",
            "16610 Traning Loss: tensor(5.3588e-05)\n",
            "16611 Traning Loss: tensor(5.7265e-05)\n",
            "16612 Traning Loss: tensor(5.9735e-05)\n",
            "16613 Traning Loss: tensor(5.4564e-05)\n",
            "16614 Traning Loss: tensor(5.4486e-05)\n",
            "16615 Traning Loss: tensor(8.0658e-05)\n",
            "16616 Traning Loss: tensor(5.3005e-05)\n",
            "16617 Traning Loss: tensor(6.3680e-05)\n",
            "16618 Traning Loss: tensor(5.1726e-05)\n",
            "16619 Traning Loss: tensor(5.3810e-05)\n",
            "16620 Traning Loss: tensor(5.7004e-05)\n",
            "16621 Traning Loss: tensor(5.4516e-05)\n",
            "16622 Traning Loss: tensor(5.2626e-05)\n",
            "16623 Traning Loss: tensor(5.3437e-05)\n",
            "16624 Traning Loss: tensor(5.3812e-05)\n",
            "16625 Traning Loss: tensor(5.8841e-05)\n",
            "16626 Traning Loss: tensor(6.4425e-05)\n",
            "16627 Traning Loss: tensor(5.9001e-05)\n",
            "16628 Traning Loss: tensor(5.6351e-05)\n",
            "16629 Traning Loss: tensor(5.4071e-05)\n",
            "16630 Traning Loss: tensor(5.3776e-05)\n",
            "16631 Traning Loss: tensor(5.3629e-05)\n",
            "16632 Traning Loss: tensor(6.0600e-05)\n",
            "16633 Traning Loss: tensor(4.9862e-05)\n",
            "16634 Traning Loss: tensor(5.7469e-05)\n",
            "16635 Traning Loss: tensor(4.8049e-05)\n",
            "16636 Traning Loss: tensor(5.3697e-05)\n",
            "16637 Traning Loss: tensor(5.4425e-05)\n",
            "16638 Traning Loss: tensor(5.4495e-05)\n",
            "16639 Traning Loss: tensor(5.4904e-05)\n",
            "16640 Traning Loss: tensor(5.2239e-05)\n",
            "16641 Traning Loss: tensor(5.3847e-05)\n",
            "16642 Traning Loss: tensor(5.3631e-05)\n",
            "16643 Traning Loss: tensor(5.7909e-05)\n",
            "16644 Traning Loss: tensor(5.6925e-05)\n",
            "16645 Traning Loss: tensor(5.3302e-05)\n",
            "16646 Traning Loss: tensor(5.0904e-05)\n",
            "16647 Traning Loss: tensor(5.0661e-05)\n",
            "16648 Traning Loss: tensor(4.9352e-05)\n",
            "16649 Traning Loss: tensor(7.3827e-05)\n",
            "16650 Traning Loss: tensor(0.0001)\n",
            "16651 Traning Loss: tensor(5.9063e-05)\n",
            "16652 Traning Loss: tensor(5.6609e-05)\n",
            "16653 Traning Loss: tensor(6.1175e-05)\n",
            "16654 Traning Loss: tensor(5.9415e-05)\n",
            "16655 Traning Loss: tensor(5.8248e-05)\n",
            "16656 Traning Loss: tensor(0.0001)\n",
            "16657 Traning Loss: tensor(6.0830e-05)\n",
            "16658 Traning Loss: tensor(5.4991e-05)\n",
            "16659 Traning Loss: tensor(7.5277e-05)\n",
            "16660 Traning Loss: tensor(5.8055e-05)\n",
            "16661 Traning Loss: tensor(5.5201e-05)\n",
            "16662 Traning Loss: tensor(5.9237e-05)\n",
            "16663 Traning Loss: tensor(5.4172e-05)\n",
            "16664 Traning Loss: tensor(5.5027e-05)\n",
            "16665 Traning Loss: tensor(5.3166e-05)\n",
            "16666 Traning Loss: tensor(5.6069e-05)\n",
            "16667 Traning Loss: tensor(5.5332e-05)\n",
            "16668 Traning Loss: tensor(5.2253e-05)\n",
            "16669 Traning Loss: tensor(9.5641e-05)\n",
            "16670 Traning Loss: tensor(6.5517e-05)\n",
            "16671 Traning Loss: tensor(0.0001)\n",
            "16672 Traning Loss: tensor(6.0996e-05)\n",
            "16673 Traning Loss: tensor(6.6720e-05)\n",
            "16674 Traning Loss: tensor(5.8189e-05)\n",
            "16675 Traning Loss: tensor(5.4007e-05)\n",
            "16676 Traning Loss: tensor(5.7562e-05)\n",
            "16677 Traning Loss: tensor(5.9932e-05)\n",
            "16678 Traning Loss: tensor(5.0633e-05)\n",
            "16679 Traning Loss: tensor(7.2211e-05)\n",
            "16680 Traning Loss: tensor(7.0913e-05)\n",
            "16681 Traning Loss: tensor(5.5632e-05)\n",
            "16682 Traning Loss: tensor(5.9525e-05)\n",
            "16683 Traning Loss: tensor(5.7312e-05)\n",
            "16684 Traning Loss: tensor(8.0539e-05)\n",
            "16685 Traning Loss: tensor(5.6646e-05)\n",
            "16686 Traning Loss: tensor(5.2517e-05)\n",
            "16687 Traning Loss: tensor(6.6143e-05)\n",
            "16688 Traning Loss: tensor(6.2430e-05)\n",
            "16689 Traning Loss: tensor(5.5940e-05)\n",
            "16690 Traning Loss: tensor(5.8035e-05)\n",
            "16691 Traning Loss: tensor(5.8628e-05)\n",
            "16692 Traning Loss: tensor(5.4832e-05)\n",
            "16693 Traning Loss: tensor(5.4342e-05)\n",
            "16694 Traning Loss: tensor(5.2631e-05)\n",
            "16695 Traning Loss: tensor(5.7680e-05)\n",
            "16696 Traning Loss: tensor(5.7295e-05)\n",
            "16697 Traning Loss: tensor(5.4432e-05)\n",
            "16698 Traning Loss: tensor(5.4695e-05)\n",
            "16699 Traning Loss: tensor(5.3329e-05)\n",
            "16700 Traning Loss: tensor(5.4376e-05)\n",
            "16701 Traning Loss: tensor(5.3582e-05)\n",
            "16702 Traning Loss: tensor(5.3098e-05)\n",
            "16703 Traning Loss: tensor(5.3779e-05)\n",
            "16704 Traning Loss: tensor(5.2480e-05)\n",
            "16705 Traning Loss: tensor(5.3225e-05)\n",
            "16706 Traning Loss: tensor(5.7642e-05)\n",
            "16707 Traning Loss: tensor(5.1502e-05)\n",
            "16708 Traning Loss: tensor(6.7090e-05)\n",
            "16709 Traning Loss: tensor(5.1134e-05)\n",
            "16710 Traning Loss: tensor(5.3618e-05)\n",
            "16711 Traning Loss: tensor(5.3960e-05)\n",
            "16712 Traning Loss: tensor(5.1358e-05)\n",
            "16713 Traning Loss: tensor(5.1434e-05)\n",
            "16714 Traning Loss: tensor(5.5806e-05)\n",
            "16715 Traning Loss: tensor(8.4468e-05)\n",
            "16716 Traning Loss: tensor(5.2305e-05)\n",
            "16717 Traning Loss: tensor(7.8617e-05)\n",
            "16718 Traning Loss: tensor(5.0404e-05)\n",
            "16719 Traning Loss: tensor(5.2705e-05)\n",
            "16720 Traning Loss: tensor(5.0903e-05)\n",
            "16721 Traning Loss: tensor(5.5937e-05)\n",
            "16722 Traning Loss: tensor(4.8952e-05)\n",
            "16723 Traning Loss: tensor(5.4339e-05)\n",
            "16724 Traning Loss: tensor(4.9176e-05)\n",
            "16725 Traning Loss: tensor(5.7414e-05)\n",
            "16726 Traning Loss: tensor(5.2947e-05)\n",
            "16727 Traning Loss: tensor(6.4191e-05)\n",
            "16728 Traning Loss: tensor(5.0660e-05)\n",
            "16729 Traning Loss: tensor(5.5884e-05)\n",
            "16730 Traning Loss: tensor(4.9678e-05)\n",
            "16731 Traning Loss: tensor(5.3702e-05)\n",
            "16732 Traning Loss: tensor(5.1529e-05)\n",
            "16733 Traning Loss: tensor(4.9641e-05)\n",
            "16734 Traning Loss: tensor(5.1011e-05)\n",
            "16735 Traning Loss: tensor(8.6779e-05)\n",
            "16736 Traning Loss: tensor(6.5262e-05)\n",
            "16737 Traning Loss: tensor(5.3018e-05)\n",
            "16738 Traning Loss: tensor(5.0821e-05)\n",
            "16739 Traning Loss: tensor(4.9047e-05)\n",
            "16740 Traning Loss: tensor(5.1202e-05)\n",
            "16741 Traning Loss: tensor(4.9907e-05)\n",
            "16742 Traning Loss: tensor(4.8086e-05)\n",
            "16743 Traning Loss: tensor(5.7527e-05)\n",
            "16744 Traning Loss: tensor(5.2641e-05)\n",
            "16745 Traning Loss: tensor(5.3199e-05)\n",
            "16746 Traning Loss: tensor(5.3608e-05)\n",
            "16747 Traning Loss: tensor(5.6900e-05)\n",
            "16748 Traning Loss: tensor(4.9801e-05)\n",
            "16749 Traning Loss: tensor(5.1913e-05)\n",
            "16750 Traning Loss: tensor(5.2857e-05)\n",
            "16751 Traning Loss: tensor(5.4745e-05)\n",
            "16752 Traning Loss: tensor(5.0231e-05)\n",
            "16753 Traning Loss: tensor(0.0001)\n",
            "16754 Traning Loss: tensor(5.7989e-05)\n",
            "16755 Traning Loss: tensor(5.1936e-05)\n",
            "16756 Traning Loss: tensor(5.6401e-05)\n",
            "16757 Traning Loss: tensor(5.5696e-05)\n",
            "16758 Traning Loss: tensor(5.4492e-05)\n",
            "16759 Traning Loss: tensor(4.9997e-05)\n",
            "16760 Traning Loss: tensor(5.0999e-05)\n",
            "16761 Traning Loss: tensor(5.3016e-05)\n",
            "16762 Traning Loss: tensor(5.2347e-05)\n",
            "16763 Traning Loss: tensor(5.1392e-05)\n",
            "16764 Traning Loss: tensor(4.8998e-05)\n",
            "16765 Traning Loss: tensor(9.4390e-05)\n",
            "16766 Traning Loss: tensor(5.9051e-05)\n",
            "16767 Traning Loss: tensor(5.3036e-05)\n",
            "16768 Traning Loss: tensor(5.2196e-05)\n",
            "16769 Traning Loss: tensor(5.1421e-05)\n",
            "16770 Traning Loss: tensor(5.1286e-05)\n",
            "16771 Traning Loss: tensor(5.9744e-05)\n",
            "16772 Traning Loss: tensor(5.1815e-05)\n",
            "16773 Traning Loss: tensor(5.5258e-05)\n",
            "16774 Traning Loss: tensor(4.7994e-05)\n",
            "16775 Traning Loss: tensor(4.6980e-05)\n",
            "16776 Traning Loss: tensor(5.7546e-05)\n",
            "16777 Traning Loss: tensor(5.0810e-05)\n",
            "16778 Traning Loss: tensor(5.1100e-05)\n",
            "16779 Traning Loss: tensor(4.9093e-05)\n",
            "16780 Traning Loss: tensor(4.6476e-05)\n",
            "16781 Traning Loss: tensor(0.0001)\n",
            "16782 Traning Loss: tensor(5.2176e-05)\n",
            "16783 Traning Loss: tensor(4.7278e-05)\n",
            "16784 Traning Loss: tensor(6.5611e-05)\n",
            "16785 Traning Loss: tensor(5.9999e-05)\n",
            "16786 Traning Loss: tensor(5.6599e-05)\n",
            "16787 Traning Loss: tensor(6.0705e-05)\n",
            "16788 Traning Loss: tensor(5.1699e-05)\n",
            "16789 Traning Loss: tensor(5.2141e-05)\n",
            "16790 Traning Loss: tensor(5.0475e-05)\n",
            "16791 Traning Loss: tensor(5.1271e-05)\n",
            "16792 Traning Loss: tensor(5.1119e-05)\n",
            "16793 Traning Loss: tensor(5.1183e-05)\n",
            "16794 Traning Loss: tensor(4.8117e-05)\n",
            "16795 Traning Loss: tensor(4.6376e-05)\n",
            "16796 Traning Loss: tensor(5.0253e-05)\n",
            "16797 Traning Loss: tensor(4.9743e-05)\n",
            "16798 Traning Loss: tensor(5.1178e-05)\n",
            "16799 Traning Loss: tensor(5.3055e-05)\n",
            "16800 Traning Loss: tensor(4.8760e-05)\n",
            "16801 Traning Loss: tensor(4.7588e-05)\n",
            "16802 Traning Loss: tensor(4.7440e-05)\n",
            "16803 Traning Loss: tensor(5.5435e-05)\n",
            "16804 Traning Loss: tensor(4.8058e-05)\n",
            "16805 Traning Loss: tensor(6.7011e-05)\n",
            "16806 Traning Loss: tensor(5.4163e-05)\n",
            "16807 Traning Loss: tensor(5.0451e-05)\n",
            "16808 Traning Loss: tensor(4.5550e-05)\n",
            "16809 Traning Loss: tensor(5.4971e-05)\n",
            "16810 Traning Loss: tensor(6.0969e-05)\n",
            "16811 Traning Loss: tensor(4.8459e-05)\n",
            "16812 Traning Loss: tensor(5.0286e-05)\n",
            "16813 Traning Loss: tensor(4.8905e-05)\n",
            "16814 Traning Loss: tensor(5.4168e-05)\n",
            "16815 Traning Loss: tensor(4.8110e-05)\n",
            "16816 Traning Loss: tensor(4.8709e-05)\n",
            "16817 Traning Loss: tensor(5.0827e-05)\n",
            "16818 Traning Loss: tensor(5.0861e-05)\n",
            "16819 Traning Loss: tensor(5.3122e-05)\n",
            "16820 Traning Loss: tensor(5.0114e-05)\n",
            "16821 Traning Loss: tensor(5.0096e-05)\n",
            "16822 Traning Loss: tensor(5.0981e-05)\n",
            "16823 Traning Loss: tensor(4.7506e-05)\n",
            "16824 Traning Loss: tensor(4.9862e-05)\n",
            "16825 Traning Loss: tensor(4.8367e-05)\n",
            "16826 Traning Loss: tensor(4.9685e-05)\n",
            "16827 Traning Loss: tensor(4.6543e-05)\n",
            "16828 Traning Loss: tensor(6.3714e-05)\n",
            "16829 Traning Loss: tensor(5.4815e-05)\n",
            "16830 Traning Loss: tensor(5.5865e-05)\n",
            "16831 Traning Loss: tensor(5.3406e-05)\n",
            "16832 Traning Loss: tensor(5.0710e-05)\n",
            "16833 Traning Loss: tensor(5.0122e-05)\n",
            "16834 Traning Loss: tensor(4.7833e-05)\n",
            "16835 Traning Loss: tensor(4.7898e-05)\n",
            "16836 Traning Loss: tensor(5.3424e-05)\n",
            "16837 Traning Loss: tensor(4.8900e-05)\n",
            "16838 Traning Loss: tensor(5.2748e-05)\n",
            "16839 Traning Loss: tensor(4.9343e-05)\n",
            "16840 Traning Loss: tensor(4.8449e-05)\n",
            "16841 Traning Loss: tensor(5.9956e-05)\n",
            "16842 Traning Loss: tensor(5.0161e-05)\n",
            "16843 Traning Loss: tensor(5.0613e-05)\n",
            "16844 Traning Loss: tensor(4.7376e-05)\n",
            "16845 Traning Loss: tensor(4.9198e-05)\n",
            "16846 Traning Loss: tensor(4.7287e-05)\n",
            "16847 Traning Loss: tensor(5.0983e-05)\n",
            "16848 Traning Loss: tensor(4.8276e-05)\n",
            "16849 Traning Loss: tensor(4.8760e-05)\n",
            "16850 Traning Loss: tensor(4.8988e-05)\n",
            "16851 Traning Loss: tensor(4.9641e-05)\n",
            "16852 Traning Loss: tensor(4.5198e-05)\n",
            "16853 Traning Loss: tensor(5.1334e-05)\n",
            "16854 Traning Loss: tensor(5.0814e-05)\n",
            "16855 Traning Loss: tensor(4.7737e-05)\n",
            "16856 Traning Loss: tensor(4.6070e-05)\n",
            "16857 Traning Loss: tensor(4.4578e-05)\n",
            "16858 Traning Loss: tensor(6.6876e-05)\n",
            "16859 Traning Loss: tensor(5.3233e-05)\n",
            "16860 Traning Loss: tensor(5.6276e-05)\n",
            "16861 Traning Loss: tensor(4.7616e-05)\n",
            "16862 Traning Loss: tensor(7.1265e-05)\n",
            "16863 Traning Loss: tensor(5.5380e-05)\n",
            "16864 Traning Loss: tensor(4.7678e-05)\n",
            "16865 Traning Loss: tensor(5.0832e-05)\n",
            "16866 Traning Loss: tensor(4.8204e-05)\n",
            "16867 Traning Loss: tensor(5.2333e-05)\n",
            "16868 Traning Loss: tensor(5.1329e-05)\n",
            "16869 Traning Loss: tensor(4.8210e-05)\n",
            "16870 Traning Loss: tensor(7.1406e-05)\n",
            "16871 Traning Loss: tensor(5.5594e-05)\n",
            "16872 Traning Loss: tensor(5.0473e-05)\n",
            "16873 Traning Loss: tensor(5.1468e-05)\n",
            "16874 Traning Loss: tensor(5.2243e-05)\n",
            "16875 Traning Loss: tensor(4.7504e-05)\n",
            "16876 Traning Loss: tensor(4.9686e-05)\n",
            "16877 Traning Loss: tensor(5.4784e-05)\n",
            "16878 Traning Loss: tensor(8.2885e-05)\n",
            "16879 Traning Loss: tensor(4.4744e-05)\n",
            "16880 Traning Loss: tensor(4.9629e-05)\n",
            "16881 Traning Loss: tensor(4.9470e-05)\n",
            "16882 Traning Loss: tensor(4.8465e-05)\n",
            "16883 Traning Loss: tensor(5.4619e-05)\n",
            "16884 Traning Loss: tensor(4.8162e-05)\n",
            "16885 Traning Loss: tensor(4.6483e-05)\n",
            "16886 Traning Loss: tensor(0.0001)\n",
            "16887 Traning Loss: tensor(5.8876e-05)\n",
            "16888 Traning Loss: tensor(5.0120e-05)\n",
            "16889 Traning Loss: tensor(6.1379e-05)\n",
            "16890 Traning Loss: tensor(5.6275e-05)\n",
            "16891 Traning Loss: tensor(4.8652e-05)\n",
            "16892 Traning Loss: tensor(4.6379e-05)\n",
            "16893 Traning Loss: tensor(5.7965e-05)\n",
            "16894 Traning Loss: tensor(4.4088e-05)\n",
            "16895 Traning Loss: tensor(4.8304e-05)\n",
            "16896 Traning Loss: tensor(6.0925e-05)\n",
            "16897 Traning Loss: tensor(4.8887e-05)\n",
            "16898 Traning Loss: tensor(4.7952e-05)\n",
            "16899 Traning Loss: tensor(4.9121e-05)\n",
            "16900 Traning Loss: tensor(5.8087e-05)\n",
            "16901 Traning Loss: tensor(4.6000e-05)\n",
            "16902 Traning Loss: tensor(5.4175e-05)\n",
            "16903 Traning Loss: tensor(4.9562e-05)\n",
            "16904 Traning Loss: tensor(4.6666e-05)\n",
            "16905 Traning Loss: tensor(4.7290e-05)\n",
            "16906 Traning Loss: tensor(5.1923e-05)\n",
            "16907 Traning Loss: tensor(5.4282e-05)\n",
            "16908 Traning Loss: tensor(6.5443e-05)\n",
            "16909 Traning Loss: tensor(5.3361e-05)\n",
            "16910 Traning Loss: tensor(4.5581e-05)\n",
            "16911 Traning Loss: tensor(4.9428e-05)\n",
            "16912 Traning Loss: tensor(4.9758e-05)\n",
            "16913 Traning Loss: tensor(4.7777e-05)\n",
            "16914 Traning Loss: tensor(4.8896e-05)\n",
            "16915 Traning Loss: tensor(4.7506e-05)\n",
            "16916 Traning Loss: tensor(5.3970e-05)\n",
            "16917 Traning Loss: tensor(5.2488e-05)\n",
            "16918 Traning Loss: tensor(4.9073e-05)\n",
            "16919 Traning Loss: tensor(4.6965e-05)\n",
            "16920 Traning Loss: tensor(4.4408e-05)\n",
            "16921 Traning Loss: tensor(5.1486e-05)\n",
            "16922 Traning Loss: tensor(4.9144e-05)\n",
            "16923 Traning Loss: tensor(4.7641e-05)\n",
            "16924 Traning Loss: tensor(4.6826e-05)\n",
            "16925 Traning Loss: tensor(5.0151e-05)\n",
            "16926 Traning Loss: tensor(4.7238e-05)\n",
            "16927 Traning Loss: tensor(5.3751e-05)\n",
            "16928 Traning Loss: tensor(4.5206e-05)\n",
            "16929 Traning Loss: tensor(4.8405e-05)\n",
            "16930 Traning Loss: tensor(4.4393e-05)\n",
            "16931 Traning Loss: tensor(6.6809e-05)\n",
            "16932 Traning Loss: tensor(5.5783e-05)\n",
            "16933 Traning Loss: tensor(4.9420e-05)\n",
            "16934 Traning Loss: tensor(5.3189e-05)\n",
            "16935 Traning Loss: tensor(4.9991e-05)\n",
            "16936 Traning Loss: tensor(5.3505e-05)\n",
            "16937 Traning Loss: tensor(4.8947e-05)\n",
            "16938 Traning Loss: tensor(5.0495e-05)\n",
            "16939 Traning Loss: tensor(4.6446e-05)\n",
            "16940 Traning Loss: tensor(4.8596e-05)\n",
            "16941 Traning Loss: tensor(4.7531e-05)\n",
            "16942 Traning Loss: tensor(4.3742e-05)\n",
            "16943 Traning Loss: tensor(4.9607e-05)\n",
            "16944 Traning Loss: tensor(5.0972e-05)\n",
            "16945 Traning Loss: tensor(4.6039e-05)\n",
            "16946 Traning Loss: tensor(5.0327e-05)\n",
            "16947 Traning Loss: tensor(4.7672e-05)\n",
            "16948 Traning Loss: tensor(4.8277e-05)\n",
            "16949 Traning Loss: tensor(4.8660e-05)\n",
            "16950 Traning Loss: tensor(4.5592e-05)\n",
            "16951 Traning Loss: tensor(4.6271e-05)\n",
            "16952 Traning Loss: tensor(4.4907e-05)\n",
            "16953 Traning Loss: tensor(4.6943e-05)\n",
            "16954 Traning Loss: tensor(4.9993e-05)\n",
            "16955 Traning Loss: tensor(4.6660e-05)\n",
            "16956 Traning Loss: tensor(4.5146e-05)\n",
            "16957 Traning Loss: tensor(4.6991e-05)\n",
            "16958 Traning Loss: tensor(4.3885e-05)\n",
            "16959 Traning Loss: tensor(4.6251e-05)\n",
            "16960 Traning Loss: tensor(4.4911e-05)\n",
            "16961 Traning Loss: tensor(4.6076e-05)\n",
            "16962 Traning Loss: tensor(4.7195e-05)\n",
            "16963 Traning Loss: tensor(9.3250e-05)\n",
            "16964 Traning Loss: tensor(4.6956e-05)\n",
            "16965 Traning Loss: tensor(4.7337e-05)\n",
            "16966 Traning Loss: tensor(4.8410e-05)\n",
            "16967 Traning Loss: tensor(5.4552e-05)\n",
            "16968 Traning Loss: tensor(4.5972e-05)\n",
            "16969 Traning Loss: tensor(5.3665e-05)\n",
            "16970 Traning Loss: tensor(7.8892e-05)\n",
            "16971 Traning Loss: tensor(5.4929e-05)\n",
            "16972 Traning Loss: tensor(6.6526e-05)\n",
            "16973 Traning Loss: tensor(5.4851e-05)\n",
            "16974 Traning Loss: tensor(5.3993e-05)\n",
            "16975 Traning Loss: tensor(5.0796e-05)\n",
            "16976 Traning Loss: tensor(4.5126e-05)\n",
            "16977 Traning Loss: tensor(4.6072e-05)\n",
            "16978 Traning Loss: tensor(4.8347e-05)\n",
            "16979 Traning Loss: tensor(5.0926e-05)\n",
            "16980 Traning Loss: tensor(4.6467e-05)\n",
            "16981 Traning Loss: tensor(5.0487e-05)\n",
            "16982 Traning Loss: tensor(4.8376e-05)\n",
            "16983 Traning Loss: tensor(4.4557e-05)\n",
            "16984 Traning Loss: tensor(4.9768e-05)\n",
            "16985 Traning Loss: tensor(4.7127e-05)\n",
            "16986 Traning Loss: tensor(4.5717e-05)\n",
            "16987 Traning Loss: tensor(5.6881e-05)\n",
            "16988 Traning Loss: tensor(4.5770e-05)\n",
            "16989 Traning Loss: tensor(4.9748e-05)\n",
            "16990 Traning Loss: tensor(4.8563e-05)\n",
            "16991 Traning Loss: tensor(6.3211e-05)\n",
            "16992 Traning Loss: tensor(4.7611e-05)\n",
            "16993 Traning Loss: tensor(4.3134e-05)\n",
            "16994 Traning Loss: tensor(4.2689e-05)\n",
            "16995 Traning Loss: tensor(4.5157e-05)\n",
            "16996 Traning Loss: tensor(4.7250e-05)\n",
            "16997 Traning Loss: tensor(4.7393e-05)\n",
            "16998 Traning Loss: tensor(4.5770e-05)\n",
            "16999 Traning Loss: tensor(4.6462e-05)\n",
            "17000 Traning Loss: tensor(4.3432e-05)\n",
            "17001 Traning Loss: tensor(4.4279e-05)\n",
            "17002 Traning Loss: tensor(4.5019e-05)\n",
            "17003 Traning Loss: tensor(4.5736e-05)\n",
            "17004 Traning Loss: tensor(4.2751e-05)\n",
            "17005 Traning Loss: tensor(4.6139e-05)\n",
            "17006 Traning Loss: tensor(4.6637e-05)\n",
            "17007 Traning Loss: tensor(5.0362e-05)\n",
            "17008 Traning Loss: tensor(4.2764e-05)\n",
            "17009 Traning Loss: tensor(4.2806e-05)\n",
            "17010 Traning Loss: tensor(5.0351e-05)\n",
            "17011 Traning Loss: tensor(5.0051e-05)\n",
            "17012 Traning Loss: tensor(4.7653e-05)\n",
            "17013 Traning Loss: tensor(4.5031e-05)\n",
            "17014 Traning Loss: tensor(4.4675e-05)\n",
            "17015 Traning Loss: tensor(4.6109e-05)\n",
            "17016 Traning Loss: tensor(4.7892e-05)\n",
            "17017 Traning Loss: tensor(4.6105e-05)\n",
            "17018 Traning Loss: tensor(7.1787e-05)\n",
            "17019 Traning Loss: tensor(4.8812e-05)\n",
            "17020 Traning Loss: tensor(4.5927e-05)\n",
            "17021 Traning Loss: tensor(7.0203e-05)\n",
            "17022 Traning Loss: tensor(5.0655e-05)\n",
            "17023 Traning Loss: tensor(5.2702e-05)\n",
            "17024 Traning Loss: tensor(4.9187e-05)\n",
            "17025 Traning Loss: tensor(4.4203e-05)\n",
            "17026 Traning Loss: tensor(4.8619e-05)\n",
            "17027 Traning Loss: tensor(6.8941e-05)\n",
            "17028 Traning Loss: tensor(4.6842e-05)\n",
            "17029 Traning Loss: tensor(4.5097e-05)\n",
            "17030 Traning Loss: tensor(4.8356e-05)\n",
            "17031 Traning Loss: tensor(4.2690e-05)\n",
            "17032 Traning Loss: tensor(4.4129e-05)\n",
            "17033 Traning Loss: tensor(8.7165e-05)\n",
            "17034 Traning Loss: tensor(4.5676e-05)\n",
            "17035 Traning Loss: tensor(4.9958e-05)\n",
            "17036 Traning Loss: tensor(4.5017e-05)\n",
            "17037 Traning Loss: tensor(5.0451e-05)\n",
            "17038 Traning Loss: tensor(4.4281e-05)\n",
            "17039 Traning Loss: tensor(4.6901e-05)\n",
            "17040 Traning Loss: tensor(4.3418e-05)\n",
            "17041 Traning Loss: tensor(5.6770e-05)\n",
            "17042 Traning Loss: tensor(4.4419e-05)\n",
            "17043 Traning Loss: tensor(4.3756e-05)\n",
            "17044 Traning Loss: tensor(4.7269e-05)\n",
            "17045 Traning Loss: tensor(4.4251e-05)\n",
            "17046 Traning Loss: tensor(4.7679e-05)\n",
            "17047 Traning Loss: tensor(5.3494e-05)\n",
            "17048 Traning Loss: tensor(4.6725e-05)\n",
            "17049 Traning Loss: tensor(4.6886e-05)\n",
            "17050 Traning Loss: tensor(4.1454e-05)\n",
            "17051 Traning Loss: tensor(4.2384e-05)\n",
            "17052 Traning Loss: tensor(4.4298e-05)\n",
            "17053 Traning Loss: tensor(5.1054e-05)\n",
            "17054 Traning Loss: tensor(4.4745e-05)\n",
            "17055 Traning Loss: tensor(4.2591e-05)\n",
            "17056 Traning Loss: tensor(4.3295e-05)\n",
            "17057 Traning Loss: tensor(4.2324e-05)\n",
            "17058 Traning Loss: tensor(4.2110e-05)\n",
            "17059 Traning Loss: tensor(4.6778e-05)\n",
            "17060 Traning Loss: tensor(4.1982e-05)\n",
            "17061 Traning Loss: tensor(4.4453e-05)\n",
            "17062 Traning Loss: tensor(0.0001)\n",
            "17063 Traning Loss: tensor(5.6607e-05)\n",
            "17064 Traning Loss: tensor(5.8084e-05)\n",
            "17065 Traning Loss: tensor(4.7566e-05)\n",
            "17066 Traning Loss: tensor(5.4289e-05)\n",
            "17067 Traning Loss: tensor(4.6329e-05)\n",
            "17068 Traning Loss: tensor(4.5211e-05)\n",
            "17069 Traning Loss: tensor(4.6678e-05)\n",
            "17070 Traning Loss: tensor(4.3364e-05)\n",
            "17071 Traning Loss: tensor(4.4401e-05)\n",
            "17072 Traning Loss: tensor(6.5560e-05)\n",
            "17073 Traning Loss: tensor(4.6180e-05)\n",
            "17074 Traning Loss: tensor(5.3333e-05)\n",
            "17075 Traning Loss: tensor(4.4530e-05)\n",
            "17076 Traning Loss: tensor(4.9099e-05)\n",
            "17077 Traning Loss: tensor(4.9230e-05)\n",
            "17078 Traning Loss: tensor(4.3915e-05)\n",
            "17079 Traning Loss: tensor(7.8311e-05)\n",
            "17080 Traning Loss: tensor(4.5475e-05)\n",
            "17081 Traning Loss: tensor(8.3334e-05)\n",
            "17082 Traning Loss: tensor(5.1737e-05)\n",
            "17083 Traning Loss: tensor(4.4223e-05)\n",
            "17084 Traning Loss: tensor(4.6899e-05)\n",
            "17085 Traning Loss: tensor(4.9009e-05)\n",
            "17086 Traning Loss: tensor(4.6148e-05)\n",
            "17087 Traning Loss: tensor(4.6442e-05)\n",
            "17088 Traning Loss: tensor(5.2309e-05)\n",
            "17089 Traning Loss: tensor(4.6623e-05)\n",
            "17090 Traning Loss: tensor(4.4165e-05)\n",
            "17091 Traning Loss: tensor(4.6445e-05)\n",
            "17092 Traning Loss: tensor(4.4858e-05)\n",
            "17093 Traning Loss: tensor(4.6253e-05)\n",
            "17094 Traning Loss: tensor(6.2818e-05)\n",
            "17095 Traning Loss: tensor(4.9307e-05)\n",
            "17096 Traning Loss: tensor(4.8183e-05)\n",
            "17097 Traning Loss: tensor(5.6583e-05)\n",
            "17098 Traning Loss: tensor(5.2147e-05)\n",
            "17099 Traning Loss: tensor(5.6302e-05)\n",
            "17100 Traning Loss: tensor(4.5467e-05)\n",
            "17101 Traning Loss: tensor(4.9010e-05)\n",
            "17102 Traning Loss: tensor(5.2025e-05)\n",
            "17103 Traning Loss: tensor(4.5319e-05)\n",
            "17104 Traning Loss: tensor(4.5855e-05)\n",
            "17105 Traning Loss: tensor(4.3011e-05)\n",
            "17106 Traning Loss: tensor(4.6338e-05)\n",
            "17107 Traning Loss: tensor(4.6824e-05)\n",
            "17108 Traning Loss: tensor(4.3936e-05)\n",
            "17109 Traning Loss: tensor(4.6284e-05)\n",
            "17110 Traning Loss: tensor(4.2355e-05)\n",
            "17111 Traning Loss: tensor(4.1638e-05)\n",
            "17112 Traning Loss: tensor(4.5132e-05)\n",
            "17113 Traning Loss: tensor(4.7200e-05)\n",
            "17114 Traning Loss: tensor(4.3356e-05)\n",
            "17115 Traning Loss: tensor(4.2663e-05)\n",
            "17116 Traning Loss: tensor(8.6332e-05)\n",
            "17117 Traning Loss: tensor(4.3957e-05)\n",
            "17118 Traning Loss: tensor(4.3318e-05)\n",
            "17119 Traning Loss: tensor(4.8160e-05)\n",
            "17120 Traning Loss: tensor(4.3236e-05)\n",
            "17121 Traning Loss: tensor(4.4918e-05)\n",
            "17122 Traning Loss: tensor(5.1638e-05)\n",
            "17123 Traning Loss: tensor(6.1364e-05)\n",
            "17124 Traning Loss: tensor(7.0002e-05)\n",
            "17125 Traning Loss: tensor(4.2512e-05)\n",
            "17126 Traning Loss: tensor(4.4789e-05)\n",
            "17127 Traning Loss: tensor(4.5669e-05)\n",
            "17128 Traning Loss: tensor(4.5261e-05)\n",
            "17129 Traning Loss: tensor(4.5537e-05)\n",
            "17130 Traning Loss: tensor(5.0789e-05)\n",
            "17131 Traning Loss: tensor(3.9879e-05)\n",
            "17132 Traning Loss: tensor(4.2122e-05)\n",
            "17133 Traning Loss: tensor(4.8246e-05)\n",
            "17134 Traning Loss: tensor(4.0495e-05)\n",
            "17135 Traning Loss: tensor(4.3548e-05)\n",
            "17136 Traning Loss: tensor(4.4548e-05)\n",
            "17137 Traning Loss: tensor(4.3830e-05)\n",
            "17138 Traning Loss: tensor(8.6082e-05)\n",
            "17139 Traning Loss: tensor(4.9376e-05)\n",
            "17140 Traning Loss: tensor(4.6789e-05)\n",
            "17141 Traning Loss: tensor(4.7805e-05)\n",
            "17142 Traning Loss: tensor(4.3571e-05)\n",
            "17143 Traning Loss: tensor(5.1162e-05)\n",
            "17144 Traning Loss: tensor(4.7440e-05)\n",
            "17145 Traning Loss: tensor(4.7942e-05)\n",
            "17146 Traning Loss: tensor(4.5456e-05)\n",
            "17147 Traning Loss: tensor(4.2903e-05)\n",
            "17148 Traning Loss: tensor(4.5473e-05)\n",
            "17149 Traning Loss: tensor(4.2614e-05)\n",
            "17150 Traning Loss: tensor(4.2226e-05)\n",
            "17151 Traning Loss: tensor(4.6603e-05)\n",
            "17152 Traning Loss: tensor(4.3212e-05)\n",
            "17153 Traning Loss: tensor(4.0754e-05)\n",
            "17154 Traning Loss: tensor(4.4500e-05)\n",
            "17155 Traning Loss: tensor(4.2130e-05)\n",
            "17156 Traning Loss: tensor(5.9050e-05)\n",
            "17157 Traning Loss: tensor(4.9605e-05)\n",
            "17158 Traning Loss: tensor(4.6823e-05)\n",
            "17159 Traning Loss: tensor(4.2129e-05)\n",
            "17160 Traning Loss: tensor(4.5496e-05)\n",
            "17161 Traning Loss: tensor(4.3586e-05)\n",
            "17162 Traning Loss: tensor(4.4521e-05)\n",
            "17163 Traning Loss: tensor(4.5821e-05)\n",
            "17164 Traning Loss: tensor(4.2835e-05)\n",
            "17165 Traning Loss: tensor(4.3061e-05)\n",
            "17166 Traning Loss: tensor(4.6358e-05)\n",
            "17167 Traning Loss: tensor(4.3209e-05)\n",
            "17168 Traning Loss: tensor(7.8572e-05)\n",
            "17169 Traning Loss: tensor(6.1478e-05)\n",
            "17170 Traning Loss: tensor(4.6209e-05)\n",
            "17171 Traning Loss: tensor(7.1359e-05)\n",
            "17172 Traning Loss: tensor(5.4214e-05)\n",
            "17173 Traning Loss: tensor(4.0208e-05)\n",
            "17174 Traning Loss: tensor(4.5455e-05)\n",
            "17175 Traning Loss: tensor(4.5209e-05)\n",
            "17176 Traning Loss: tensor(4.4206e-05)\n",
            "17177 Traning Loss: tensor(4.7870e-05)\n",
            "17178 Traning Loss: tensor(4.3909e-05)\n",
            "17179 Traning Loss: tensor(4.4683e-05)\n",
            "17180 Traning Loss: tensor(4.9063e-05)\n",
            "17181 Traning Loss: tensor(8.9880e-05)\n",
            "17182 Traning Loss: tensor(5.1821e-05)\n",
            "17183 Traning Loss: tensor(5.2851e-05)\n",
            "17184 Traning Loss: tensor(4.7340e-05)\n",
            "17185 Traning Loss: tensor(4.8690e-05)\n",
            "17186 Traning Loss: tensor(4.6337e-05)\n",
            "17187 Traning Loss: tensor(3.9573e-05)\n",
            "17188 Traning Loss: tensor(5.0184e-05)\n",
            "17189 Traning Loss: tensor(4.4579e-05)\n",
            "17190 Traning Loss: tensor(4.7140e-05)\n",
            "17191 Traning Loss: tensor(4.4708e-05)\n",
            "17192 Traning Loss: tensor(4.0989e-05)\n",
            "17193 Traning Loss: tensor(4.7413e-05)\n",
            "17194 Traning Loss: tensor(4.5545e-05)\n",
            "17195 Traning Loss: tensor(4.3281e-05)\n",
            "17196 Traning Loss: tensor(6.3590e-05)\n",
            "17197 Traning Loss: tensor(4.1837e-05)\n",
            "17198 Traning Loss: tensor(4.2551e-05)\n",
            "17199 Traning Loss: tensor(4.3756e-05)\n",
            "17200 Traning Loss: tensor(4.4725e-05)\n",
            "17201 Traning Loss: tensor(4.7750e-05)\n",
            "17202 Traning Loss: tensor(4.7047e-05)\n",
            "17203 Traning Loss: tensor(6.5319e-05)\n",
            "17204 Traning Loss: tensor(5.5064e-05)\n",
            "17205 Traning Loss: tensor(4.4476e-05)\n",
            "17206 Traning Loss: tensor(4.4930e-05)\n",
            "17207 Traning Loss: tensor(5.3994e-05)\n",
            "17208 Traning Loss: tensor(4.2656e-05)\n",
            "17209 Traning Loss: tensor(4.7551e-05)\n",
            "17210 Traning Loss: tensor(4.8606e-05)\n",
            "17211 Traning Loss: tensor(4.2159e-05)\n",
            "17212 Traning Loss: tensor(4.6371e-05)\n",
            "17213 Traning Loss: tensor(4.2550e-05)\n",
            "17214 Traning Loss: tensor(4.0045e-05)\n",
            "17215 Traning Loss: tensor(4.5368e-05)\n",
            "17216 Traning Loss: tensor(4.4486e-05)\n",
            "17217 Traning Loss: tensor(4.6482e-05)\n",
            "17218 Traning Loss: tensor(4.7626e-05)\n",
            "17219 Traning Loss: tensor(4.8235e-05)\n",
            "17220 Traning Loss: tensor(4.1701e-05)\n",
            "17221 Traning Loss: tensor(5.4253e-05)\n",
            "17222 Traning Loss: tensor(4.8826e-05)\n",
            "17223 Traning Loss: tensor(4.2257e-05)\n",
            "17224 Traning Loss: tensor(4.2226e-05)\n",
            "17225 Traning Loss: tensor(4.3944e-05)\n",
            "17226 Traning Loss: tensor(4.4994e-05)\n",
            "17227 Traning Loss: tensor(4.1832e-05)\n",
            "17228 Traning Loss: tensor(4.0591e-05)\n",
            "17229 Traning Loss: tensor(3.8394e-05)\n",
            "17230 Traning Loss: tensor(4.0955e-05)\n",
            "17231 Traning Loss: tensor(4.2044e-05)\n",
            "17232 Traning Loss: tensor(4.3586e-05)\n",
            "17233 Traning Loss: tensor(4.2246e-05)\n",
            "17234 Traning Loss: tensor(4.2259e-05)\n",
            "17235 Traning Loss: tensor(4.3447e-05)\n",
            "17236 Traning Loss: tensor(4.3512e-05)\n",
            "17237 Traning Loss: tensor(4.4698e-05)\n",
            "17238 Traning Loss: tensor(4.4442e-05)\n",
            "17239 Traning Loss: tensor(4.1731e-05)\n",
            "17240 Traning Loss: tensor(4.2486e-05)\n",
            "17241 Traning Loss: tensor(4.2482e-05)\n",
            "17242 Traning Loss: tensor(4.0662e-05)\n",
            "17243 Traning Loss: tensor(4.2436e-05)\n",
            "17244 Traning Loss: tensor(4.3566e-05)\n",
            "17245 Traning Loss: tensor(4.0988e-05)\n",
            "17246 Traning Loss: tensor(5.7606e-05)\n",
            "17247 Traning Loss: tensor(3.9091e-05)\n",
            "17248 Traning Loss: tensor(4.3803e-05)\n",
            "17249 Traning Loss: tensor(4.4746e-05)\n",
            "17250 Traning Loss: tensor(4.2566e-05)\n",
            "17251 Traning Loss: tensor(4.2276e-05)\n",
            "17252 Traning Loss: tensor(4.1180e-05)\n",
            "17253 Traning Loss: tensor(4.2165e-05)\n",
            "17254 Traning Loss: tensor(4.2396e-05)\n",
            "17255 Traning Loss: tensor(4.1106e-05)\n",
            "17256 Traning Loss: tensor(4.2487e-05)\n",
            "17257 Traning Loss: tensor(4.3058e-05)\n",
            "17258 Traning Loss: tensor(3.8690e-05)\n",
            "17259 Traning Loss: tensor(4.2639e-05)\n",
            "17260 Traning Loss: tensor(4.4315e-05)\n",
            "17261 Traning Loss: tensor(4.3785e-05)\n",
            "17262 Traning Loss: tensor(4.0446e-05)\n",
            "17263 Traning Loss: tensor(4.1365e-05)\n",
            "17264 Traning Loss: tensor(3.9888e-05)\n",
            "17265 Traning Loss: tensor(4.3347e-05)\n",
            "17266 Traning Loss: tensor(3.7333e-05)\n",
            "17267 Traning Loss: tensor(3.9834e-05)\n",
            "17268 Traning Loss: tensor(4.0314e-05)\n",
            "17269 Traning Loss: tensor(4.0328e-05)\n",
            "17270 Traning Loss: tensor(4.0144e-05)\n",
            "17271 Traning Loss: tensor(3.9357e-05)\n",
            "17272 Traning Loss: tensor(7.1405e-05)\n",
            "17273 Traning Loss: tensor(4.1628e-05)\n",
            "17274 Traning Loss: tensor(4.2708e-05)\n",
            "17275 Traning Loss: tensor(4.2815e-05)\n",
            "17276 Traning Loss: tensor(4.1587e-05)\n",
            "17277 Traning Loss: tensor(4.1345e-05)\n",
            "17278 Traning Loss: tensor(4.2621e-05)\n",
            "17279 Traning Loss: tensor(4.2409e-05)\n",
            "17280 Traning Loss: tensor(4.1876e-05)\n",
            "17281 Traning Loss: tensor(4.7766e-05)\n",
            "17282 Traning Loss: tensor(7.2505e-05)\n",
            "17283 Traning Loss: tensor(4.4622e-05)\n",
            "17284 Traning Loss: tensor(4.3447e-05)\n",
            "17285 Traning Loss: tensor(4.3957e-05)\n",
            "17286 Traning Loss: tensor(5.2760e-05)\n",
            "17287 Traning Loss: tensor(4.1935e-05)\n",
            "17288 Traning Loss: tensor(4.2817e-05)\n",
            "17289 Traning Loss: tensor(4.2687e-05)\n",
            "17290 Traning Loss: tensor(4.0406e-05)\n",
            "17291 Traning Loss: tensor(4.2882e-05)\n",
            "17292 Traning Loss: tensor(3.9419e-05)\n",
            "17293 Traning Loss: tensor(4.2210e-05)\n",
            "17294 Traning Loss: tensor(4.1968e-05)\n",
            "17295 Traning Loss: tensor(3.8915e-05)\n",
            "17296 Traning Loss: tensor(5.2148e-05)\n",
            "17297 Traning Loss: tensor(4.2975e-05)\n",
            "17298 Traning Loss: tensor(4.3476e-05)\n",
            "17299 Traning Loss: tensor(4.4912e-05)\n",
            "17300 Traning Loss: tensor(4.1568e-05)\n",
            "17301 Traning Loss: tensor(4.0465e-05)\n",
            "17302 Traning Loss: tensor(4.5006e-05)\n",
            "17303 Traning Loss: tensor(4.2537e-05)\n",
            "17304 Traning Loss: tensor(4.9050e-05)\n",
            "17305 Traning Loss: tensor(4.2946e-05)\n",
            "17306 Traning Loss: tensor(4.0442e-05)\n",
            "17307 Traning Loss: tensor(4.3587e-05)\n",
            "17308 Traning Loss: tensor(4.4083e-05)\n",
            "17309 Traning Loss: tensor(4.1695e-05)\n",
            "17310 Traning Loss: tensor(4.1029e-05)\n",
            "17311 Traning Loss: tensor(3.7579e-05)\n",
            "17312 Traning Loss: tensor(4.6128e-05)\n",
            "17313 Traning Loss: tensor(4.2036e-05)\n",
            "17314 Traning Loss: tensor(4.2006e-05)\n",
            "17315 Traning Loss: tensor(3.8778e-05)\n",
            "17316 Traning Loss: tensor(4.3672e-05)\n",
            "17317 Traning Loss: tensor(4.0659e-05)\n",
            "17318 Traning Loss: tensor(4.0813e-05)\n",
            "17319 Traning Loss: tensor(4.0285e-05)\n",
            "17320 Traning Loss: tensor(4.2629e-05)\n",
            "17321 Traning Loss: tensor(4.2589e-05)\n",
            "17322 Traning Loss: tensor(3.7525e-05)\n",
            "17323 Traning Loss: tensor(3.8635e-05)\n",
            "17324 Traning Loss: tensor(6.3207e-05)\n",
            "17325 Traning Loss: tensor(3.8070e-05)\n",
            "17326 Traning Loss: tensor(3.9544e-05)\n",
            "17327 Traning Loss: tensor(4.3023e-05)\n",
            "17328 Traning Loss: tensor(3.8822e-05)\n",
            "17329 Traning Loss: tensor(3.9088e-05)\n",
            "17330 Traning Loss: tensor(3.9983e-05)\n",
            "17331 Traning Loss: tensor(4.0322e-05)\n",
            "17332 Traning Loss: tensor(5.7058e-05)\n",
            "17333 Traning Loss: tensor(3.6716e-05)\n",
            "17334 Traning Loss: tensor(3.7143e-05)\n",
            "17335 Traning Loss: tensor(3.9562e-05)\n",
            "17336 Traning Loss: tensor(3.8960e-05)\n",
            "17337 Traning Loss: tensor(3.9650e-05)\n",
            "17338 Traning Loss: tensor(3.8655e-05)\n",
            "17339 Traning Loss: tensor(3.8054e-05)\n",
            "17340 Traning Loss: tensor(4.0254e-05)\n",
            "17341 Traning Loss: tensor(3.6878e-05)\n",
            "17342 Traning Loss: tensor(3.7053e-05)\n",
            "17343 Traning Loss: tensor(4.3942e-05)\n",
            "17344 Traning Loss: tensor(3.9514e-05)\n",
            "17345 Traning Loss: tensor(7.9501e-05)\n",
            "17346 Traning Loss: tensor(4.4726e-05)\n",
            "17347 Traning Loss: tensor(4.5241e-05)\n",
            "17348 Traning Loss: tensor(3.9593e-05)\n",
            "17349 Traning Loss: tensor(4.3974e-05)\n",
            "17350 Traning Loss: tensor(4.2011e-05)\n",
            "17351 Traning Loss: tensor(4.2329e-05)\n",
            "17352 Traning Loss: tensor(9.3853e-05)\n",
            "17353 Traning Loss: tensor(4.1091e-05)\n",
            "17354 Traning Loss: tensor(4.8610e-05)\n",
            "17355 Traning Loss: tensor(4.2956e-05)\n",
            "17356 Traning Loss: tensor(4.1824e-05)\n",
            "17357 Traning Loss: tensor(4.3157e-05)\n",
            "17358 Traning Loss: tensor(4.3724e-05)\n",
            "17359 Traning Loss: tensor(4.8060e-05)\n",
            "17360 Traning Loss: tensor(4.3594e-05)\n",
            "17361 Traning Loss: tensor(3.9051e-05)\n",
            "17362 Traning Loss: tensor(4.4084e-05)\n",
            "17363 Traning Loss: tensor(4.0075e-05)\n",
            "17364 Traning Loss: tensor(3.9965e-05)\n",
            "17365 Traning Loss: tensor(4.3914e-05)\n",
            "17366 Traning Loss: tensor(5.2917e-05)\n",
            "17367 Traning Loss: tensor(3.9136e-05)\n",
            "17368 Traning Loss: tensor(4.1906e-05)\n",
            "17369 Traning Loss: tensor(5.2105e-05)\n",
            "17370 Traning Loss: tensor(4.3667e-05)\n",
            "17371 Traning Loss: tensor(4.2013e-05)\n",
            "17372 Traning Loss: tensor(3.8189e-05)\n",
            "17373 Traning Loss: tensor(4.7593e-05)\n",
            "17374 Traning Loss: tensor(4.8699e-05)\n",
            "17375 Traning Loss: tensor(3.9158e-05)\n",
            "17376 Traning Loss: tensor(5.5411e-05)\n",
            "17377 Traning Loss: tensor(5.4114e-05)\n",
            "17378 Traning Loss: tensor(4.6893e-05)\n",
            "17379 Traning Loss: tensor(3.9914e-05)\n",
            "17380 Traning Loss: tensor(4.5272e-05)\n",
            "17381 Traning Loss: tensor(4.2174e-05)\n",
            "17382 Traning Loss: tensor(3.9878e-05)\n",
            "17383 Traning Loss: tensor(3.8045e-05)\n",
            "17384 Traning Loss: tensor(4.3109e-05)\n",
            "17385 Traning Loss: tensor(4.1591e-05)\n",
            "17386 Traning Loss: tensor(3.8525e-05)\n",
            "17387 Traning Loss: tensor(4.3809e-05)\n",
            "17388 Traning Loss: tensor(3.7926e-05)\n",
            "17389 Traning Loss: tensor(3.9883e-05)\n",
            "17390 Traning Loss: tensor(4.1613e-05)\n",
            "17391 Traning Loss: tensor(4.0795e-05)\n",
            "17392 Traning Loss: tensor(5.0403e-05)\n",
            "17393 Traning Loss: tensor(4.1624e-05)\n",
            "17394 Traning Loss: tensor(4.4482e-05)\n",
            "17395 Traning Loss: tensor(3.6055e-05)\n",
            "17396 Traning Loss: tensor(6.3576e-05)\n",
            "17397 Traning Loss: tensor(4.0744e-05)\n",
            "17398 Traning Loss: tensor(4.2809e-05)\n",
            "17399 Traning Loss: tensor(4.3352e-05)\n",
            "17400 Traning Loss: tensor(4.1822e-05)\n",
            "17401 Traning Loss: tensor(4.1960e-05)\n",
            "17402 Traning Loss: tensor(3.8076e-05)\n",
            "17403 Traning Loss: tensor(3.8561e-05)\n",
            "17404 Traning Loss: tensor(4.1044e-05)\n",
            "17405 Traning Loss: tensor(6.3745e-05)\n",
            "17406 Traning Loss: tensor(4.2334e-05)\n",
            "17407 Traning Loss: tensor(4.0507e-05)\n",
            "17408 Traning Loss: tensor(4.1135e-05)\n",
            "17409 Traning Loss: tensor(4.1682e-05)\n",
            "17410 Traning Loss: tensor(3.9448e-05)\n",
            "17411 Traning Loss: tensor(4.3434e-05)\n",
            "17412 Traning Loss: tensor(3.8669e-05)\n",
            "17413 Traning Loss: tensor(3.9698e-05)\n",
            "17414 Traning Loss: tensor(3.8499e-05)\n",
            "17415 Traning Loss: tensor(3.9442e-05)\n",
            "17416 Traning Loss: tensor(6.3802e-05)\n",
            "17417 Traning Loss: tensor(3.9593e-05)\n",
            "17418 Traning Loss: tensor(4.1297e-05)\n",
            "17419 Traning Loss: tensor(4.5320e-05)\n",
            "17420 Traning Loss: tensor(3.9628e-05)\n",
            "17421 Traning Loss: tensor(4.5035e-05)\n",
            "17422 Traning Loss: tensor(3.7988e-05)\n",
            "17423 Traning Loss: tensor(4.2006e-05)\n",
            "17424 Traning Loss: tensor(4.3253e-05)\n",
            "17425 Traning Loss: tensor(4.0363e-05)\n",
            "17426 Traning Loss: tensor(4.2375e-05)\n",
            "17427 Traning Loss: tensor(3.9704e-05)\n",
            "17428 Traning Loss: tensor(3.9357e-05)\n",
            "17429 Traning Loss: tensor(4.2611e-05)\n",
            "17430 Traning Loss: tensor(3.8200e-05)\n",
            "17431 Traning Loss: tensor(3.7050e-05)\n",
            "17432 Traning Loss: tensor(3.9991e-05)\n",
            "17433 Traning Loss: tensor(3.8234e-05)\n",
            "17434 Traning Loss: tensor(3.5642e-05)\n",
            "17435 Traning Loss: tensor(4.3109e-05)\n",
            "17436 Traning Loss: tensor(3.9854e-05)\n",
            "17437 Traning Loss: tensor(3.8805e-05)\n",
            "17438 Traning Loss: tensor(4.1906e-05)\n",
            "17439 Traning Loss: tensor(3.7436e-05)\n",
            "17440 Traning Loss: tensor(3.9501e-05)\n",
            "17441 Traning Loss: tensor(4.2933e-05)\n",
            "17442 Traning Loss: tensor(3.9931e-05)\n",
            "17443 Traning Loss: tensor(4.2017e-05)\n",
            "17444 Traning Loss: tensor(3.8347e-05)\n",
            "17445 Traning Loss: tensor(3.8427e-05)\n",
            "17446 Traning Loss: tensor(4.0686e-05)\n",
            "17447 Traning Loss: tensor(3.7374e-05)\n",
            "17448 Traning Loss: tensor(3.7870e-05)\n",
            "17449 Traning Loss: tensor(3.8015e-05)\n",
            "17450 Traning Loss: tensor(5.3754e-05)\n",
            "17451 Traning Loss: tensor(4.0956e-05)\n",
            "17452 Traning Loss: tensor(4.5875e-05)\n",
            "17453 Traning Loss: tensor(4.5428e-05)\n",
            "17454 Traning Loss: tensor(4.2684e-05)\n",
            "17455 Traning Loss: tensor(4.1239e-05)\n",
            "17456 Traning Loss: tensor(3.9716e-05)\n",
            "17457 Traning Loss: tensor(4.1952e-05)\n",
            "17458 Traning Loss: tensor(4.4213e-05)\n",
            "17459 Traning Loss: tensor(3.9289e-05)\n",
            "17460 Traning Loss: tensor(4.5828e-05)\n",
            "17461 Traning Loss: tensor(5.3536e-05)\n",
            "17462 Traning Loss: tensor(4.1998e-05)\n",
            "17463 Traning Loss: tensor(4.2599e-05)\n",
            "17464 Traning Loss: tensor(4.2181e-05)\n",
            "17465 Traning Loss: tensor(3.8547e-05)\n",
            "17466 Traning Loss: tensor(4.0903e-05)\n",
            "17467 Traning Loss: tensor(4.0015e-05)\n",
            "17468 Traning Loss: tensor(4.0164e-05)\n",
            "17469 Traning Loss: tensor(3.9133e-05)\n",
            "17470 Traning Loss: tensor(4.4365e-05)\n",
            "17471 Traning Loss: tensor(3.8618e-05)\n",
            "17472 Traning Loss: tensor(3.7858e-05)\n",
            "17473 Traning Loss: tensor(3.6535e-05)\n",
            "17474 Traning Loss: tensor(3.6398e-05)\n",
            "17475 Traning Loss: tensor(3.7830e-05)\n",
            "17476 Traning Loss: tensor(3.7680e-05)\n",
            "17477 Traning Loss: tensor(6.4895e-05)\n",
            "17478 Traning Loss: tensor(3.8473e-05)\n",
            "17479 Traning Loss: tensor(3.7699e-05)\n",
            "17480 Traning Loss: tensor(3.6330e-05)\n",
            "17481 Traning Loss: tensor(3.5834e-05)\n",
            "17482 Traning Loss: tensor(3.6185e-05)\n",
            "17483 Traning Loss: tensor(3.6701e-05)\n",
            "17484 Traning Loss: tensor(3.8179e-05)\n",
            "17485 Traning Loss: tensor(4.2556e-05)\n",
            "17486 Traning Loss: tensor(3.3383e-05)\n",
            "17487 Traning Loss: tensor(3.5774e-05)\n",
            "17488 Traning Loss: tensor(3.6144e-05)\n",
            "17489 Traning Loss: tensor(3.9433e-05)\n",
            "17490 Traning Loss: tensor(3.7831e-05)\n",
            "17491 Traning Loss: tensor(3.6934e-05)\n",
            "17492 Traning Loss: tensor(3.7627e-05)\n",
            "17493 Traning Loss: tensor(3.8867e-05)\n",
            "17494 Traning Loss: tensor(3.7773e-05)\n",
            "17495 Traning Loss: tensor(3.8137e-05)\n",
            "17496 Traning Loss: tensor(4.0056e-05)\n",
            "17497 Traning Loss: tensor(3.5806e-05)\n",
            "17498 Traning Loss: tensor(4.4623e-05)\n",
            "17499 Traning Loss: tensor(4.0696e-05)\n",
            "17500 Traning Loss: tensor(5.0062e-05)\n",
            "17501 Traning Loss: tensor(3.9930e-05)\n",
            "17502 Traning Loss: tensor(4.4625e-05)\n",
            "17503 Traning Loss: tensor(3.9325e-05)\n",
            "17504 Traning Loss: tensor(3.8826e-05)\n",
            "17505 Traning Loss: tensor(3.7470e-05)\n",
            "17506 Traning Loss: tensor(4.1180e-05)\n",
            "17507 Traning Loss: tensor(3.8826e-05)\n",
            "17508 Traning Loss: tensor(5.3948e-05)\n",
            "17509 Traning Loss: tensor(3.7738e-05)\n",
            "17510 Traning Loss: tensor(3.8332e-05)\n",
            "17511 Traning Loss: tensor(3.6276e-05)\n",
            "17512 Traning Loss: tensor(3.7801e-05)\n",
            "17513 Traning Loss: tensor(3.5880e-05)\n",
            "17514 Traning Loss: tensor(3.8286e-05)\n",
            "17515 Traning Loss: tensor(4.3268e-05)\n",
            "17516 Traning Loss: tensor(3.4078e-05)\n",
            "17517 Traning Loss: tensor(3.7512e-05)\n",
            "17518 Traning Loss: tensor(5.5094e-05)\n",
            "17519 Traning Loss: tensor(3.9734e-05)\n",
            "17520 Traning Loss: tensor(3.8881e-05)\n",
            "17521 Traning Loss: tensor(3.8596e-05)\n",
            "17522 Traning Loss: tensor(3.9222e-05)\n",
            "17523 Traning Loss: tensor(5.0484e-05)\n",
            "17524 Traning Loss: tensor(3.9521e-05)\n",
            "17525 Traning Loss: tensor(6.8875e-05)\n",
            "17526 Traning Loss: tensor(4.0996e-05)\n",
            "17527 Traning Loss: tensor(4.0641e-05)\n",
            "17528 Traning Loss: tensor(4.1355e-05)\n",
            "17529 Traning Loss: tensor(4.0027e-05)\n",
            "17530 Traning Loss: tensor(3.4334e-05)\n",
            "17531 Traning Loss: tensor(9.4252e-05)\n",
            "17532 Traning Loss: tensor(5.3113e-05)\n",
            "17533 Traning Loss: tensor(5.4394e-05)\n",
            "17534 Traning Loss: tensor(4.7414e-05)\n",
            "17535 Traning Loss: tensor(5.0968e-05)\n",
            "17536 Traning Loss: tensor(4.0509e-05)\n",
            "17537 Traning Loss: tensor(4.2701e-05)\n",
            "17538 Traning Loss: tensor(5.3732e-05)\n",
            "17539 Traning Loss: tensor(3.9848e-05)\n",
            "17540 Traning Loss: tensor(4.7214e-05)\n",
            "17541 Traning Loss: tensor(4.5938e-05)\n",
            "17542 Traning Loss: tensor(3.9483e-05)\n",
            "17543 Traning Loss: tensor(4.2480e-05)\n",
            "17544 Traning Loss: tensor(4.0924e-05)\n",
            "17545 Traning Loss: tensor(3.7257e-05)\n",
            "17546 Traning Loss: tensor(3.8949e-05)\n",
            "17547 Traning Loss: tensor(3.8963e-05)\n",
            "17548 Traning Loss: tensor(4.1457e-05)\n",
            "17549 Traning Loss: tensor(3.7679e-05)\n",
            "17550 Traning Loss: tensor(3.9125e-05)\n",
            "17551 Traning Loss: tensor(5.2902e-05)\n",
            "17552 Traning Loss: tensor(3.9024e-05)\n",
            "17553 Traning Loss: tensor(3.5665e-05)\n",
            "17554 Traning Loss: tensor(3.5565e-05)\n",
            "17555 Traning Loss: tensor(3.6052e-05)\n",
            "17556 Traning Loss: tensor(4.7889e-05)\n",
            "17557 Traning Loss: tensor(3.5355e-05)\n",
            "17558 Traning Loss: tensor(3.6387e-05)\n",
            "17559 Traning Loss: tensor(3.8126e-05)\n",
            "17560 Traning Loss: tensor(3.6983e-05)\n",
            "17561 Traning Loss: tensor(3.6315e-05)\n",
            "17562 Traning Loss: tensor(3.8459e-05)\n",
            "17563 Traning Loss: tensor(4.1094e-05)\n",
            "17564 Traning Loss: tensor(3.8444e-05)\n",
            "17565 Traning Loss: tensor(3.7929e-05)\n",
            "17566 Traning Loss: tensor(3.6833e-05)\n",
            "17567 Traning Loss: tensor(3.9300e-05)\n",
            "17568 Traning Loss: tensor(4.0176e-05)\n",
            "17569 Traning Loss: tensor(3.7050e-05)\n",
            "17570 Traning Loss: tensor(5.0170e-05)\n",
            "17571 Traning Loss: tensor(3.7373e-05)\n",
            "17572 Traning Loss: tensor(4.3124e-05)\n",
            "17573 Traning Loss: tensor(3.8021e-05)\n",
            "17574 Traning Loss: tensor(3.6191e-05)\n",
            "17575 Traning Loss: tensor(3.5654e-05)\n",
            "17576 Traning Loss: tensor(3.6360e-05)\n",
            "17577 Traning Loss: tensor(3.5152e-05)\n",
            "17578 Traning Loss: tensor(3.5229e-05)\n",
            "17579 Traning Loss: tensor(4.0884e-05)\n",
            "17580 Traning Loss: tensor(3.6425e-05)\n",
            "17581 Traning Loss: tensor(4.4247e-05)\n",
            "17582 Traning Loss: tensor(3.1919e-05)\n",
            "17583 Traning Loss: tensor(3.9872e-05)\n",
            "17584 Traning Loss: tensor(3.7289e-05)\n",
            "17585 Traning Loss: tensor(3.5222e-05)\n",
            "17586 Traning Loss: tensor(3.5491e-05)\n",
            "17587 Traning Loss: tensor(3.7859e-05)\n",
            "17588 Traning Loss: tensor(3.6020e-05)\n",
            "17589 Traning Loss: tensor(3.9172e-05)\n",
            "17590 Traning Loss: tensor(3.8018e-05)\n",
            "17591 Traning Loss: tensor(3.8915e-05)\n",
            "17592 Traning Loss: tensor(3.8889e-05)\n",
            "17593 Traning Loss: tensor(3.9421e-05)\n",
            "17594 Traning Loss: tensor(3.7164e-05)\n",
            "17595 Traning Loss: tensor(4.0171e-05)\n",
            "17596 Traning Loss: tensor(3.8028e-05)\n",
            "17597 Traning Loss: tensor(3.6561e-05)\n",
            "17598 Traning Loss: tensor(3.6254e-05)\n",
            "17599 Traning Loss: tensor(3.9793e-05)\n",
            "17600 Traning Loss: tensor(3.5623e-05)\n",
            "17601 Traning Loss: tensor(3.6571e-05)\n",
            "17602 Traning Loss: tensor(3.6277e-05)\n",
            "17603 Traning Loss: tensor(3.6573e-05)\n",
            "17604 Traning Loss: tensor(3.8682e-05)\n",
            "17605 Traning Loss: tensor(3.8069e-05)\n",
            "17606 Traning Loss: tensor(3.8945e-05)\n",
            "17607 Traning Loss: tensor(4.0291e-05)\n",
            "17608 Traning Loss: tensor(3.6463e-05)\n",
            "17609 Traning Loss: tensor(3.6663e-05)\n",
            "17610 Traning Loss: tensor(3.8069e-05)\n",
            "17611 Traning Loss: tensor(3.8477e-05)\n",
            "17612 Traning Loss: tensor(4.5727e-05)\n",
            "17613 Traning Loss: tensor(3.9546e-05)\n",
            "17614 Traning Loss: tensor(3.6356e-05)\n",
            "17615 Traning Loss: tensor(4.0136e-05)\n",
            "17616 Traning Loss: tensor(3.7043e-05)\n",
            "17617 Traning Loss: tensor(3.5314e-05)\n",
            "17618 Traning Loss: tensor(4.3930e-05)\n",
            "17619 Traning Loss: tensor(3.6727e-05)\n",
            "17620 Traning Loss: tensor(3.5337e-05)\n",
            "17621 Traning Loss: tensor(3.7278e-05)\n",
            "17622 Traning Loss: tensor(3.5569e-05)\n",
            "17623 Traning Loss: tensor(3.7641e-05)\n",
            "17624 Traning Loss: tensor(3.7379e-05)\n",
            "17625 Traning Loss: tensor(3.7856e-05)\n",
            "17626 Traning Loss: tensor(3.7007e-05)\n",
            "17627 Traning Loss: tensor(3.6730e-05)\n",
            "17628 Traning Loss: tensor(3.3965e-05)\n",
            "17629 Traning Loss: tensor(3.7693e-05)\n",
            "17630 Traning Loss: tensor(4.0152e-05)\n",
            "17631 Traning Loss: tensor(3.6207e-05)\n",
            "17632 Traning Loss: tensor(3.7499e-05)\n",
            "17633 Traning Loss: tensor(3.4865e-05)\n",
            "17634 Traning Loss: tensor(4.4396e-05)\n",
            "17635 Traning Loss: tensor(3.5967e-05)\n",
            "17636 Traning Loss: tensor(3.5833e-05)\n",
            "17637 Traning Loss: tensor(3.8300e-05)\n",
            "17638 Traning Loss: tensor(3.7778e-05)\n",
            "17639 Traning Loss: tensor(3.5601e-05)\n",
            "17640 Traning Loss: tensor(3.8850e-05)\n",
            "17641 Traning Loss: tensor(3.6737e-05)\n",
            "17642 Traning Loss: tensor(3.2628e-05)\n",
            "17643 Traning Loss: tensor(3.4231e-05)\n",
            "17644 Traning Loss: tensor(3.3096e-05)\n",
            "17645 Traning Loss: tensor(3.6113e-05)\n",
            "17646 Traning Loss: tensor(3.6797e-05)\n",
            "17647 Traning Loss: tensor(3.4316e-05)\n",
            "17648 Traning Loss: tensor(3.8577e-05)\n",
            "17649 Traning Loss: tensor(4.5880e-05)\n",
            "17650 Traning Loss: tensor(3.8117e-05)\n",
            "17651 Traning Loss: tensor(3.2368e-05)\n",
            "17652 Traning Loss: tensor(3.7210e-05)\n",
            "17653 Traning Loss: tensor(3.3141e-05)\n",
            "17654 Traning Loss: tensor(3.6270e-05)\n",
            "17655 Traning Loss: tensor(3.5109e-05)\n",
            "17656 Traning Loss: tensor(3.6452e-05)\n",
            "17657 Traning Loss: tensor(3.7577e-05)\n",
            "17658 Traning Loss: tensor(3.6819e-05)\n",
            "17659 Traning Loss: tensor(3.5109e-05)\n",
            "17660 Traning Loss: tensor(3.7293e-05)\n",
            "17661 Traning Loss: tensor(3.8516e-05)\n",
            "17662 Traning Loss: tensor(3.4372e-05)\n",
            "17663 Traning Loss: tensor(4.0153e-05)\n",
            "17664 Traning Loss: tensor(3.7678e-05)\n",
            "17665 Traning Loss: tensor(5.3458e-05)\n",
            "17666 Traning Loss: tensor(4.1601e-05)\n",
            "17667 Traning Loss: tensor(3.9394e-05)\n",
            "17668 Traning Loss: tensor(3.4543e-05)\n",
            "17669 Traning Loss: tensor(4.1534e-05)\n",
            "17670 Traning Loss: tensor(3.8211e-05)\n",
            "17671 Traning Loss: tensor(3.6322e-05)\n",
            "17672 Traning Loss: tensor(4.1583e-05)\n",
            "17673 Traning Loss: tensor(6.5580e-05)\n",
            "17674 Traning Loss: tensor(3.8001e-05)\n",
            "17675 Traning Loss: tensor(3.7894e-05)\n",
            "17676 Traning Loss: tensor(4.9531e-05)\n",
            "17677 Traning Loss: tensor(3.9171e-05)\n",
            "17678 Traning Loss: tensor(3.9653e-05)\n",
            "17679 Traning Loss: tensor(3.7558e-05)\n",
            "17680 Traning Loss: tensor(3.5034e-05)\n",
            "17681 Traning Loss: tensor(3.6350e-05)\n",
            "17682 Traning Loss: tensor(3.5018e-05)\n",
            "17683 Traning Loss: tensor(3.4830e-05)\n",
            "17684 Traning Loss: tensor(6.1500e-05)\n",
            "17685 Traning Loss: tensor(3.5326e-05)\n",
            "17686 Traning Loss: tensor(3.4866e-05)\n",
            "17687 Traning Loss: tensor(3.5791e-05)\n",
            "17688 Traning Loss: tensor(3.9832e-05)\n",
            "17689 Traning Loss: tensor(3.9161e-05)\n",
            "17690 Traning Loss: tensor(3.6230e-05)\n",
            "17691 Traning Loss: tensor(3.4151e-05)\n",
            "17692 Traning Loss: tensor(3.8676e-05)\n",
            "17693 Traning Loss: tensor(3.6594e-05)\n",
            "17694 Traning Loss: tensor(3.6432e-05)\n",
            "17695 Traning Loss: tensor(3.7076e-05)\n",
            "17696 Traning Loss: tensor(3.6890e-05)\n",
            "17697 Traning Loss: tensor(3.3335e-05)\n",
            "17698 Traning Loss: tensor(3.4601e-05)\n",
            "17699 Traning Loss: tensor(3.7849e-05)\n",
            "17700 Traning Loss: tensor(3.5445e-05)\n",
            "17701 Traning Loss: tensor(3.4352e-05)\n",
            "17702 Traning Loss: tensor(3.6944e-05)\n",
            "17703 Traning Loss: tensor(3.4620e-05)\n",
            "17704 Traning Loss: tensor(3.3940e-05)\n",
            "17705 Traning Loss: tensor(3.4992e-05)\n",
            "17706 Traning Loss: tensor(4.1285e-05)\n",
            "17707 Traning Loss: tensor(4.0603e-05)\n",
            "17708 Traning Loss: tensor(3.9753e-05)\n",
            "17709 Traning Loss: tensor(3.5855e-05)\n",
            "17710 Traning Loss: tensor(3.9543e-05)\n",
            "17711 Traning Loss: tensor(3.9798e-05)\n",
            "17712 Traning Loss: tensor(3.9265e-05)\n",
            "17713 Traning Loss: tensor(3.6696e-05)\n",
            "17714 Traning Loss: tensor(3.4517e-05)\n",
            "17715 Traning Loss: tensor(3.3552e-05)\n",
            "17716 Traning Loss: tensor(3.4717e-05)\n",
            "17717 Traning Loss: tensor(3.7720e-05)\n",
            "17718 Traning Loss: tensor(3.8689e-05)\n",
            "17719 Traning Loss: tensor(4.0857e-05)\n",
            "17720 Traning Loss: tensor(4.8244e-05)\n",
            "17721 Traning Loss: tensor(3.5405e-05)\n",
            "17722 Traning Loss: tensor(3.7468e-05)\n",
            "17723 Traning Loss: tensor(3.8147e-05)\n",
            "17724 Traning Loss: tensor(3.7821e-05)\n",
            "17725 Traning Loss: tensor(3.6570e-05)\n",
            "17726 Traning Loss: tensor(3.3354e-05)\n",
            "17727 Traning Loss: tensor(6.2821e-05)\n",
            "17728 Traning Loss: tensor(3.6418e-05)\n",
            "17729 Traning Loss: tensor(3.5068e-05)\n",
            "17730 Traning Loss: tensor(3.8354e-05)\n",
            "17731 Traning Loss: tensor(3.6045e-05)\n",
            "17732 Traning Loss: tensor(3.5385e-05)\n",
            "17733 Traning Loss: tensor(3.4825e-05)\n",
            "17734 Traning Loss: tensor(3.4915e-05)\n",
            "17735 Traning Loss: tensor(3.4473e-05)\n",
            "17736 Traning Loss: tensor(3.4978e-05)\n",
            "17737 Traning Loss: tensor(3.2069e-05)\n",
            "17738 Traning Loss: tensor(3.5712e-05)\n",
            "17739 Traning Loss: tensor(3.7688e-05)\n",
            "17740 Traning Loss: tensor(3.4745e-05)\n",
            "17741 Traning Loss: tensor(3.4928e-05)\n",
            "17742 Traning Loss: tensor(3.7797e-05)\n",
            "17743 Traning Loss: tensor(3.4400e-05)\n",
            "17744 Traning Loss: tensor(3.3995e-05)\n",
            "17745 Traning Loss: tensor(3.4624e-05)\n",
            "17746 Traning Loss: tensor(3.4459e-05)\n",
            "17747 Traning Loss: tensor(4.5195e-05)\n",
            "17748 Traning Loss: tensor(3.6582e-05)\n",
            "17749 Traning Loss: tensor(3.4187e-05)\n",
            "17750 Traning Loss: tensor(3.7116e-05)\n",
            "17751 Traning Loss: tensor(3.2684e-05)\n",
            "17752 Traning Loss: tensor(3.4274e-05)\n",
            "17753 Traning Loss: tensor(3.7427e-05)\n",
            "17754 Traning Loss: tensor(3.4385e-05)\n",
            "17755 Traning Loss: tensor(3.4839e-05)\n",
            "17756 Traning Loss: tensor(3.5211e-05)\n",
            "17757 Traning Loss: tensor(3.4654e-05)\n",
            "17758 Traning Loss: tensor(3.6089e-05)\n",
            "17759 Traning Loss: tensor(3.4162e-05)\n",
            "17760 Traning Loss: tensor(3.5974e-05)\n",
            "17761 Traning Loss: tensor(3.3844e-05)\n",
            "17762 Traning Loss: tensor(3.5733e-05)\n",
            "17763 Traning Loss: tensor(3.5097e-05)\n",
            "17764 Traning Loss: tensor(3.3547e-05)\n",
            "17765 Traning Loss: tensor(3.7470e-05)\n",
            "17766 Traning Loss: tensor(4.0798e-05)\n",
            "17767 Traning Loss: tensor(3.7857e-05)\n",
            "17768 Traning Loss: tensor(3.5914e-05)\n",
            "17769 Traning Loss: tensor(3.5688e-05)\n",
            "17770 Traning Loss: tensor(4.2738e-05)\n",
            "17771 Traning Loss: tensor(3.3384e-05)\n",
            "17772 Traning Loss: tensor(3.5707e-05)\n",
            "17773 Traning Loss: tensor(3.3015e-05)\n",
            "17774 Traning Loss: tensor(3.5252e-05)\n",
            "17775 Traning Loss: tensor(3.8584e-05)\n",
            "17776 Traning Loss: tensor(3.2915e-05)\n",
            "17777 Traning Loss: tensor(3.4743e-05)\n",
            "17778 Traning Loss: tensor(3.5985e-05)\n",
            "17779 Traning Loss: tensor(3.3206e-05)\n",
            "17780 Traning Loss: tensor(3.5148e-05)\n",
            "17781 Traning Loss: tensor(3.5249e-05)\n",
            "17782 Traning Loss: tensor(3.4543e-05)\n",
            "17783 Traning Loss: tensor(3.3739e-05)\n",
            "17784 Traning Loss: tensor(3.3206e-05)\n",
            "17785 Traning Loss: tensor(4.1181e-05)\n",
            "17786 Traning Loss: tensor(3.6209e-05)\n",
            "17787 Traning Loss: tensor(3.1173e-05)\n",
            "17788 Traning Loss: tensor(3.5727e-05)\n",
            "17789 Traning Loss: tensor(3.2726e-05)\n",
            "17790 Traning Loss: tensor(3.3545e-05)\n",
            "17791 Traning Loss: tensor(3.5480e-05)\n",
            "17792 Traning Loss: tensor(3.2902e-05)\n",
            "17793 Traning Loss: tensor(3.0960e-05)\n",
            "17794 Traning Loss: tensor(6.7752e-05)\n",
            "17795 Traning Loss: tensor(3.4524e-05)\n",
            "17796 Traning Loss: tensor(3.6601e-05)\n",
            "17797 Traning Loss: tensor(4.0114e-05)\n",
            "17798 Traning Loss: tensor(4.1930e-05)\n",
            "17799 Traning Loss: tensor(3.6620e-05)\n",
            "17800 Traning Loss: tensor(3.6622e-05)\n",
            "17801 Traning Loss: tensor(3.5254e-05)\n",
            "17802 Traning Loss: tensor(3.3675e-05)\n",
            "17803 Traning Loss: tensor(3.7863e-05)\n",
            "17804 Traning Loss: tensor(4.2431e-05)\n",
            "17805 Traning Loss: tensor(4.1805e-05)\n",
            "17806 Traning Loss: tensor(4.0901e-05)\n",
            "17807 Traning Loss: tensor(3.4626e-05)\n",
            "17808 Traning Loss: tensor(3.5270e-05)\n",
            "17809 Traning Loss: tensor(3.6618e-05)\n",
            "17810 Traning Loss: tensor(3.3520e-05)\n",
            "17811 Traning Loss: tensor(3.6361e-05)\n",
            "17812 Traning Loss: tensor(3.5331e-05)\n",
            "17813 Traning Loss: tensor(3.7573e-05)\n",
            "17814 Traning Loss: tensor(4.0582e-05)\n",
            "17815 Traning Loss: tensor(3.5477e-05)\n",
            "17816 Traning Loss: tensor(6.0205e-05)\n",
            "17817 Traning Loss: tensor(5.3887e-05)\n",
            "17818 Traning Loss: tensor(4.0945e-05)\n",
            "17819 Traning Loss: tensor(3.7669e-05)\n",
            "17820 Traning Loss: tensor(4.2920e-05)\n",
            "17821 Traning Loss: tensor(3.6288e-05)\n",
            "17822 Traning Loss: tensor(3.3902e-05)\n",
            "17823 Traning Loss: tensor(3.9257e-05)\n",
            "17824 Traning Loss: tensor(3.4349e-05)\n",
            "17825 Traning Loss: tensor(3.6249e-05)\n",
            "17826 Traning Loss: tensor(3.6055e-05)\n",
            "17827 Traning Loss: tensor(4.0358e-05)\n",
            "17828 Traning Loss: tensor(3.6460e-05)\n",
            "17829 Traning Loss: tensor(4.0675e-05)\n",
            "17830 Traning Loss: tensor(3.6584e-05)\n",
            "17831 Traning Loss: tensor(3.8559e-05)\n",
            "17832 Traning Loss: tensor(3.5921e-05)\n",
            "17833 Traning Loss: tensor(3.8161e-05)\n",
            "17834 Traning Loss: tensor(3.8588e-05)\n",
            "17835 Traning Loss: tensor(3.6735e-05)\n",
            "17836 Traning Loss: tensor(3.9493e-05)\n",
            "17837 Traning Loss: tensor(3.6042e-05)\n",
            "17838 Traning Loss: tensor(3.3646e-05)\n",
            "17839 Traning Loss: tensor(3.9362e-05)\n",
            "17840 Traning Loss: tensor(4.9215e-05)\n",
            "17841 Traning Loss: tensor(3.4055e-05)\n",
            "17842 Traning Loss: tensor(3.7679e-05)\n",
            "17843 Traning Loss: tensor(3.4311e-05)\n",
            "17844 Traning Loss: tensor(3.7661e-05)\n",
            "17845 Traning Loss: tensor(3.3484e-05)\n",
            "17846 Traning Loss: tensor(3.7782e-05)\n",
            "17847 Traning Loss: tensor(3.6299e-05)\n",
            "17848 Traning Loss: tensor(3.2816e-05)\n",
            "17849 Traning Loss: tensor(3.4269e-05)\n",
            "17850 Traning Loss: tensor(3.4079e-05)\n",
            "17851 Traning Loss: tensor(3.8558e-05)\n",
            "17852 Traning Loss: tensor(3.4623e-05)\n",
            "17853 Traning Loss: tensor(3.7246e-05)\n",
            "17854 Traning Loss: tensor(3.3546e-05)\n",
            "17855 Traning Loss: tensor(3.4991e-05)\n",
            "17856 Traning Loss: tensor(3.6516e-05)\n",
            "17857 Traning Loss: tensor(3.3237e-05)\n",
            "17858 Traning Loss: tensor(3.6627e-05)\n",
            "17859 Traning Loss: tensor(4.0268e-05)\n",
            "17860 Traning Loss: tensor(3.3549e-05)\n",
            "17861 Traning Loss: tensor(3.8569e-05)\n",
            "17862 Traning Loss: tensor(3.4381e-05)\n",
            "17863 Traning Loss: tensor(3.2016e-05)\n",
            "17864 Traning Loss: tensor(3.4399e-05)\n",
            "17865 Traning Loss: tensor(3.5684e-05)\n",
            "17866 Traning Loss: tensor(3.4699e-05)\n",
            "17867 Traning Loss: tensor(3.4249e-05)\n",
            "17868 Traning Loss: tensor(3.4223e-05)\n",
            "17869 Traning Loss: tensor(6.6614e-05)\n",
            "17870 Traning Loss: tensor(3.2413e-05)\n",
            "17871 Traning Loss: tensor(3.7021e-05)\n",
            "17872 Traning Loss: tensor(3.4623e-05)\n",
            "17873 Traning Loss: tensor(3.7719e-05)\n",
            "17874 Traning Loss: tensor(3.5610e-05)\n",
            "17875 Traning Loss: tensor(3.4095e-05)\n",
            "17876 Traning Loss: tensor(3.2835e-05)\n",
            "17877 Traning Loss: tensor(3.7681e-05)\n",
            "17878 Traning Loss: tensor(3.7152e-05)\n",
            "17879 Traning Loss: tensor(3.4553e-05)\n",
            "17880 Traning Loss: tensor(3.4398e-05)\n",
            "17881 Traning Loss: tensor(3.3239e-05)\n",
            "17882 Traning Loss: tensor(3.4743e-05)\n",
            "17883 Traning Loss: tensor(3.2554e-05)\n",
            "17884 Traning Loss: tensor(3.3500e-05)\n",
            "17885 Traning Loss: tensor(3.3570e-05)\n",
            "17886 Traning Loss: tensor(3.5407e-05)\n",
            "17887 Traning Loss: tensor(3.3445e-05)\n",
            "17888 Traning Loss: tensor(3.8392e-05)\n",
            "17889 Traning Loss: tensor(3.3910e-05)\n",
            "17890 Traning Loss: tensor(3.3132e-05)\n",
            "17891 Traning Loss: tensor(4.9899e-05)\n",
            "17892 Traning Loss: tensor(3.4419e-05)\n",
            "17893 Traning Loss: tensor(3.9151e-05)\n",
            "17894 Traning Loss: tensor(3.2286e-05)\n",
            "17895 Traning Loss: tensor(3.5274e-05)\n",
            "17896 Traning Loss: tensor(3.5489e-05)\n",
            "17897 Traning Loss: tensor(3.7601e-05)\n",
            "17898 Traning Loss: tensor(3.2558e-05)\n",
            "17899 Traning Loss: tensor(3.5065e-05)\n",
            "17900 Traning Loss: tensor(3.2380e-05)\n",
            "17901 Traning Loss: tensor(3.4711e-05)\n",
            "17902 Traning Loss: tensor(3.4432e-05)\n",
            "17903 Traning Loss: tensor(3.2357e-05)\n",
            "17904 Traning Loss: tensor(3.4756e-05)\n",
            "17905 Traning Loss: tensor(3.7161e-05)\n",
            "17906 Traning Loss: tensor(3.1656e-05)\n",
            "17907 Traning Loss: tensor(3.0786e-05)\n",
            "17908 Traning Loss: tensor(3.5379e-05)\n",
            "17909 Traning Loss: tensor(3.3300e-05)\n",
            "17910 Traning Loss: tensor(4.4313e-05)\n",
            "17911 Traning Loss: tensor(3.3262e-05)\n",
            "17912 Traning Loss: tensor(3.1608e-05)\n",
            "17913 Traning Loss: tensor(3.3051e-05)\n",
            "17914 Traning Loss: tensor(3.2881e-05)\n",
            "17915 Traning Loss: tensor(3.4059e-05)\n",
            "17916 Traning Loss: tensor(3.4199e-05)\n",
            "17917 Traning Loss: tensor(5.3150e-05)\n",
            "17918 Traning Loss: tensor(3.1246e-05)\n",
            "17919 Traning Loss: tensor(3.1326e-05)\n",
            "17920 Traning Loss: tensor(3.2790e-05)\n",
            "17921 Traning Loss: tensor(3.4217e-05)\n",
            "17922 Traning Loss: tensor(3.3059e-05)\n",
            "17923 Traning Loss: tensor(3.2057e-05)\n",
            "17924 Traning Loss: tensor(3.4969e-05)\n",
            "17925 Traning Loss: tensor(3.1312e-05)\n",
            "17926 Traning Loss: tensor(3.6893e-05)\n",
            "17927 Traning Loss: tensor(3.2594e-05)\n",
            "17928 Traning Loss: tensor(3.2305e-05)\n",
            "17929 Traning Loss: tensor(3.1787e-05)\n",
            "17930 Traning Loss: tensor(3.2263e-05)\n",
            "17931 Traning Loss: tensor(4.4203e-05)\n",
            "17932 Traning Loss: tensor(3.2150e-05)\n",
            "17933 Traning Loss: tensor(3.1753e-05)\n",
            "17934 Traning Loss: tensor(3.1430e-05)\n",
            "17935 Traning Loss: tensor(3.2213e-05)\n",
            "17936 Traning Loss: tensor(3.3227e-05)\n",
            "17937 Traning Loss: tensor(3.2953e-05)\n",
            "17938 Traning Loss: tensor(3.1165e-05)\n",
            "17939 Traning Loss: tensor(3.3473e-05)\n",
            "17940 Traning Loss: tensor(3.1649e-05)\n",
            "17941 Traning Loss: tensor(3.1818e-05)\n",
            "17942 Traning Loss: tensor(3.1978e-05)\n",
            "17943 Traning Loss: tensor(3.1897e-05)\n",
            "17944 Traning Loss: tensor(3.3020e-05)\n",
            "17945 Traning Loss: tensor(3.4375e-05)\n",
            "17946 Traning Loss: tensor(3.2463e-05)\n",
            "17947 Traning Loss: tensor(3.3636e-05)\n",
            "17948 Traning Loss: tensor(3.4281e-05)\n",
            "17949 Traning Loss: tensor(3.7001e-05)\n",
            "17950 Traning Loss: tensor(3.2730e-05)\n",
            "17951 Traning Loss: tensor(3.2111e-05)\n",
            "17952 Traning Loss: tensor(3.1595e-05)\n",
            "17953 Traning Loss: tensor(3.2780e-05)\n",
            "17954 Traning Loss: tensor(3.3251e-05)\n",
            "17955 Traning Loss: tensor(3.3208e-05)\n",
            "17956 Traning Loss: tensor(6.0363e-05)\n",
            "17957 Traning Loss: tensor(3.4781e-05)\n",
            "17958 Traning Loss: tensor(3.4072e-05)\n",
            "17959 Traning Loss: tensor(3.2788e-05)\n",
            "17960 Traning Loss: tensor(3.2308e-05)\n",
            "17961 Traning Loss: tensor(3.0149e-05)\n",
            "17962 Traning Loss: tensor(3.4240e-05)\n",
            "17963 Traning Loss: tensor(3.1929e-05)\n",
            "17964 Traning Loss: tensor(3.5179e-05)\n",
            "17965 Traning Loss: tensor(3.1950e-05)\n",
            "17966 Traning Loss: tensor(3.1732e-05)\n",
            "17967 Traning Loss: tensor(3.5104e-05)\n",
            "17968 Traning Loss: tensor(3.2956e-05)\n",
            "17969 Traning Loss: tensor(3.2661e-05)\n",
            "17970 Traning Loss: tensor(3.3173e-05)\n",
            "17971 Traning Loss: tensor(3.0684e-05)\n",
            "17972 Traning Loss: tensor(3.0056e-05)\n",
            "17973 Traning Loss: tensor(3.6005e-05)\n",
            "17974 Traning Loss: tensor(3.0321e-05)\n",
            "17975 Traning Loss: tensor(3.5465e-05)\n",
            "17976 Traning Loss: tensor(3.2034e-05)\n",
            "17977 Traning Loss: tensor(3.2784e-05)\n",
            "17978 Traning Loss: tensor(3.4713e-05)\n",
            "17979 Traning Loss: tensor(5.9893e-05)\n",
            "17980 Traning Loss: tensor(3.5015e-05)\n",
            "17981 Traning Loss: tensor(3.3924e-05)\n",
            "17982 Traning Loss: tensor(3.3714e-05)\n",
            "17983 Traning Loss: tensor(3.4621e-05)\n",
            "17984 Traning Loss: tensor(3.2782e-05)\n",
            "17985 Traning Loss: tensor(3.1497e-05)\n",
            "17986 Traning Loss: tensor(3.2083e-05)\n",
            "17987 Traning Loss: tensor(3.0270e-05)\n",
            "17988 Traning Loss: tensor(3.5514e-05)\n",
            "17989 Traning Loss: tensor(3.0326e-05)\n",
            "17990 Traning Loss: tensor(3.2944e-05)\n",
            "17991 Traning Loss: tensor(3.5939e-05)\n",
            "17992 Traning Loss: tensor(3.3419e-05)\n",
            "17993 Traning Loss: tensor(3.0645e-05)\n",
            "17994 Traning Loss: tensor(3.3863e-05)\n",
            "17995 Traning Loss: tensor(3.3515e-05)\n",
            "17996 Traning Loss: tensor(3.2265e-05)\n",
            "17997 Traning Loss: tensor(3.5822e-05)\n",
            "17998 Traning Loss: tensor(3.2897e-05)\n",
            "17999 Traning Loss: tensor(3.1481e-05)\n",
            "18000 Traning Loss: tensor(3.6222e-05)\n",
            "18001 Traning Loss: tensor(3.3403e-05)\n",
            "18002 Traning Loss: tensor(3.1083e-05)\n",
            "18003 Traning Loss: tensor(4.0670e-05)\n",
            "18004 Traning Loss: tensor(3.0348e-05)\n",
            "18005 Traning Loss: tensor(2.9770e-05)\n",
            "18006 Traning Loss: tensor(2.9974e-05)\n",
            "18007 Traning Loss: tensor(3.6489e-05)\n",
            "18008 Traning Loss: tensor(5.6408e-05)\n",
            "18009 Traning Loss: tensor(3.2552e-05)\n",
            "18010 Traning Loss: tensor(3.3747e-05)\n",
            "18011 Traning Loss: tensor(3.4998e-05)\n",
            "18012 Traning Loss: tensor(3.2861e-05)\n",
            "18013 Traning Loss: tensor(3.0598e-05)\n",
            "18014 Traning Loss: tensor(3.0286e-05)\n",
            "18015 Traning Loss: tensor(3.5454e-05)\n",
            "18016 Traning Loss: tensor(3.2901e-05)\n",
            "18017 Traning Loss: tensor(3.5091e-05)\n",
            "18018 Traning Loss: tensor(3.2869e-05)\n",
            "18019 Traning Loss: tensor(3.3155e-05)\n",
            "18020 Traning Loss: tensor(3.3027e-05)\n",
            "18021 Traning Loss: tensor(3.4285e-05)\n",
            "18022 Traning Loss: tensor(3.3642e-05)\n",
            "18023 Traning Loss: tensor(3.1356e-05)\n",
            "18024 Traning Loss: tensor(3.1128e-05)\n",
            "18025 Traning Loss: tensor(3.0158e-05)\n",
            "18026 Traning Loss: tensor(3.2656e-05)\n",
            "18027 Traning Loss: tensor(3.3006e-05)\n",
            "18028 Traning Loss: tensor(3.3439e-05)\n",
            "18029 Traning Loss: tensor(3.0815e-05)\n",
            "18030 Traning Loss: tensor(4.3083e-05)\n",
            "18031 Traning Loss: tensor(3.4652e-05)\n",
            "18032 Traning Loss: tensor(3.1265e-05)\n",
            "18033 Traning Loss: tensor(3.5688e-05)\n",
            "18034 Traning Loss: tensor(3.4775e-05)\n",
            "18035 Traning Loss: tensor(3.1494e-05)\n",
            "18036 Traning Loss: tensor(3.1833e-05)\n",
            "18037 Traning Loss: tensor(3.2902e-05)\n",
            "18038 Traning Loss: tensor(3.3859e-05)\n",
            "18039 Traning Loss: tensor(3.3634e-05)\n",
            "18040 Traning Loss: tensor(3.3982e-05)\n",
            "18041 Traning Loss: tensor(3.1164e-05)\n",
            "18042 Traning Loss: tensor(3.1878e-05)\n",
            "18043 Traning Loss: tensor(3.2445e-05)\n",
            "18044 Traning Loss: tensor(3.2498e-05)\n",
            "18045 Traning Loss: tensor(3.7690e-05)\n",
            "18046 Traning Loss: tensor(3.0146e-05)\n",
            "18047 Traning Loss: tensor(2.8491e-05)\n",
            "18048 Traning Loss: tensor(3.0948e-05)\n",
            "18049 Traning Loss: tensor(3.1570e-05)\n",
            "18050 Traning Loss: tensor(3.2939e-05)\n",
            "18051 Traning Loss: tensor(3.2220e-05)\n",
            "18052 Traning Loss: tensor(2.9123e-05)\n",
            "18053 Traning Loss: tensor(3.3223e-05)\n",
            "18054 Traning Loss: tensor(3.0249e-05)\n",
            "18055 Traning Loss: tensor(3.7371e-05)\n",
            "18056 Traning Loss: tensor(3.1761e-05)\n",
            "18057 Traning Loss: tensor(3.0871e-05)\n",
            "18058 Traning Loss: tensor(3.1966e-05)\n",
            "18059 Traning Loss: tensor(3.0299e-05)\n",
            "18060 Traning Loss: tensor(3.0885e-05)\n",
            "18061 Traning Loss: tensor(3.3183e-05)\n",
            "18062 Traning Loss: tensor(3.1442e-05)\n",
            "18063 Traning Loss: tensor(3.1274e-05)\n",
            "18064 Traning Loss: tensor(3.1538e-05)\n",
            "18065 Traning Loss: tensor(3.0633e-05)\n",
            "18066 Traning Loss: tensor(3.2341e-05)\n",
            "18067 Traning Loss: tensor(2.9760e-05)\n",
            "18068 Traning Loss: tensor(3.2655e-05)\n",
            "18069 Traning Loss: tensor(3.0197e-05)\n",
            "18070 Traning Loss: tensor(2.9325e-05)\n",
            "18071 Traning Loss: tensor(2.8517e-05)\n",
            "18072 Traning Loss: tensor(3.2938e-05)\n",
            "18073 Traning Loss: tensor(3.3043e-05)\n",
            "18074 Traning Loss: tensor(3.1294e-05)\n",
            "18075 Traning Loss: tensor(3.4436e-05)\n",
            "18076 Traning Loss: tensor(2.9314e-05)\n",
            "18077 Traning Loss: tensor(3.1402e-05)\n",
            "18078 Traning Loss: tensor(2.9846e-05)\n",
            "18079 Traning Loss: tensor(3.3817e-05)\n",
            "18080 Traning Loss: tensor(3.4302e-05)\n",
            "18081 Traning Loss: tensor(3.0403e-05)\n",
            "18082 Traning Loss: tensor(3.1392e-05)\n",
            "18083 Traning Loss: tensor(3.2154e-05)\n",
            "18084 Traning Loss: tensor(2.9782e-05)\n",
            "18085 Traning Loss: tensor(3.0994e-05)\n",
            "18086 Traning Loss: tensor(3.0403e-05)\n",
            "18087 Traning Loss: tensor(2.9500e-05)\n",
            "18088 Traning Loss: tensor(3.2588e-05)\n",
            "18089 Traning Loss: tensor(3.0348e-05)\n",
            "18090 Traning Loss: tensor(3.7201e-05)\n",
            "18091 Traning Loss: tensor(3.1254e-05)\n",
            "18092 Traning Loss: tensor(3.3499e-05)\n",
            "18093 Traning Loss: tensor(3.0210e-05)\n",
            "18094 Traning Loss: tensor(3.3695e-05)\n",
            "18095 Traning Loss: tensor(3.0027e-05)\n",
            "18096 Traning Loss: tensor(3.3086e-05)\n",
            "18097 Traning Loss: tensor(3.3038e-05)\n",
            "18098 Traning Loss: tensor(2.8513e-05)\n",
            "18099 Traning Loss: tensor(2.9868e-05)\n",
            "18100 Traning Loss: tensor(2.9521e-05)\n",
            "18101 Traning Loss: tensor(2.8908e-05)\n",
            "18102 Traning Loss: tensor(3.0520e-05)\n",
            "18103 Traning Loss: tensor(3.0546e-05)\n",
            "18104 Traning Loss: tensor(2.9205e-05)\n",
            "18105 Traning Loss: tensor(2.9917e-05)\n",
            "18106 Traning Loss: tensor(3.3317e-05)\n",
            "18107 Traning Loss: tensor(3.0396e-05)\n",
            "18108 Traning Loss: tensor(3.1621e-05)\n",
            "18109 Traning Loss: tensor(2.8652e-05)\n",
            "18110 Traning Loss: tensor(3.1741e-05)\n",
            "18111 Traning Loss: tensor(2.9711e-05)\n",
            "18112 Traning Loss: tensor(3.4061e-05)\n",
            "18113 Traning Loss: tensor(3.5101e-05)\n",
            "18114 Traning Loss: tensor(3.1661e-05)\n",
            "18115 Traning Loss: tensor(3.0969e-05)\n",
            "18116 Traning Loss: tensor(3.2029e-05)\n",
            "18117 Traning Loss: tensor(5.0621e-05)\n",
            "18118 Traning Loss: tensor(5.2059e-05)\n",
            "18119 Traning Loss: tensor(3.2915e-05)\n",
            "18120 Traning Loss: tensor(3.2694e-05)\n",
            "18121 Traning Loss: tensor(3.2402e-05)\n",
            "18122 Traning Loss: tensor(3.1125e-05)\n",
            "18123 Traning Loss: tensor(3.2903e-05)\n",
            "18124 Traning Loss: tensor(3.2726e-05)\n",
            "18125 Traning Loss: tensor(3.6651e-05)\n",
            "18126 Traning Loss: tensor(3.0072e-05)\n",
            "18127 Traning Loss: tensor(3.0702e-05)\n",
            "18128 Traning Loss: tensor(3.2329e-05)\n",
            "18129 Traning Loss: tensor(3.2161e-05)\n",
            "18130 Traning Loss: tensor(3.1647e-05)\n",
            "18131 Traning Loss: tensor(3.0218e-05)\n",
            "18132 Traning Loss: tensor(2.8627e-05)\n",
            "18133 Traning Loss: tensor(3.1122e-05)\n",
            "18134 Traning Loss: tensor(3.1512e-05)\n",
            "18135 Traning Loss: tensor(3.0440e-05)\n",
            "18136 Traning Loss: tensor(3.1626e-05)\n",
            "18137 Traning Loss: tensor(3.0040e-05)\n",
            "18138 Traning Loss: tensor(2.8482e-05)\n",
            "18139 Traning Loss: tensor(2.9211e-05)\n",
            "18140 Traning Loss: tensor(2.9316e-05)\n",
            "18141 Traning Loss: tensor(3.1913e-05)\n",
            "18142 Traning Loss: tensor(3.2239e-05)\n",
            "18143 Traning Loss: tensor(3.0166e-05)\n",
            "18144 Traning Loss: tensor(2.8945e-05)\n",
            "18145 Traning Loss: tensor(2.9013e-05)\n",
            "18146 Traning Loss: tensor(3.2214e-05)\n",
            "18147 Traning Loss: tensor(2.8502e-05)\n",
            "18148 Traning Loss: tensor(2.9899e-05)\n",
            "18149 Traning Loss: tensor(3.1636e-05)\n",
            "18150 Traning Loss: tensor(3.1818e-05)\n",
            "18151 Traning Loss: tensor(2.9414e-05)\n",
            "18152 Traning Loss: tensor(3.2222e-05)\n",
            "18153 Traning Loss: tensor(3.0292e-05)\n",
            "18154 Traning Loss: tensor(3.0542e-05)\n",
            "18155 Traning Loss: tensor(3.4845e-05)\n",
            "18156 Traning Loss: tensor(2.8824e-05)\n",
            "18157 Traning Loss: tensor(3.0881e-05)\n",
            "18158 Traning Loss: tensor(3.3926e-05)\n",
            "18159 Traning Loss: tensor(3.0458e-05)\n",
            "18160 Traning Loss: tensor(3.0939e-05)\n",
            "18161 Traning Loss: tensor(2.8675e-05)\n",
            "18162 Traning Loss: tensor(3.1937e-05)\n",
            "18163 Traning Loss: tensor(3.0808e-05)\n",
            "18164 Traning Loss: tensor(3.0014e-05)\n",
            "18165 Traning Loss: tensor(2.9621e-05)\n",
            "18166 Traning Loss: tensor(3.1743e-05)\n",
            "18167 Traning Loss: tensor(3.0146e-05)\n",
            "18168 Traning Loss: tensor(2.8953e-05)\n",
            "18169 Traning Loss: tensor(3.0446e-05)\n",
            "18170 Traning Loss: tensor(3.1493e-05)\n",
            "18171 Traning Loss: tensor(2.8487e-05)\n",
            "18172 Traning Loss: tensor(3.1973e-05)\n",
            "18173 Traning Loss: tensor(3.2938e-05)\n",
            "18174 Traning Loss: tensor(2.9878e-05)\n",
            "18175 Traning Loss: tensor(2.9972e-05)\n",
            "18176 Traning Loss: tensor(2.8501e-05)\n",
            "18177 Traning Loss: tensor(2.9575e-05)\n",
            "18178 Traning Loss: tensor(2.7882e-05)\n",
            "18179 Traning Loss: tensor(3.0827e-05)\n",
            "18180 Traning Loss: tensor(3.1718e-05)\n",
            "18181 Traning Loss: tensor(3.0631e-05)\n",
            "18182 Traning Loss: tensor(3.1478e-05)\n",
            "18183 Traning Loss: tensor(3.0031e-05)\n",
            "18184 Traning Loss: tensor(3.1004e-05)\n",
            "18185 Traning Loss: tensor(2.7662e-05)\n",
            "18186 Traning Loss: tensor(2.9025e-05)\n",
            "18187 Traning Loss: tensor(3.1199e-05)\n",
            "18188 Traning Loss: tensor(3.1333e-05)\n",
            "18189 Traning Loss: tensor(2.9377e-05)\n",
            "18190 Traning Loss: tensor(3.0825e-05)\n",
            "18191 Traning Loss: tensor(2.8821e-05)\n",
            "18192 Traning Loss: tensor(3.1601e-05)\n",
            "18193 Traning Loss: tensor(2.9487e-05)\n",
            "18194 Traning Loss: tensor(3.0942e-05)\n",
            "18195 Traning Loss: tensor(2.8411e-05)\n",
            "18196 Traning Loss: tensor(3.0272e-05)\n",
            "18197 Traning Loss: tensor(3.1202e-05)\n",
            "18198 Traning Loss: tensor(2.8422e-05)\n",
            "18199 Traning Loss: tensor(2.8768e-05)\n",
            "18200 Traning Loss: tensor(2.8275e-05)\n",
            "18201 Traning Loss: tensor(3.1495e-05)\n",
            "18202 Traning Loss: tensor(3.0844e-05)\n",
            "18203 Traning Loss: tensor(2.6845e-05)\n",
            "18204 Traning Loss: tensor(3.0859e-05)\n",
            "18205 Traning Loss: tensor(2.8619e-05)\n",
            "18206 Traning Loss: tensor(3.0435e-05)\n",
            "18207 Traning Loss: tensor(2.9728e-05)\n",
            "18208 Traning Loss: tensor(3.2829e-05)\n",
            "18209 Traning Loss: tensor(3.0646e-05)\n",
            "18210 Traning Loss: tensor(3.2415e-05)\n",
            "18211 Traning Loss: tensor(2.9383e-05)\n",
            "18212 Traning Loss: tensor(3.0027e-05)\n",
            "18213 Traning Loss: tensor(3.0787e-05)\n",
            "18214 Traning Loss: tensor(3.0359e-05)\n",
            "18215 Traning Loss: tensor(2.9731e-05)\n",
            "18216 Traning Loss: tensor(4.8009e-05)\n",
            "18217 Traning Loss: tensor(3.0047e-05)\n",
            "18218 Traning Loss: tensor(3.0347e-05)\n",
            "18219 Traning Loss: tensor(3.2305e-05)\n",
            "18220 Traning Loss: tensor(2.9636e-05)\n",
            "18221 Traning Loss: tensor(2.9631e-05)\n",
            "18222 Traning Loss: tensor(3.4353e-05)\n",
            "18223 Traning Loss: tensor(3.1539e-05)\n",
            "18224 Traning Loss: tensor(2.9540e-05)\n",
            "18225 Traning Loss: tensor(3.0542e-05)\n",
            "18226 Traning Loss: tensor(2.8824e-05)\n",
            "18227 Traning Loss: tensor(3.0462e-05)\n",
            "18228 Traning Loss: tensor(3.1355e-05)\n",
            "18229 Traning Loss: tensor(2.6994e-05)\n",
            "18230 Traning Loss: tensor(3.0829e-05)\n",
            "18231 Traning Loss: tensor(2.8513e-05)\n",
            "18232 Traning Loss: tensor(2.8101e-05)\n",
            "18233 Traning Loss: tensor(2.9942e-05)\n",
            "18234 Traning Loss: tensor(3.1335e-05)\n",
            "18235 Traning Loss: tensor(3.5266e-05)\n",
            "18236 Traning Loss: tensor(2.9457e-05)\n",
            "18237 Traning Loss: tensor(3.0507e-05)\n",
            "18238 Traning Loss: tensor(3.0878e-05)\n",
            "18239 Traning Loss: tensor(3.0460e-05)\n",
            "18240 Traning Loss: tensor(2.8049e-05)\n",
            "18241 Traning Loss: tensor(2.6966e-05)\n",
            "18242 Traning Loss: tensor(3.0460e-05)\n",
            "18243 Traning Loss: tensor(4.8131e-05)\n",
            "18244 Traning Loss: tensor(2.8364e-05)\n",
            "18245 Traning Loss: tensor(3.1572e-05)\n",
            "18246 Traning Loss: tensor(3.0884e-05)\n",
            "18247 Traning Loss: tensor(3.3222e-05)\n",
            "18248 Traning Loss: tensor(3.0612e-05)\n",
            "18249 Traning Loss: tensor(2.8529e-05)\n",
            "18250 Traning Loss: tensor(2.9150e-05)\n",
            "18251 Traning Loss: tensor(2.8502e-05)\n",
            "18252 Traning Loss: tensor(3.0238e-05)\n",
            "18253 Traning Loss: tensor(3.1092e-05)\n",
            "18254 Traning Loss: tensor(2.9826e-05)\n",
            "18255 Traning Loss: tensor(2.9499e-05)\n",
            "18256 Traning Loss: tensor(2.9234e-05)\n",
            "18257 Traning Loss: tensor(3.0036e-05)\n",
            "18258 Traning Loss: tensor(3.0543e-05)\n",
            "18259 Traning Loss: tensor(2.9328e-05)\n",
            "18260 Traning Loss: tensor(3.0084e-05)\n",
            "18261 Traning Loss: tensor(3.1673e-05)\n",
            "18262 Traning Loss: tensor(2.7540e-05)\n",
            "18263 Traning Loss: tensor(3.0066e-05)\n",
            "18264 Traning Loss: tensor(2.9188e-05)\n",
            "18265 Traning Loss: tensor(3.0306e-05)\n",
            "18266 Traning Loss: tensor(2.6366e-05)\n",
            "18267 Traning Loss: tensor(3.0063e-05)\n",
            "18268 Traning Loss: tensor(2.9415e-05)\n",
            "18269 Traning Loss: tensor(2.7959e-05)\n",
            "18270 Traning Loss: tensor(2.8789e-05)\n",
            "18271 Traning Loss: tensor(2.6557e-05)\n",
            "18272 Traning Loss: tensor(3.0863e-05)\n",
            "18273 Traning Loss: tensor(2.9001e-05)\n",
            "18274 Traning Loss: tensor(2.8401e-05)\n",
            "18275 Traning Loss: tensor(2.7813e-05)\n",
            "18276 Traning Loss: tensor(2.7679e-05)\n",
            "18277 Traning Loss: tensor(3.6428e-05)\n",
            "18278 Traning Loss: tensor(2.9672e-05)\n",
            "18279 Traning Loss: tensor(2.8685e-05)\n",
            "18280 Traning Loss: tensor(2.9017e-05)\n",
            "18281 Traning Loss: tensor(2.8080e-05)\n",
            "18282 Traning Loss: tensor(2.7639e-05)\n",
            "18283 Traning Loss: tensor(2.8290e-05)\n",
            "18284 Traning Loss: tensor(2.6457e-05)\n",
            "18285 Traning Loss: tensor(2.8047e-05)\n",
            "18286 Traning Loss: tensor(3.0124e-05)\n",
            "18287 Traning Loss: tensor(3.0174e-05)\n",
            "18288 Traning Loss: tensor(2.9870e-05)\n",
            "18289 Traning Loss: tensor(2.7291e-05)\n",
            "18290 Traning Loss: tensor(2.9564e-05)\n",
            "18291 Traning Loss: tensor(3.0203e-05)\n",
            "18292 Traning Loss: tensor(3.1043e-05)\n",
            "18293 Traning Loss: tensor(2.7295e-05)\n",
            "18294 Traning Loss: tensor(4.2257e-05)\n",
            "18295 Traning Loss: tensor(3.0161e-05)\n",
            "18296 Traning Loss: tensor(2.8769e-05)\n",
            "18297 Traning Loss: tensor(2.9077e-05)\n",
            "18298 Traning Loss: tensor(3.5295e-05)\n",
            "18299 Traning Loss: tensor(3.1051e-05)\n",
            "18300 Traning Loss: tensor(3.0141e-05)\n",
            "18301 Traning Loss: tensor(2.6653e-05)\n",
            "18302 Traning Loss: tensor(2.9874e-05)\n",
            "18303 Traning Loss: tensor(2.8370e-05)\n",
            "18304 Traning Loss: tensor(2.8545e-05)\n",
            "18305 Traning Loss: tensor(2.8543e-05)\n",
            "18306 Traning Loss: tensor(2.8680e-05)\n",
            "18307 Traning Loss: tensor(2.8907e-05)\n",
            "18308 Traning Loss: tensor(2.9303e-05)\n",
            "18309 Traning Loss: tensor(2.8445e-05)\n",
            "18310 Traning Loss: tensor(2.9462e-05)\n",
            "18311 Traning Loss: tensor(2.9509e-05)\n",
            "18312 Traning Loss: tensor(2.8504e-05)\n",
            "18313 Traning Loss: tensor(2.8869e-05)\n",
            "18314 Traning Loss: tensor(2.9525e-05)\n",
            "18315 Traning Loss: tensor(3.1233e-05)\n",
            "18316 Traning Loss: tensor(2.8224e-05)\n",
            "18317 Traning Loss: tensor(3.0017e-05)\n",
            "18318 Traning Loss: tensor(2.9024e-05)\n",
            "18319 Traning Loss: tensor(2.7768e-05)\n",
            "18320 Traning Loss: tensor(2.7835e-05)\n",
            "18321 Traning Loss: tensor(2.7214e-05)\n",
            "18322 Traning Loss: tensor(2.8912e-05)\n",
            "18323 Traning Loss: tensor(3.0365e-05)\n",
            "18324 Traning Loss: tensor(2.9907e-05)\n",
            "18325 Traning Loss: tensor(2.8385e-05)\n",
            "18326 Traning Loss: tensor(2.7422e-05)\n",
            "18327 Traning Loss: tensor(2.7601e-05)\n",
            "18328 Traning Loss: tensor(2.7569e-05)\n",
            "18329 Traning Loss: tensor(2.7032e-05)\n",
            "18330 Traning Loss: tensor(2.7458e-05)\n",
            "18331 Traning Loss: tensor(3.0531e-05)\n",
            "18332 Traning Loss: tensor(2.7981e-05)\n",
            "18333 Traning Loss: tensor(2.7692e-05)\n",
            "18334 Traning Loss: tensor(2.8612e-05)\n",
            "18335 Traning Loss: tensor(2.9827e-05)\n",
            "18336 Traning Loss: tensor(2.8692e-05)\n",
            "18337 Traning Loss: tensor(2.6522e-05)\n",
            "18338 Traning Loss: tensor(3.6468e-05)\n",
            "18339 Traning Loss: tensor(2.9430e-05)\n",
            "18340 Traning Loss: tensor(2.8899e-05)\n",
            "18341 Traning Loss: tensor(2.9714e-05)\n",
            "18342 Traning Loss: tensor(2.8837e-05)\n",
            "18343 Traning Loss: tensor(2.9821e-05)\n",
            "18344 Traning Loss: tensor(2.7974e-05)\n",
            "18345 Traning Loss: tensor(2.9195e-05)\n",
            "18346 Traning Loss: tensor(2.8398e-05)\n",
            "18347 Traning Loss: tensor(2.6963e-05)\n",
            "18348 Traning Loss: tensor(2.8963e-05)\n",
            "18349 Traning Loss: tensor(2.8688e-05)\n",
            "18350 Traning Loss: tensor(2.7853e-05)\n",
            "18351 Traning Loss: tensor(2.5342e-05)\n",
            "18352 Traning Loss: tensor(2.7430e-05)\n",
            "18353 Traning Loss: tensor(2.6199e-05)\n",
            "18354 Traning Loss: tensor(2.7639e-05)\n",
            "18355 Traning Loss: tensor(2.7921e-05)\n",
            "18356 Traning Loss: tensor(2.7995e-05)\n",
            "18357 Traning Loss: tensor(2.8090e-05)\n",
            "18358 Traning Loss: tensor(2.8861e-05)\n",
            "18359 Traning Loss: tensor(2.7204e-05)\n",
            "18360 Traning Loss: tensor(3.0181e-05)\n",
            "18361 Traning Loss: tensor(2.8791e-05)\n",
            "18362 Traning Loss: tensor(3.0145e-05)\n",
            "18363 Traning Loss: tensor(2.9579e-05)\n",
            "18364 Traning Loss: tensor(2.9074e-05)\n",
            "18365 Traning Loss: tensor(2.8010e-05)\n",
            "18366 Traning Loss: tensor(2.7355e-05)\n",
            "18367 Traning Loss: tensor(3.0513e-05)\n",
            "18368 Traning Loss: tensor(3.4163e-05)\n",
            "18369 Traning Loss: tensor(3.1627e-05)\n",
            "18370 Traning Loss: tensor(3.0303e-05)\n",
            "18371 Traning Loss: tensor(2.7616e-05)\n",
            "18372 Traning Loss: tensor(2.9583e-05)\n",
            "18373 Traning Loss: tensor(3.1668e-05)\n",
            "18374 Traning Loss: tensor(3.4504e-05)\n",
            "18375 Traning Loss: tensor(4.9789e-05)\n",
            "18376 Traning Loss: tensor(3.1611e-05)\n",
            "18377 Traning Loss: tensor(3.3307e-05)\n",
            "18378 Traning Loss: tensor(3.0447e-05)\n",
            "18379 Traning Loss: tensor(2.8677e-05)\n",
            "18380 Traning Loss: tensor(3.0115e-05)\n",
            "18381 Traning Loss: tensor(3.5776e-05)\n",
            "18382 Traning Loss: tensor(2.8940e-05)\n",
            "18383 Traning Loss: tensor(2.7816e-05)\n",
            "18384 Traning Loss: tensor(2.8507e-05)\n",
            "18385 Traning Loss: tensor(2.8893e-05)\n",
            "18386 Traning Loss: tensor(3.4166e-05)\n",
            "18387 Traning Loss: tensor(2.8145e-05)\n",
            "18388 Traning Loss: tensor(2.9560e-05)\n",
            "18389 Traning Loss: tensor(3.0090e-05)\n",
            "18390 Traning Loss: tensor(2.7544e-05)\n",
            "18391 Traning Loss: tensor(2.8505e-05)\n",
            "18392 Traning Loss: tensor(3.1011e-05)\n",
            "18393 Traning Loss: tensor(3.0600e-05)\n",
            "18394 Traning Loss: tensor(3.3064e-05)\n",
            "18395 Traning Loss: tensor(2.9044e-05)\n",
            "18396 Traning Loss: tensor(3.0456e-05)\n",
            "18397 Traning Loss: tensor(3.2834e-05)\n",
            "18398 Traning Loss: tensor(2.7687e-05)\n",
            "18399 Traning Loss: tensor(2.9332e-05)\n",
            "18400 Traning Loss: tensor(3.0650e-05)\n",
            "18401 Traning Loss: tensor(2.9842e-05)\n",
            "18402 Traning Loss: tensor(2.9949e-05)\n",
            "18403 Traning Loss: tensor(2.7773e-05)\n",
            "18404 Traning Loss: tensor(2.6694e-05)\n",
            "18405 Traning Loss: tensor(2.6026e-05)\n",
            "18406 Traning Loss: tensor(2.7857e-05)\n",
            "18407 Traning Loss: tensor(2.9503e-05)\n",
            "18408 Traning Loss: tensor(3.0088e-05)\n",
            "18409 Traning Loss: tensor(2.8806e-05)\n",
            "18410 Traning Loss: tensor(2.6452e-05)\n",
            "18411 Traning Loss: tensor(2.9427e-05)\n",
            "18412 Traning Loss: tensor(2.6192e-05)\n",
            "18413 Traning Loss: tensor(2.8135e-05)\n",
            "18414 Traning Loss: tensor(3.3776e-05)\n",
            "18415 Traning Loss: tensor(2.6389e-05)\n",
            "18416 Traning Loss: tensor(2.9975e-05)\n",
            "18417 Traning Loss: tensor(2.8826e-05)\n",
            "18418 Traning Loss: tensor(2.7553e-05)\n",
            "18419 Traning Loss: tensor(2.7297e-05)\n",
            "18420 Traning Loss: tensor(3.2583e-05)\n",
            "18421 Traning Loss: tensor(3.0800e-05)\n",
            "18422 Traning Loss: tensor(2.7834e-05)\n",
            "18423 Traning Loss: tensor(2.6716e-05)\n",
            "18424 Traning Loss: tensor(2.8472e-05)\n",
            "18425 Traning Loss: tensor(2.9552e-05)\n",
            "18426 Traning Loss: tensor(2.8462e-05)\n",
            "18427 Traning Loss: tensor(2.9605e-05)\n",
            "18428 Traning Loss: tensor(2.7064e-05)\n",
            "18429 Traning Loss: tensor(2.7434e-05)\n",
            "18430 Traning Loss: tensor(2.8439e-05)\n",
            "18431 Traning Loss: tensor(3.1324e-05)\n",
            "18432 Traning Loss: tensor(2.6689e-05)\n",
            "18433 Traning Loss: tensor(4.6245e-05)\n",
            "18434 Traning Loss: tensor(2.9128e-05)\n",
            "18435 Traning Loss: tensor(2.7995e-05)\n",
            "18436 Traning Loss: tensor(2.8308e-05)\n",
            "18437 Traning Loss: tensor(2.7178e-05)\n",
            "18438 Traning Loss: tensor(2.6350e-05)\n",
            "18439 Traning Loss: tensor(2.7975e-05)\n",
            "18440 Traning Loss: tensor(2.8355e-05)\n",
            "18441 Traning Loss: tensor(2.6697e-05)\n",
            "18442 Traning Loss: tensor(2.8200e-05)\n",
            "18443 Traning Loss: tensor(2.7560e-05)\n",
            "18444 Traning Loss: tensor(3.1532e-05)\n",
            "18445 Traning Loss: tensor(2.8939e-05)\n",
            "18446 Traning Loss: tensor(2.9709e-05)\n",
            "18447 Traning Loss: tensor(2.9495e-05)\n",
            "18448 Traning Loss: tensor(2.9454e-05)\n",
            "18449 Traning Loss: tensor(2.8051e-05)\n",
            "18450 Traning Loss: tensor(2.7269e-05)\n",
            "18451 Traning Loss: tensor(2.4949e-05)\n",
            "18452 Traning Loss: tensor(2.5538e-05)\n",
            "18453 Traning Loss: tensor(2.7545e-05)\n",
            "18454 Traning Loss: tensor(2.7016e-05)\n",
            "18455 Traning Loss: tensor(2.6561e-05)\n",
            "18456 Traning Loss: tensor(2.7868e-05)\n",
            "18457 Traning Loss: tensor(2.7735e-05)\n",
            "18458 Traning Loss: tensor(2.6925e-05)\n",
            "18459 Traning Loss: tensor(2.7827e-05)\n",
            "18460 Traning Loss: tensor(2.5927e-05)\n",
            "18461 Traning Loss: tensor(3.0418e-05)\n",
            "18462 Traning Loss: tensor(2.7896e-05)\n",
            "18463 Traning Loss: tensor(2.8646e-05)\n",
            "18464 Traning Loss: tensor(2.7036e-05)\n",
            "18465 Traning Loss: tensor(2.6363e-05)\n",
            "18466 Traning Loss: tensor(2.8657e-05)\n",
            "18467 Traning Loss: tensor(2.7654e-05)\n",
            "18468 Traning Loss: tensor(2.5097e-05)\n",
            "18469 Traning Loss: tensor(2.6190e-05)\n",
            "18470 Traning Loss: tensor(2.8304e-05)\n",
            "18471 Traning Loss: tensor(2.9057e-05)\n",
            "18472 Traning Loss: tensor(2.7004e-05)\n",
            "18473 Traning Loss: tensor(3.4331e-05)\n",
            "18474 Traning Loss: tensor(3.0178e-05)\n",
            "18475 Traning Loss: tensor(2.8167e-05)\n",
            "18476 Traning Loss: tensor(2.8645e-05)\n",
            "18477 Traning Loss: tensor(2.7284e-05)\n",
            "18478 Traning Loss: tensor(2.8562e-05)\n",
            "18479 Traning Loss: tensor(2.5924e-05)\n",
            "18480 Traning Loss: tensor(2.7013e-05)\n",
            "18481 Traning Loss: tensor(2.7305e-05)\n",
            "18482 Traning Loss: tensor(3.0130e-05)\n",
            "18483 Traning Loss: tensor(2.7390e-05)\n",
            "18484 Traning Loss: tensor(2.6266e-05)\n",
            "18485 Traning Loss: tensor(2.5888e-05)\n",
            "18486 Traning Loss: tensor(2.7023e-05)\n",
            "18487 Traning Loss: tensor(2.5800e-05)\n",
            "18488 Traning Loss: tensor(2.9040e-05)\n",
            "18489 Traning Loss: tensor(2.9465e-05)\n",
            "18490 Traning Loss: tensor(2.8795e-05)\n",
            "18491 Traning Loss: tensor(2.6463e-05)\n",
            "18492 Traning Loss: tensor(3.2447e-05)\n",
            "18493 Traning Loss: tensor(2.8041e-05)\n",
            "18494 Traning Loss: tensor(2.9298e-05)\n",
            "18495 Traning Loss: tensor(2.8596e-05)\n",
            "18496 Traning Loss: tensor(3.0951e-05)\n",
            "18497 Traning Loss: tensor(2.5938e-05)\n",
            "18498 Traning Loss: tensor(2.7146e-05)\n",
            "18499 Traning Loss: tensor(2.7691e-05)\n",
            "18500 Traning Loss: tensor(4.1168e-05)\n",
            "18501 Traning Loss: tensor(3.3347e-05)\n",
            "18502 Traning Loss: tensor(2.8999e-05)\n",
            "18503 Traning Loss: tensor(3.1245e-05)\n",
            "18504 Traning Loss: tensor(3.3352e-05)\n",
            "18505 Traning Loss: tensor(3.2457e-05)\n",
            "18506 Traning Loss: tensor(2.7857e-05)\n",
            "18507 Traning Loss: tensor(2.9876e-05)\n",
            "18508 Traning Loss: tensor(3.1847e-05)\n",
            "18509 Traning Loss: tensor(3.2769e-05)\n",
            "18510 Traning Loss: tensor(2.7157e-05)\n",
            "18511 Traning Loss: tensor(3.1117e-05)\n",
            "18512 Traning Loss: tensor(3.0355e-05)\n",
            "18513 Traning Loss: tensor(3.1664e-05)\n",
            "18514 Traning Loss: tensor(2.9450e-05)\n",
            "18515 Traning Loss: tensor(2.9671e-05)\n",
            "18516 Traning Loss: tensor(3.2185e-05)\n",
            "18517 Traning Loss: tensor(3.1305e-05)\n",
            "18518 Traning Loss: tensor(2.7890e-05)\n",
            "18519 Traning Loss: tensor(2.7781e-05)\n",
            "18520 Traning Loss: tensor(3.1896e-05)\n",
            "18521 Traning Loss: tensor(3.0755e-05)\n",
            "18522 Traning Loss: tensor(3.4523e-05)\n",
            "18523 Traning Loss: tensor(2.9499e-05)\n",
            "18524 Traning Loss: tensor(2.7565e-05)\n",
            "18525 Traning Loss: tensor(2.7162e-05)\n",
            "18526 Traning Loss: tensor(2.8517e-05)\n",
            "18527 Traning Loss: tensor(3.3898e-05)\n",
            "18528 Traning Loss: tensor(2.9365e-05)\n",
            "18529 Traning Loss: tensor(2.8261e-05)\n",
            "18530 Traning Loss: tensor(2.7933e-05)\n",
            "18531 Traning Loss: tensor(2.9567e-05)\n",
            "18532 Traning Loss: tensor(2.8378e-05)\n",
            "18533 Traning Loss: tensor(3.0945e-05)\n",
            "18534 Traning Loss: tensor(2.6688e-05)\n",
            "18535 Traning Loss: tensor(2.9566e-05)\n",
            "18536 Traning Loss: tensor(2.8380e-05)\n",
            "18537 Traning Loss: tensor(2.7713e-05)\n",
            "18538 Traning Loss: tensor(2.9885e-05)\n",
            "18539 Traning Loss: tensor(2.5471e-05)\n",
            "18540 Traning Loss: tensor(2.7184e-05)\n",
            "18541 Traning Loss: tensor(2.7303e-05)\n",
            "18542 Traning Loss: tensor(2.7825e-05)\n",
            "18543 Traning Loss: tensor(2.7979e-05)\n",
            "18544 Traning Loss: tensor(2.5754e-05)\n",
            "18545 Traning Loss: tensor(2.7537e-05)\n",
            "18546 Traning Loss: tensor(2.7601e-05)\n",
            "18547 Traning Loss: tensor(2.7125e-05)\n",
            "18548 Traning Loss: tensor(2.5595e-05)\n",
            "18549 Traning Loss: tensor(2.5703e-05)\n",
            "18550 Traning Loss: tensor(2.6934e-05)\n",
            "18551 Traning Loss: tensor(2.6457e-05)\n",
            "18552 Traning Loss: tensor(2.7293e-05)\n",
            "18553 Traning Loss: tensor(2.7380e-05)\n",
            "18554 Traning Loss: tensor(2.4729e-05)\n",
            "18555 Traning Loss: tensor(2.5173e-05)\n",
            "18556 Traning Loss: tensor(2.5706e-05)\n",
            "18557 Traning Loss: tensor(2.9673e-05)\n",
            "18558 Traning Loss: tensor(2.6889e-05)\n",
            "18559 Traning Loss: tensor(2.4636e-05)\n",
            "18560 Traning Loss: tensor(2.5507e-05)\n",
            "18561 Traning Loss: tensor(2.7843e-05)\n",
            "18562 Traning Loss: tensor(2.8128e-05)\n",
            "18563 Traning Loss: tensor(2.7055e-05)\n",
            "18564 Traning Loss: tensor(2.5550e-05)\n",
            "18565 Traning Loss: tensor(2.7036e-05)\n",
            "18566 Traning Loss: tensor(2.8176e-05)\n",
            "18567 Traning Loss: tensor(2.9335e-05)\n",
            "18568 Traning Loss: tensor(2.5843e-05)\n",
            "18569 Traning Loss: tensor(2.7978e-05)\n",
            "18570 Traning Loss: tensor(2.6102e-05)\n",
            "18571 Traning Loss: tensor(2.7047e-05)\n",
            "18572 Traning Loss: tensor(2.6031e-05)\n",
            "18573 Traning Loss: tensor(2.7528e-05)\n",
            "18574 Traning Loss: tensor(2.6182e-05)\n",
            "18575 Traning Loss: tensor(2.5875e-05)\n",
            "18576 Traning Loss: tensor(2.6577e-05)\n",
            "18577 Traning Loss: tensor(2.4842e-05)\n",
            "18578 Traning Loss: tensor(2.2302e-05)\n",
            "18579 Traning Loss: tensor(2.6921e-05)\n",
            "18580 Traning Loss: tensor(2.6368e-05)\n",
            "18581 Traning Loss: tensor(2.5725e-05)\n",
            "18582 Traning Loss: tensor(2.7415e-05)\n",
            "18583 Traning Loss: tensor(2.8172e-05)\n",
            "18584 Traning Loss: tensor(2.7757e-05)\n",
            "18585 Traning Loss: tensor(2.7640e-05)\n",
            "18586 Traning Loss: tensor(2.7526e-05)\n",
            "18587 Traning Loss: tensor(2.8038e-05)\n",
            "18588 Traning Loss: tensor(2.8556e-05)\n",
            "18589 Traning Loss: tensor(2.5111e-05)\n",
            "18590 Traning Loss: tensor(2.6658e-05)\n",
            "18591 Traning Loss: tensor(2.6637e-05)\n",
            "18592 Traning Loss: tensor(2.5705e-05)\n",
            "18593 Traning Loss: tensor(2.6626e-05)\n",
            "18594 Traning Loss: tensor(2.7061e-05)\n",
            "18595 Traning Loss: tensor(2.6336e-05)\n",
            "18596 Traning Loss: tensor(3.6491e-05)\n",
            "18597 Traning Loss: tensor(2.7300e-05)\n",
            "18598 Traning Loss: tensor(2.8793e-05)\n",
            "18599 Traning Loss: tensor(2.6463e-05)\n",
            "18600 Traning Loss: tensor(2.5513e-05)\n",
            "18601 Traning Loss: tensor(2.7708e-05)\n",
            "18602 Traning Loss: tensor(2.7647e-05)\n",
            "18603 Traning Loss: tensor(2.6253e-05)\n",
            "18604 Traning Loss: tensor(3.5667e-05)\n",
            "18605 Traning Loss: tensor(2.4750e-05)\n",
            "18606 Traning Loss: tensor(2.6463e-05)\n",
            "18607 Traning Loss: tensor(2.5713e-05)\n",
            "18608 Traning Loss: tensor(2.4820e-05)\n",
            "18609 Traning Loss: tensor(2.7068e-05)\n",
            "18610 Traning Loss: tensor(2.7048e-05)\n",
            "18611 Traning Loss: tensor(2.4425e-05)\n",
            "18612 Traning Loss: tensor(2.6124e-05)\n",
            "18613 Traning Loss: tensor(2.6156e-05)\n",
            "18614 Traning Loss: tensor(2.5815e-05)\n",
            "18615 Traning Loss: tensor(2.6856e-05)\n",
            "18616 Traning Loss: tensor(2.7200e-05)\n",
            "18617 Traning Loss: tensor(2.5989e-05)\n",
            "18618 Traning Loss: tensor(2.7642e-05)\n",
            "18619 Traning Loss: tensor(2.7259e-05)\n",
            "18620 Traning Loss: tensor(3.0496e-05)\n",
            "18621 Traning Loss: tensor(2.6295e-05)\n",
            "18622 Traning Loss: tensor(2.6622e-05)\n",
            "18623 Traning Loss: tensor(2.8699e-05)\n",
            "18624 Traning Loss: tensor(3.0309e-05)\n",
            "18625 Traning Loss: tensor(2.5915e-05)\n",
            "18626 Traning Loss: tensor(2.7667e-05)\n",
            "18627 Traning Loss: tensor(2.5809e-05)\n",
            "18628 Traning Loss: tensor(2.9766e-05)\n",
            "18629 Traning Loss: tensor(2.9570e-05)\n",
            "18630 Traning Loss: tensor(2.7714e-05)\n",
            "18631 Traning Loss: tensor(2.6805e-05)\n",
            "18632 Traning Loss: tensor(2.7801e-05)\n",
            "18633 Traning Loss: tensor(2.7347e-05)\n",
            "18634 Traning Loss: tensor(2.8204e-05)\n",
            "18635 Traning Loss: tensor(2.6257e-05)\n",
            "18636 Traning Loss: tensor(2.7734e-05)\n",
            "18637 Traning Loss: tensor(3.1612e-05)\n",
            "18638 Traning Loss: tensor(2.8504e-05)\n",
            "18639 Traning Loss: tensor(2.7877e-05)\n",
            "18640 Traning Loss: tensor(2.6514e-05)\n",
            "18641 Traning Loss: tensor(2.5891e-05)\n",
            "18642 Traning Loss: tensor(2.9973e-05)\n",
            "18643 Traning Loss: tensor(2.8371e-05)\n",
            "18644 Traning Loss: tensor(2.5769e-05)\n",
            "18645 Traning Loss: tensor(2.5779e-05)\n",
            "18646 Traning Loss: tensor(3.0135e-05)\n",
            "18647 Traning Loss: tensor(2.9073e-05)\n",
            "18648 Traning Loss: tensor(2.8424e-05)\n",
            "18649 Traning Loss: tensor(2.7160e-05)\n",
            "18650 Traning Loss: tensor(2.6515e-05)\n",
            "18651 Traning Loss: tensor(2.5882e-05)\n",
            "18652 Traning Loss: tensor(2.6440e-05)\n",
            "18653 Traning Loss: tensor(2.7081e-05)\n",
            "18654 Traning Loss: tensor(2.6127e-05)\n",
            "18655 Traning Loss: tensor(2.6329e-05)\n",
            "18656 Traning Loss: tensor(2.5608e-05)\n",
            "18657 Traning Loss: tensor(2.7902e-05)\n",
            "18658 Traning Loss: tensor(2.6550e-05)\n",
            "18659 Traning Loss: tensor(3.3061e-05)\n",
            "18660 Traning Loss: tensor(2.6724e-05)\n",
            "18661 Traning Loss: tensor(2.7153e-05)\n",
            "18662 Traning Loss: tensor(2.5655e-05)\n",
            "18663 Traning Loss: tensor(2.5189e-05)\n",
            "18664 Traning Loss: tensor(2.5944e-05)\n",
            "18665 Traning Loss: tensor(2.5971e-05)\n",
            "18666 Traning Loss: tensor(2.7048e-05)\n",
            "18667 Traning Loss: tensor(2.6521e-05)\n",
            "18668 Traning Loss: tensor(2.5407e-05)\n",
            "18669 Traning Loss: tensor(2.7231e-05)\n",
            "18670 Traning Loss: tensor(2.7723e-05)\n",
            "18671 Traning Loss: tensor(2.8792e-05)\n",
            "18672 Traning Loss: tensor(2.5477e-05)\n",
            "18673 Traning Loss: tensor(2.6851e-05)\n",
            "18674 Traning Loss: tensor(2.6318e-05)\n",
            "18675 Traning Loss: tensor(2.7275e-05)\n",
            "18676 Traning Loss: tensor(2.5427e-05)\n",
            "18677 Traning Loss: tensor(2.4936e-05)\n",
            "18678 Traning Loss: tensor(2.7180e-05)\n",
            "18679 Traning Loss: tensor(2.9709e-05)\n",
            "18680 Traning Loss: tensor(2.3897e-05)\n",
            "18681 Traning Loss: tensor(2.6954e-05)\n",
            "18682 Traning Loss: tensor(2.6761e-05)\n",
            "18683 Traning Loss: tensor(2.7114e-05)\n",
            "18684 Traning Loss: tensor(2.6915e-05)\n",
            "18685 Traning Loss: tensor(2.6467e-05)\n",
            "18686 Traning Loss: tensor(2.6089e-05)\n",
            "18687 Traning Loss: tensor(2.4096e-05)\n",
            "18688 Traning Loss: tensor(2.9666e-05)\n",
            "18689 Traning Loss: tensor(2.5152e-05)\n",
            "18690 Traning Loss: tensor(2.6686e-05)\n",
            "18691 Traning Loss: tensor(2.6205e-05)\n",
            "18692 Traning Loss: tensor(2.6443e-05)\n",
            "18693 Traning Loss: tensor(2.8215e-05)\n",
            "18694 Traning Loss: tensor(2.5865e-05)\n",
            "18695 Traning Loss: tensor(2.4970e-05)\n",
            "18696 Traning Loss: tensor(2.6259e-05)\n",
            "18697 Traning Loss: tensor(2.6676e-05)\n",
            "18698 Traning Loss: tensor(2.4021e-05)\n",
            "18699 Traning Loss: tensor(2.6458e-05)\n",
            "18700 Traning Loss: tensor(2.6508e-05)\n",
            "18701 Traning Loss: tensor(2.7644e-05)\n",
            "18702 Traning Loss: tensor(2.4950e-05)\n",
            "18703 Traning Loss: tensor(2.4571e-05)\n",
            "18704 Traning Loss: tensor(2.5218e-05)\n",
            "18705 Traning Loss: tensor(2.5667e-05)\n",
            "18706 Traning Loss: tensor(2.5638e-05)\n",
            "18707 Traning Loss: tensor(2.7412e-05)\n",
            "18708 Traning Loss: tensor(2.5800e-05)\n",
            "18709 Traning Loss: tensor(2.6850e-05)\n",
            "18710 Traning Loss: tensor(2.6175e-05)\n",
            "18711 Traning Loss: tensor(2.6082e-05)\n",
            "18712 Traning Loss: tensor(2.4876e-05)\n",
            "18713 Traning Loss: tensor(2.4845e-05)\n",
            "18714 Traning Loss: tensor(2.5735e-05)\n",
            "18715 Traning Loss: tensor(2.6409e-05)\n",
            "18716 Traning Loss: tensor(2.4888e-05)\n",
            "18717 Traning Loss: tensor(2.5145e-05)\n",
            "18718 Traning Loss: tensor(2.6947e-05)\n",
            "18719 Traning Loss: tensor(2.5110e-05)\n",
            "18720 Traning Loss: tensor(2.6698e-05)\n",
            "18721 Traning Loss: tensor(2.6680e-05)\n",
            "18722 Traning Loss: tensor(2.6650e-05)\n",
            "18723 Traning Loss: tensor(2.7528e-05)\n",
            "18724 Traning Loss: tensor(2.6860e-05)\n",
            "18725 Traning Loss: tensor(2.6787e-05)\n",
            "18726 Traning Loss: tensor(2.7627e-05)\n",
            "18727 Traning Loss: tensor(2.6808e-05)\n",
            "18728 Traning Loss: tensor(2.5137e-05)\n",
            "18729 Traning Loss: tensor(2.3761e-05)\n",
            "18730 Traning Loss: tensor(2.5381e-05)\n",
            "18731 Traning Loss: tensor(2.6475e-05)\n",
            "18732 Traning Loss: tensor(2.3522e-05)\n",
            "18733 Traning Loss: tensor(2.6513e-05)\n",
            "18734 Traning Loss: tensor(2.5452e-05)\n",
            "18735 Traning Loss: tensor(2.6109e-05)\n",
            "18736 Traning Loss: tensor(2.5297e-05)\n",
            "18737 Traning Loss: tensor(2.7929e-05)\n",
            "18738 Traning Loss: tensor(2.5439e-05)\n",
            "18739 Traning Loss: tensor(2.6072e-05)\n",
            "18740 Traning Loss: tensor(2.3401e-05)\n",
            "18741 Traning Loss: tensor(2.5344e-05)\n",
            "18742 Traning Loss: tensor(2.5896e-05)\n",
            "18743 Traning Loss: tensor(2.6255e-05)\n",
            "18744 Traning Loss: tensor(2.6234e-05)\n",
            "18745 Traning Loss: tensor(2.6646e-05)\n",
            "18746 Traning Loss: tensor(2.5840e-05)\n",
            "18747 Traning Loss: tensor(2.6338e-05)\n",
            "18748 Traning Loss: tensor(2.4996e-05)\n",
            "18749 Traning Loss: tensor(2.4114e-05)\n",
            "18750 Traning Loss: tensor(2.5296e-05)\n",
            "18751 Traning Loss: tensor(2.6253e-05)\n",
            "18752 Traning Loss: tensor(2.4905e-05)\n",
            "18753 Traning Loss: tensor(2.5311e-05)\n",
            "18754 Traning Loss: tensor(2.6021e-05)\n",
            "18755 Traning Loss: tensor(3.0106e-05)\n",
            "18756 Traning Loss: tensor(2.6024e-05)\n",
            "18757 Traning Loss: tensor(2.6552e-05)\n",
            "18758 Traning Loss: tensor(3.0132e-05)\n",
            "18759 Traning Loss: tensor(2.5431e-05)\n",
            "18760 Traning Loss: tensor(2.8289e-05)\n",
            "18761 Traning Loss: tensor(2.4347e-05)\n",
            "18762 Traning Loss: tensor(2.7233e-05)\n",
            "18763 Traning Loss: tensor(2.4758e-05)\n",
            "18764 Traning Loss: tensor(2.5198e-05)\n",
            "18765 Traning Loss: tensor(2.6983e-05)\n",
            "18766 Traning Loss: tensor(2.8282e-05)\n",
            "18767 Traning Loss: tensor(2.5867e-05)\n",
            "18768 Traning Loss: tensor(2.4458e-05)\n",
            "18769 Traning Loss: tensor(2.5866e-05)\n",
            "18770 Traning Loss: tensor(2.7469e-05)\n",
            "18771 Traning Loss: tensor(2.7718e-05)\n",
            "18772 Traning Loss: tensor(2.6284e-05)\n",
            "18773 Traning Loss: tensor(2.6181e-05)\n",
            "18774 Traning Loss: tensor(2.6623e-05)\n",
            "18775 Traning Loss: tensor(2.7124e-05)\n",
            "18776 Traning Loss: tensor(2.6038e-05)\n",
            "18777 Traning Loss: tensor(2.7775e-05)\n",
            "18778 Traning Loss: tensor(2.6511e-05)\n",
            "18779 Traning Loss: tensor(2.5570e-05)\n",
            "18780 Traning Loss: tensor(3.1081e-05)\n",
            "18781 Traning Loss: tensor(2.5658e-05)\n",
            "18782 Traning Loss: tensor(3.1352e-05)\n",
            "18783 Traning Loss: tensor(2.6453e-05)\n",
            "18784 Traning Loss: tensor(2.6805e-05)\n",
            "18785 Traning Loss: tensor(2.5266e-05)\n",
            "18786 Traning Loss: tensor(2.7652e-05)\n",
            "18787 Traning Loss: tensor(3.0621e-05)\n",
            "18788 Traning Loss: tensor(2.5761e-05)\n",
            "18789 Traning Loss: tensor(2.5241e-05)\n",
            "18790 Traning Loss: tensor(2.3338e-05)\n",
            "18791 Traning Loss: tensor(2.6304e-05)\n",
            "18792 Traning Loss: tensor(2.8281e-05)\n",
            "18793 Traning Loss: tensor(2.6307e-05)\n",
            "18794 Traning Loss: tensor(2.7230e-05)\n",
            "18795 Traning Loss: tensor(2.5330e-05)\n",
            "18796 Traning Loss: tensor(2.4868e-05)\n",
            "18797 Traning Loss: tensor(2.6957e-05)\n",
            "18798 Traning Loss: tensor(2.5876e-05)\n",
            "18799 Traning Loss: tensor(2.5425e-05)\n",
            "18800 Traning Loss: tensor(2.5701e-05)\n",
            "18801 Traning Loss: tensor(2.7099e-05)\n",
            "18802 Traning Loss: tensor(2.6469e-05)\n",
            "18803 Traning Loss: tensor(2.3948e-05)\n",
            "18804 Traning Loss: tensor(2.6714e-05)\n",
            "18805 Traning Loss: tensor(3.5161e-05)\n",
            "18806 Traning Loss: tensor(3.1021e-05)\n",
            "18807 Traning Loss: tensor(2.8974e-05)\n",
            "18808 Traning Loss: tensor(2.9035e-05)\n",
            "18809 Traning Loss: tensor(2.4896e-05)\n",
            "18810 Traning Loss: tensor(3.0445e-05)\n",
            "18811 Traning Loss: tensor(2.5680e-05)\n",
            "18812 Traning Loss: tensor(2.4982e-05)\n",
            "18813 Traning Loss: tensor(2.7259e-05)\n",
            "18814 Traning Loss: tensor(2.2918e-05)\n",
            "18815 Traning Loss: tensor(2.5580e-05)\n",
            "18816 Traning Loss: tensor(2.7498e-05)\n",
            "18817 Traning Loss: tensor(2.8908e-05)\n",
            "18818 Traning Loss: tensor(2.7418e-05)\n",
            "18819 Traning Loss: tensor(2.5626e-05)\n",
            "18820 Traning Loss: tensor(2.6294e-05)\n",
            "18821 Traning Loss: tensor(2.7448e-05)\n",
            "18822 Traning Loss: tensor(2.8234e-05)\n",
            "18823 Traning Loss: tensor(2.8657e-05)\n",
            "18824 Traning Loss: tensor(2.5450e-05)\n",
            "18825 Traning Loss: tensor(3.4511e-05)\n",
            "18826 Traning Loss: tensor(2.8768e-05)\n",
            "18827 Traning Loss: tensor(2.9945e-05)\n",
            "18828 Traning Loss: tensor(3.1688e-05)\n",
            "18829 Traning Loss: tensor(2.5102e-05)\n",
            "18830 Traning Loss: tensor(2.5860e-05)\n",
            "18831 Traning Loss: tensor(2.8300e-05)\n",
            "18832 Traning Loss: tensor(2.5933e-05)\n",
            "18833 Traning Loss: tensor(2.6942e-05)\n",
            "18834 Traning Loss: tensor(2.4549e-05)\n",
            "18835 Traning Loss: tensor(2.3681e-05)\n",
            "18836 Traning Loss: tensor(2.5367e-05)\n",
            "18837 Traning Loss: tensor(2.6238e-05)\n",
            "18838 Traning Loss: tensor(2.8459e-05)\n",
            "18839 Traning Loss: tensor(2.4010e-05)\n",
            "18840 Traning Loss: tensor(2.5918e-05)\n",
            "18841 Traning Loss: tensor(2.6195e-05)\n",
            "18842 Traning Loss: tensor(2.5175e-05)\n",
            "18843 Traning Loss: tensor(2.5761e-05)\n",
            "18844 Traning Loss: tensor(2.4819e-05)\n",
            "18845 Traning Loss: tensor(2.4631e-05)\n",
            "18846 Traning Loss: tensor(2.3651e-05)\n",
            "18847 Traning Loss: tensor(2.4646e-05)\n",
            "18848 Traning Loss: tensor(2.4459e-05)\n",
            "18849 Traning Loss: tensor(2.4856e-05)\n",
            "18850 Traning Loss: tensor(2.6195e-05)\n",
            "18851 Traning Loss: tensor(2.4770e-05)\n",
            "18852 Traning Loss: tensor(2.4463e-05)\n",
            "18853 Traning Loss: tensor(2.5771e-05)\n",
            "18854 Traning Loss: tensor(2.4065e-05)\n",
            "18855 Traning Loss: tensor(2.6019e-05)\n",
            "18856 Traning Loss: tensor(2.5284e-05)\n",
            "18857 Traning Loss: tensor(2.9427e-05)\n",
            "18858 Traning Loss: tensor(2.5363e-05)\n",
            "18859 Traning Loss: tensor(2.6311e-05)\n",
            "18860 Traning Loss: tensor(3.4959e-05)\n",
            "18861 Traning Loss: tensor(3.8281e-05)\n",
            "18862 Traning Loss: tensor(2.8404e-05)\n",
            "18863 Traning Loss: tensor(2.6405e-05)\n",
            "18864 Traning Loss: tensor(2.7413e-05)\n",
            "18865 Traning Loss: tensor(2.4154e-05)\n",
            "18866 Traning Loss: tensor(2.5925e-05)\n",
            "18867 Traning Loss: tensor(2.6421e-05)\n",
            "18868 Traning Loss: tensor(2.5986e-05)\n",
            "18869 Traning Loss: tensor(2.7863e-05)\n",
            "18870 Traning Loss: tensor(2.8352e-05)\n",
            "18871 Traning Loss: tensor(2.4307e-05)\n",
            "18872 Traning Loss: tensor(2.8442e-05)\n",
            "18873 Traning Loss: tensor(2.6751e-05)\n",
            "18874 Traning Loss: tensor(2.7765e-05)\n",
            "18875 Traning Loss: tensor(2.5522e-05)\n",
            "18876 Traning Loss: tensor(2.7051e-05)\n",
            "18877 Traning Loss: tensor(2.9790e-05)\n",
            "18878 Traning Loss: tensor(2.3941e-05)\n",
            "18879 Traning Loss: tensor(2.8235e-05)\n",
            "18880 Traning Loss: tensor(2.5726e-05)\n",
            "18881 Traning Loss: tensor(2.5073e-05)\n",
            "18882 Traning Loss: tensor(2.6006e-05)\n",
            "18883 Traning Loss: tensor(2.4301e-05)\n",
            "18884 Traning Loss: tensor(2.5759e-05)\n",
            "18885 Traning Loss: tensor(2.5322e-05)\n",
            "18886 Traning Loss: tensor(2.4888e-05)\n",
            "18887 Traning Loss: tensor(4.3258e-05)\n",
            "18888 Traning Loss: tensor(2.6479e-05)\n",
            "18889 Traning Loss: tensor(2.5778e-05)\n",
            "18890 Traning Loss: tensor(2.7668e-05)\n",
            "18891 Traning Loss: tensor(2.5842e-05)\n",
            "18892 Traning Loss: tensor(2.4977e-05)\n",
            "18893 Traning Loss: tensor(2.6074e-05)\n",
            "18894 Traning Loss: tensor(2.7665e-05)\n",
            "18895 Traning Loss: tensor(2.7097e-05)\n",
            "18896 Traning Loss: tensor(2.3464e-05)\n",
            "18897 Traning Loss: tensor(2.6397e-05)\n",
            "18898 Traning Loss: tensor(2.4229e-05)\n",
            "18899 Traning Loss: tensor(2.5657e-05)\n",
            "18900 Traning Loss: tensor(2.4035e-05)\n",
            "18901 Traning Loss: tensor(2.4967e-05)\n",
            "18902 Traning Loss: tensor(2.4895e-05)\n",
            "18903 Traning Loss: tensor(2.5917e-05)\n",
            "18904 Traning Loss: tensor(2.6369e-05)\n",
            "18905 Traning Loss: tensor(2.4979e-05)\n",
            "18906 Traning Loss: tensor(2.3747e-05)\n",
            "18907 Traning Loss: tensor(2.3931e-05)\n",
            "18908 Traning Loss: tensor(2.5377e-05)\n",
            "18909 Traning Loss: tensor(2.4787e-05)\n",
            "18910 Traning Loss: tensor(2.3925e-05)\n",
            "18911 Traning Loss: tensor(2.3998e-05)\n",
            "18912 Traning Loss: tensor(2.6190e-05)\n",
            "18913 Traning Loss: tensor(2.4768e-05)\n",
            "18914 Traning Loss: tensor(2.4985e-05)\n",
            "18915 Traning Loss: tensor(2.4894e-05)\n",
            "18916 Traning Loss: tensor(2.6785e-05)\n",
            "18917 Traning Loss: tensor(2.4985e-05)\n",
            "18918 Traning Loss: tensor(2.5637e-05)\n",
            "18919 Traning Loss: tensor(2.4105e-05)\n",
            "18920 Traning Loss: tensor(2.5831e-05)\n",
            "18921 Traning Loss: tensor(2.4920e-05)\n",
            "18922 Traning Loss: tensor(3.2875e-05)\n",
            "18923 Traning Loss: tensor(2.5773e-05)\n",
            "18924 Traning Loss: tensor(2.6743e-05)\n",
            "18925 Traning Loss: tensor(2.5162e-05)\n",
            "18926 Traning Loss: tensor(2.5454e-05)\n",
            "18927 Traning Loss: tensor(2.5278e-05)\n",
            "18928 Traning Loss: tensor(2.4958e-05)\n",
            "18929 Traning Loss: tensor(2.3213e-05)\n",
            "18930 Traning Loss: tensor(2.4204e-05)\n",
            "18931 Traning Loss: tensor(2.4401e-05)\n",
            "18932 Traning Loss: tensor(2.5445e-05)\n",
            "18933 Traning Loss: tensor(2.3911e-05)\n",
            "18934 Traning Loss: tensor(2.5252e-05)\n",
            "18935 Traning Loss: tensor(2.5118e-05)\n",
            "18936 Traning Loss: tensor(2.5467e-05)\n",
            "18937 Traning Loss: tensor(2.4642e-05)\n",
            "18938 Traning Loss: tensor(2.4114e-05)\n",
            "18939 Traning Loss: tensor(2.6255e-05)\n",
            "18940 Traning Loss: tensor(2.5432e-05)\n",
            "18941 Traning Loss: tensor(2.7693e-05)\n",
            "18942 Traning Loss: tensor(2.5272e-05)\n",
            "18943 Traning Loss: tensor(2.4547e-05)\n",
            "18944 Traning Loss: tensor(2.4740e-05)\n",
            "18945 Traning Loss: tensor(2.6101e-05)\n",
            "18946 Traning Loss: tensor(2.6969e-05)\n",
            "18947 Traning Loss: tensor(2.4259e-05)\n",
            "18948 Traning Loss: tensor(2.4917e-05)\n",
            "18949 Traning Loss: tensor(2.6932e-05)\n",
            "18950 Traning Loss: tensor(2.5763e-05)\n",
            "18951 Traning Loss: tensor(2.4799e-05)\n",
            "18952 Traning Loss: tensor(2.4209e-05)\n",
            "18953 Traning Loss: tensor(2.3170e-05)\n",
            "18954 Traning Loss: tensor(2.4932e-05)\n",
            "18955 Traning Loss: tensor(2.4884e-05)\n",
            "18956 Traning Loss: tensor(2.4708e-05)\n",
            "18957 Traning Loss: tensor(2.2483e-05)\n",
            "18958 Traning Loss: tensor(2.3722e-05)\n",
            "18959 Traning Loss: tensor(2.5066e-05)\n",
            "18960 Traning Loss: tensor(2.3473e-05)\n",
            "18961 Traning Loss: tensor(2.3975e-05)\n",
            "18962 Traning Loss: tensor(2.5074e-05)\n",
            "18963 Traning Loss: tensor(2.6093e-05)\n",
            "18964 Traning Loss: tensor(2.5268e-05)\n",
            "18965 Traning Loss: tensor(2.3996e-05)\n",
            "18966 Traning Loss: tensor(2.4910e-05)\n",
            "18967 Traning Loss: tensor(2.5681e-05)\n",
            "18968 Traning Loss: tensor(2.4011e-05)\n",
            "18969 Traning Loss: tensor(2.5859e-05)\n",
            "18970 Traning Loss: tensor(2.6037e-05)\n",
            "18971 Traning Loss: tensor(2.3131e-05)\n",
            "18972 Traning Loss: tensor(2.3812e-05)\n",
            "18973 Traning Loss: tensor(2.2913e-05)\n",
            "18974 Traning Loss: tensor(2.5497e-05)\n",
            "18975 Traning Loss: tensor(2.5499e-05)\n",
            "18976 Traning Loss: tensor(2.5190e-05)\n",
            "18977 Traning Loss: tensor(2.4370e-05)\n",
            "18978 Traning Loss: tensor(2.5188e-05)\n",
            "18979 Traning Loss: tensor(2.3819e-05)\n",
            "18980 Traning Loss: tensor(2.3630e-05)\n",
            "18981 Traning Loss: tensor(2.4389e-05)\n",
            "18982 Traning Loss: tensor(2.2583e-05)\n",
            "18983 Traning Loss: tensor(2.3065e-05)\n",
            "18984 Traning Loss: tensor(2.4551e-05)\n",
            "18985 Traning Loss: tensor(2.5529e-05)\n",
            "18986 Traning Loss: tensor(2.5869e-05)\n",
            "18987 Traning Loss: tensor(2.4926e-05)\n",
            "18988 Traning Loss: tensor(2.4179e-05)\n",
            "18989 Traning Loss: tensor(2.2671e-05)\n",
            "18990 Traning Loss: tensor(2.4562e-05)\n",
            "18991 Traning Loss: tensor(2.4208e-05)\n",
            "18992 Traning Loss: tensor(2.5996e-05)\n",
            "18993 Traning Loss: tensor(2.3795e-05)\n",
            "18994 Traning Loss: tensor(2.5159e-05)\n",
            "18995 Traning Loss: tensor(2.4684e-05)\n",
            "18996 Traning Loss: tensor(2.5585e-05)\n",
            "18997 Traning Loss: tensor(2.3043e-05)\n",
            "18998 Traning Loss: tensor(2.6790e-05)\n",
            "18999 Traning Loss: tensor(2.4550e-05)\n",
            "19000 Traning Loss: tensor(2.8047e-05)\n",
            "19001 Traning Loss: tensor(2.8729e-05)\n",
            "19002 Traning Loss: tensor(2.4624e-05)\n",
            "19003 Traning Loss: tensor(2.3213e-05)\n",
            "19004 Traning Loss: tensor(2.5867e-05)\n",
            "19005 Traning Loss: tensor(2.8685e-05)\n",
            "19006 Traning Loss: tensor(3.0035e-05)\n",
            "19007 Traning Loss: tensor(3.1931e-05)\n",
            "19008 Traning Loss: tensor(2.4819e-05)\n",
            "19009 Traning Loss: tensor(3.1297e-05)\n",
            "19010 Traning Loss: tensor(3.9630e-05)\n",
            "19011 Traning Loss: tensor(3.2940e-05)\n",
            "19012 Traning Loss: tensor(2.6875e-05)\n",
            "19013 Traning Loss: tensor(2.3819e-05)\n",
            "19014 Traning Loss: tensor(2.8279e-05)\n",
            "19015 Traning Loss: tensor(3.0466e-05)\n",
            "19016 Traning Loss: tensor(2.9678e-05)\n",
            "19017 Traning Loss: tensor(2.3333e-05)\n",
            "19018 Traning Loss: tensor(2.3736e-05)\n",
            "19019 Traning Loss: tensor(2.8114e-05)\n",
            "19020 Traning Loss: tensor(2.7129e-05)\n",
            "19021 Traning Loss: tensor(2.3791e-05)\n",
            "19022 Traning Loss: tensor(2.5883e-05)\n",
            "19023 Traning Loss: tensor(2.3948e-05)\n",
            "19024 Traning Loss: tensor(2.4981e-05)\n",
            "19025 Traning Loss: tensor(2.6103e-05)\n",
            "19026 Traning Loss: tensor(2.6765e-05)\n",
            "19027 Traning Loss: tensor(2.4490e-05)\n",
            "19028 Traning Loss: tensor(2.7702e-05)\n",
            "19029 Traning Loss: tensor(2.5687e-05)\n",
            "19030 Traning Loss: tensor(2.4598e-05)\n",
            "19031 Traning Loss: tensor(2.3319e-05)\n",
            "19032 Traning Loss: tensor(2.2786e-05)\n",
            "19033 Traning Loss: tensor(2.3502e-05)\n",
            "19034 Traning Loss: tensor(2.5525e-05)\n",
            "19035 Traning Loss: tensor(2.5315e-05)\n",
            "19036 Traning Loss: tensor(2.4301e-05)\n",
            "19037 Traning Loss: tensor(2.4047e-05)\n",
            "19038 Traning Loss: tensor(2.3383e-05)\n",
            "19039 Traning Loss: tensor(2.4850e-05)\n",
            "19040 Traning Loss: tensor(2.3797e-05)\n",
            "19041 Traning Loss: tensor(2.2441e-05)\n",
            "19042 Traning Loss: tensor(2.6509e-05)\n",
            "19043 Traning Loss: tensor(2.5537e-05)\n",
            "19044 Traning Loss: tensor(2.4698e-05)\n",
            "19045 Traning Loss: tensor(2.4050e-05)\n",
            "19046 Traning Loss: tensor(2.3561e-05)\n",
            "19047 Traning Loss: tensor(2.2811e-05)\n",
            "19048 Traning Loss: tensor(2.4814e-05)\n",
            "19049 Traning Loss: tensor(3.1572e-05)\n",
            "19050 Traning Loss: tensor(2.3973e-05)\n",
            "19051 Traning Loss: tensor(2.4905e-05)\n",
            "19052 Traning Loss: tensor(2.3516e-05)\n",
            "19053 Traning Loss: tensor(2.5260e-05)\n",
            "19054 Traning Loss: tensor(2.4455e-05)\n",
            "19055 Traning Loss: tensor(2.5714e-05)\n",
            "19056 Traning Loss: tensor(2.4019e-05)\n",
            "19057 Traning Loss: tensor(2.5201e-05)\n",
            "19058 Traning Loss: tensor(2.4200e-05)\n",
            "19059 Traning Loss: tensor(2.4791e-05)\n",
            "19060 Traning Loss: tensor(2.4879e-05)\n",
            "19061 Traning Loss: tensor(2.4545e-05)\n",
            "19062 Traning Loss: tensor(2.5915e-05)\n",
            "19063 Traning Loss: tensor(2.4434e-05)\n",
            "19064 Traning Loss: tensor(2.2384e-05)\n",
            "19065 Traning Loss: tensor(2.0851e-05)\n",
            "19066 Traning Loss: tensor(2.3313e-05)\n",
            "19067 Traning Loss: tensor(2.3791e-05)\n",
            "19068 Traning Loss: tensor(2.4059e-05)\n",
            "19069 Traning Loss: tensor(2.4955e-05)\n",
            "19070 Traning Loss: tensor(2.6599e-05)\n",
            "19071 Traning Loss: tensor(2.3390e-05)\n",
            "19072 Traning Loss: tensor(2.3435e-05)\n",
            "19073 Traning Loss: tensor(2.3599e-05)\n",
            "19074 Traning Loss: tensor(2.4605e-05)\n",
            "19075 Traning Loss: tensor(2.6951e-05)\n",
            "19076 Traning Loss: tensor(2.4299e-05)\n",
            "19077 Traning Loss: tensor(2.3413e-05)\n",
            "19078 Traning Loss: tensor(2.4888e-05)\n",
            "19079 Traning Loss: tensor(2.3073e-05)\n",
            "19080 Traning Loss: tensor(2.3558e-05)\n",
            "19081 Traning Loss: tensor(2.3509e-05)\n",
            "19082 Traning Loss: tensor(2.2739e-05)\n",
            "19083 Traning Loss: tensor(2.2037e-05)\n",
            "19084 Traning Loss: tensor(2.6689e-05)\n",
            "19085 Traning Loss: tensor(2.5380e-05)\n",
            "19086 Traning Loss: tensor(2.5524e-05)\n",
            "19087 Traning Loss: tensor(2.3602e-05)\n",
            "19088 Traning Loss: tensor(2.3447e-05)\n",
            "19089 Traning Loss: tensor(2.4474e-05)\n",
            "19090 Traning Loss: tensor(2.4511e-05)\n",
            "19091 Traning Loss: tensor(2.5483e-05)\n",
            "19092 Traning Loss: tensor(2.3596e-05)\n",
            "19093 Traning Loss: tensor(3.2521e-05)\n",
            "19094 Traning Loss: tensor(3.0338e-05)\n",
            "19095 Traning Loss: tensor(2.5494e-05)\n",
            "19096 Traning Loss: tensor(2.6183e-05)\n",
            "19097 Traning Loss: tensor(2.5933e-05)\n",
            "19098 Traning Loss: tensor(2.4961e-05)\n",
            "19099 Traning Loss: tensor(2.5546e-05)\n",
            "19100 Traning Loss: tensor(2.4957e-05)\n",
            "19101 Traning Loss: tensor(2.3090e-05)\n",
            "19102 Traning Loss: tensor(2.4758e-05)\n",
            "19103 Traning Loss: tensor(2.5106e-05)\n",
            "19104 Traning Loss: tensor(2.3093e-05)\n",
            "19105 Traning Loss: tensor(2.6562e-05)\n",
            "19106 Traning Loss: tensor(2.9467e-05)\n",
            "19107 Traning Loss: tensor(2.7562e-05)\n",
            "19108 Traning Loss: tensor(2.9997e-05)\n",
            "19109 Traning Loss: tensor(2.7123e-05)\n",
            "19110 Traning Loss: tensor(2.6395e-05)\n",
            "19111 Traning Loss: tensor(2.3660e-05)\n",
            "19112 Traning Loss: tensor(2.2788e-05)\n",
            "19113 Traning Loss: tensor(2.5777e-05)\n",
            "19114 Traning Loss: tensor(2.4403e-05)\n",
            "19115 Traning Loss: tensor(2.5717e-05)\n",
            "19116 Traning Loss: tensor(2.4582e-05)\n",
            "19117 Traning Loss: tensor(2.5639e-05)\n",
            "19118 Traning Loss: tensor(2.4426e-05)\n",
            "19119 Traning Loss: tensor(2.5112e-05)\n",
            "19120 Traning Loss: tensor(2.5628e-05)\n",
            "19121 Traning Loss: tensor(2.3443e-05)\n",
            "19122 Traning Loss: tensor(2.3497e-05)\n",
            "19123 Traning Loss: tensor(2.2625e-05)\n",
            "19124 Traning Loss: tensor(2.3872e-05)\n",
            "19125 Traning Loss: tensor(2.2563e-05)\n",
            "19126 Traning Loss: tensor(2.3737e-05)\n",
            "19127 Traning Loss: tensor(2.3681e-05)\n",
            "19128 Traning Loss: tensor(2.3048e-05)\n",
            "19129 Traning Loss: tensor(2.4896e-05)\n",
            "19130 Traning Loss: tensor(2.4337e-05)\n",
            "19131 Traning Loss: tensor(2.6045e-05)\n",
            "19132 Traning Loss: tensor(2.4989e-05)\n",
            "19133 Traning Loss: tensor(2.6020e-05)\n",
            "19134 Traning Loss: tensor(2.2762e-05)\n",
            "19135 Traning Loss: tensor(2.4873e-05)\n",
            "19136 Traning Loss: tensor(2.5100e-05)\n",
            "19137 Traning Loss: tensor(2.4896e-05)\n",
            "19138 Traning Loss: tensor(2.2825e-05)\n",
            "19139 Traning Loss: tensor(2.4367e-05)\n",
            "19140 Traning Loss: tensor(2.3338e-05)\n",
            "19141 Traning Loss: tensor(2.4254e-05)\n",
            "19142 Traning Loss: tensor(2.3055e-05)\n",
            "19143 Traning Loss: tensor(2.4908e-05)\n",
            "19144 Traning Loss: tensor(2.4902e-05)\n",
            "19145 Traning Loss: tensor(2.2514e-05)\n",
            "19146 Traning Loss: tensor(2.3951e-05)\n",
            "19147 Traning Loss: tensor(2.3040e-05)\n",
            "19148 Traning Loss: tensor(2.4369e-05)\n",
            "19149 Traning Loss: tensor(2.1276e-05)\n",
            "19150 Traning Loss: tensor(2.3970e-05)\n",
            "19151 Traning Loss: tensor(2.3954e-05)\n",
            "19152 Traning Loss: tensor(2.4363e-05)\n",
            "19153 Traning Loss: tensor(2.3921e-05)\n",
            "19154 Traning Loss: tensor(2.2487e-05)\n",
            "19155 Traning Loss: tensor(2.2117e-05)\n",
            "19156 Traning Loss: tensor(2.3691e-05)\n",
            "19157 Traning Loss: tensor(2.2785e-05)\n",
            "19158 Traning Loss: tensor(3.5416e-05)\n",
            "19159 Traning Loss: tensor(2.6927e-05)\n",
            "19160 Traning Loss: tensor(2.4313e-05)\n",
            "19161 Traning Loss: tensor(2.6163e-05)\n",
            "19162 Traning Loss: tensor(2.5434e-05)\n",
            "19163 Traning Loss: tensor(2.5013e-05)\n",
            "19164 Traning Loss: tensor(2.4770e-05)\n",
            "19165 Traning Loss: tensor(2.4770e-05)\n",
            "19166 Traning Loss: tensor(2.2938e-05)\n",
            "19167 Traning Loss: tensor(2.3146e-05)\n",
            "19168 Traning Loss: tensor(2.4892e-05)\n",
            "19169 Traning Loss: tensor(2.5629e-05)\n",
            "19170 Traning Loss: tensor(2.4946e-05)\n",
            "19171 Traning Loss: tensor(2.4124e-05)\n",
            "19172 Traning Loss: tensor(2.5715e-05)\n",
            "19173 Traning Loss: tensor(2.3663e-05)\n",
            "19174 Traning Loss: tensor(2.3570e-05)\n",
            "19175 Traning Loss: tensor(2.3627e-05)\n",
            "19176 Traning Loss: tensor(2.5084e-05)\n",
            "19177 Traning Loss: tensor(2.3608e-05)\n",
            "19178 Traning Loss: tensor(2.2881e-05)\n",
            "19179 Traning Loss: tensor(2.3181e-05)\n",
            "19180 Traning Loss: tensor(2.4880e-05)\n",
            "19181 Traning Loss: tensor(2.3964e-05)\n",
            "19182 Traning Loss: tensor(2.3766e-05)\n",
            "19183 Traning Loss: tensor(2.3711e-05)\n",
            "19184 Traning Loss: tensor(2.3220e-05)\n",
            "19185 Traning Loss: tensor(2.3236e-05)\n",
            "19186 Traning Loss: tensor(2.3746e-05)\n",
            "19187 Traning Loss: tensor(2.2344e-05)\n",
            "19188 Traning Loss: tensor(2.4241e-05)\n",
            "19189 Traning Loss: tensor(2.4494e-05)\n",
            "19190 Traning Loss: tensor(2.3974e-05)\n",
            "19191 Traning Loss: tensor(2.3593e-05)\n",
            "19192 Traning Loss: tensor(2.3851e-05)\n",
            "19193 Traning Loss: tensor(2.4935e-05)\n",
            "19194 Traning Loss: tensor(2.2307e-05)\n",
            "19195 Traning Loss: tensor(2.4466e-05)\n",
            "19196 Traning Loss: tensor(2.1505e-05)\n",
            "19197 Traning Loss: tensor(2.2903e-05)\n",
            "19198 Traning Loss: tensor(2.5346e-05)\n",
            "19199 Traning Loss: tensor(2.3136e-05)\n",
            "19200 Traning Loss: tensor(2.4256e-05)\n",
            "19201 Traning Loss: tensor(2.2987e-05)\n",
            "19202 Traning Loss: tensor(2.3665e-05)\n",
            "19203 Traning Loss: tensor(2.3503e-05)\n",
            "19204 Traning Loss: tensor(2.4351e-05)\n",
            "19205 Traning Loss: tensor(2.4478e-05)\n",
            "19206 Traning Loss: tensor(2.4728e-05)\n",
            "19207 Traning Loss: tensor(2.6143e-05)\n",
            "19208 Traning Loss: tensor(2.2955e-05)\n",
            "19209 Traning Loss: tensor(2.3078e-05)\n",
            "19210 Traning Loss: tensor(2.3783e-05)\n",
            "19211 Traning Loss: tensor(2.2026e-05)\n",
            "19212 Traning Loss: tensor(2.3185e-05)\n",
            "19213 Traning Loss: tensor(2.2229e-05)\n",
            "19214 Traning Loss: tensor(2.7708e-05)\n",
            "19215 Traning Loss: tensor(2.2191e-05)\n",
            "19216 Traning Loss: tensor(2.4752e-05)\n",
            "19217 Traning Loss: tensor(2.4231e-05)\n",
            "19218 Traning Loss: tensor(2.4972e-05)\n",
            "19219 Traning Loss: tensor(2.4697e-05)\n",
            "19220 Traning Loss: tensor(2.5782e-05)\n",
            "19221 Traning Loss: tensor(2.6429e-05)\n",
            "19222 Traning Loss: tensor(2.2288e-05)\n",
            "19223 Traning Loss: tensor(2.1906e-05)\n",
            "19224 Traning Loss: tensor(2.3573e-05)\n",
            "19225 Traning Loss: tensor(2.1760e-05)\n",
            "19226 Traning Loss: tensor(2.3969e-05)\n",
            "19227 Traning Loss: tensor(2.4562e-05)\n",
            "19228 Traning Loss: tensor(2.2709e-05)\n",
            "19229 Traning Loss: tensor(2.3016e-05)\n",
            "19230 Traning Loss: tensor(2.3353e-05)\n",
            "19231 Traning Loss: tensor(2.3429e-05)\n",
            "19232 Traning Loss: tensor(2.3094e-05)\n",
            "19233 Traning Loss: tensor(2.3534e-05)\n",
            "19234 Traning Loss: tensor(2.4227e-05)\n",
            "19235 Traning Loss: tensor(2.4227e-05)\n",
            "19236 Traning Loss: tensor(2.4213e-05)\n",
            "19237 Traning Loss: tensor(2.2568e-05)\n",
            "19238 Traning Loss: tensor(2.4648e-05)\n",
            "19239 Traning Loss: tensor(2.3191e-05)\n",
            "19240 Traning Loss: tensor(2.2781e-05)\n",
            "19241 Traning Loss: tensor(2.3310e-05)\n",
            "19242 Traning Loss: tensor(2.3904e-05)\n",
            "19243 Traning Loss: tensor(2.2262e-05)\n",
            "19244 Traning Loss: tensor(2.5557e-05)\n",
            "19245 Traning Loss: tensor(2.4163e-05)\n",
            "19246 Traning Loss: tensor(2.4505e-05)\n",
            "19247 Traning Loss: tensor(2.4160e-05)\n",
            "19248 Traning Loss: tensor(2.3918e-05)\n",
            "19249 Traning Loss: tensor(2.3634e-05)\n",
            "19250 Traning Loss: tensor(2.4897e-05)\n",
            "19251 Traning Loss: tensor(2.5762e-05)\n",
            "19252 Traning Loss: tensor(2.5266e-05)\n",
            "19253 Traning Loss: tensor(2.5668e-05)\n",
            "19254 Traning Loss: tensor(2.3045e-05)\n",
            "19255 Traning Loss: tensor(2.3722e-05)\n",
            "19256 Traning Loss: tensor(2.3246e-05)\n",
            "19257 Traning Loss: tensor(2.2888e-05)\n",
            "19258 Traning Loss: tensor(2.3845e-05)\n",
            "19259 Traning Loss: tensor(2.2167e-05)\n",
            "19260 Traning Loss: tensor(2.2470e-05)\n",
            "19261 Traning Loss: tensor(2.2352e-05)\n",
            "19262 Traning Loss: tensor(2.3363e-05)\n",
            "19263 Traning Loss: tensor(2.2024e-05)\n",
            "19264 Traning Loss: tensor(2.3128e-05)\n",
            "19265 Traning Loss: tensor(2.7113e-05)\n",
            "19266 Traning Loss: tensor(2.7604e-05)\n",
            "19267 Traning Loss: tensor(2.7484e-05)\n",
            "19268 Traning Loss: tensor(2.6123e-05)\n",
            "19269 Traning Loss: tensor(2.3861e-05)\n",
            "19270 Traning Loss: tensor(2.2959e-05)\n",
            "19271 Traning Loss: tensor(2.3059e-05)\n",
            "19272 Traning Loss: tensor(2.3630e-05)\n",
            "19273 Traning Loss: tensor(2.3143e-05)\n",
            "19274 Traning Loss: tensor(2.2256e-05)\n",
            "19275 Traning Loss: tensor(2.2562e-05)\n",
            "19276 Traning Loss: tensor(2.4554e-05)\n",
            "19277 Traning Loss: tensor(2.5124e-05)\n",
            "19278 Traning Loss: tensor(2.4311e-05)\n",
            "19279 Traning Loss: tensor(2.2554e-05)\n",
            "19280 Traning Loss: tensor(2.2620e-05)\n",
            "19281 Traning Loss: tensor(2.5846e-05)\n",
            "19282 Traning Loss: tensor(2.4825e-05)\n",
            "19283 Traning Loss: tensor(2.5249e-05)\n",
            "19284 Traning Loss: tensor(2.5185e-05)\n",
            "19285 Traning Loss: tensor(2.4346e-05)\n",
            "19286 Traning Loss: tensor(2.4440e-05)\n",
            "19287 Traning Loss: tensor(2.3172e-05)\n",
            "19288 Traning Loss: tensor(2.5686e-05)\n",
            "19289 Traning Loss: tensor(2.5334e-05)\n",
            "19290 Traning Loss: tensor(2.4374e-05)\n",
            "19291 Traning Loss: tensor(2.3123e-05)\n",
            "19292 Traning Loss: tensor(2.5645e-05)\n",
            "19293 Traning Loss: tensor(2.4504e-05)\n",
            "19294 Traning Loss: tensor(2.5260e-05)\n",
            "19295 Traning Loss: tensor(2.4092e-05)\n",
            "19296 Traning Loss: tensor(2.3725e-05)\n",
            "19297 Traning Loss: tensor(2.3142e-05)\n",
            "19298 Traning Loss: tensor(2.3986e-05)\n",
            "19299 Traning Loss: tensor(2.4083e-05)\n",
            "19300 Traning Loss: tensor(2.3457e-05)\n",
            "19301 Traning Loss: tensor(2.2546e-05)\n",
            "19302 Traning Loss: tensor(2.2946e-05)\n",
            "19303 Traning Loss: tensor(2.2263e-05)\n",
            "19304 Traning Loss: tensor(2.2924e-05)\n",
            "19305 Traning Loss: tensor(2.3440e-05)\n",
            "19306 Traning Loss: tensor(2.3226e-05)\n",
            "19307 Traning Loss: tensor(2.1241e-05)\n",
            "19308 Traning Loss: tensor(2.2917e-05)\n",
            "19309 Traning Loss: tensor(2.2866e-05)\n",
            "19310 Traning Loss: tensor(2.3756e-05)\n",
            "19311 Traning Loss: tensor(2.3302e-05)\n",
            "19312 Traning Loss: tensor(2.3099e-05)\n",
            "19313 Traning Loss: tensor(2.2305e-05)\n",
            "19314 Traning Loss: tensor(2.3543e-05)\n",
            "19315 Traning Loss: tensor(2.5062e-05)\n",
            "19316 Traning Loss: tensor(2.4328e-05)\n",
            "19317 Traning Loss: tensor(2.3075e-05)\n",
            "19318 Traning Loss: tensor(2.5877e-05)\n",
            "19319 Traning Loss: tensor(2.3748e-05)\n",
            "19320 Traning Loss: tensor(2.5607e-05)\n",
            "19321 Traning Loss: tensor(2.6486e-05)\n",
            "19322 Traning Loss: tensor(2.4668e-05)\n",
            "19323 Traning Loss: tensor(2.3858e-05)\n",
            "19324 Traning Loss: tensor(2.3506e-05)\n",
            "19325 Traning Loss: tensor(2.5375e-05)\n",
            "19326 Traning Loss: tensor(2.5108e-05)\n",
            "19327 Traning Loss: tensor(2.5091e-05)\n",
            "19328 Traning Loss: tensor(2.2818e-05)\n",
            "19329 Traning Loss: tensor(2.3101e-05)\n",
            "19330 Traning Loss: tensor(2.3532e-05)\n",
            "19331 Traning Loss: tensor(2.5399e-05)\n",
            "19332 Traning Loss: tensor(2.2588e-05)\n",
            "19333 Traning Loss: tensor(2.3308e-05)\n",
            "19334 Traning Loss: tensor(2.2133e-05)\n",
            "19335 Traning Loss: tensor(2.0639e-05)\n",
            "19336 Traning Loss: tensor(2.3729e-05)\n",
            "19337 Traning Loss: tensor(2.2784e-05)\n",
            "19338 Traning Loss: tensor(2.2297e-05)\n",
            "19339 Traning Loss: tensor(2.3993e-05)\n",
            "19340 Traning Loss: tensor(2.2323e-05)\n",
            "19341 Traning Loss: tensor(2.2121e-05)\n",
            "19342 Traning Loss: tensor(2.1866e-05)\n",
            "19343 Traning Loss: tensor(2.3469e-05)\n",
            "19344 Traning Loss: tensor(2.3398e-05)\n",
            "19345 Traning Loss: tensor(2.2920e-05)\n",
            "19346 Traning Loss: tensor(2.2501e-05)\n",
            "19347 Traning Loss: tensor(2.2838e-05)\n",
            "19348 Traning Loss: tensor(2.2118e-05)\n",
            "19349 Traning Loss: tensor(2.3057e-05)\n",
            "19350 Traning Loss: tensor(2.2945e-05)\n",
            "19351 Traning Loss: tensor(2.1790e-05)\n",
            "19352 Traning Loss: tensor(2.3218e-05)\n",
            "19353 Traning Loss: tensor(3.1615e-05)\n",
            "19354 Traning Loss: tensor(2.4456e-05)\n",
            "19355 Traning Loss: tensor(2.3576e-05)\n",
            "19356 Traning Loss: tensor(2.3928e-05)\n",
            "19357 Traning Loss: tensor(2.2733e-05)\n",
            "19358 Traning Loss: tensor(2.4963e-05)\n",
            "19359 Traning Loss: tensor(2.2540e-05)\n",
            "19360 Traning Loss: tensor(2.2742e-05)\n",
            "19361 Traning Loss: tensor(2.2577e-05)\n",
            "19362 Traning Loss: tensor(2.4390e-05)\n",
            "19363 Traning Loss: tensor(2.3333e-05)\n",
            "19364 Traning Loss: tensor(2.2377e-05)\n",
            "19365 Traning Loss: tensor(2.2153e-05)\n",
            "19366 Traning Loss: tensor(2.3131e-05)\n",
            "19367 Traning Loss: tensor(2.3563e-05)\n",
            "19368 Traning Loss: tensor(2.3231e-05)\n",
            "19369 Traning Loss: tensor(2.2514e-05)\n",
            "19370 Traning Loss: tensor(2.1573e-05)\n",
            "19371 Traning Loss: tensor(2.1502e-05)\n",
            "19372 Traning Loss: tensor(2.4486e-05)\n",
            "19373 Traning Loss: tensor(2.3730e-05)\n",
            "19374 Traning Loss: tensor(2.2883e-05)\n",
            "19375 Traning Loss: tensor(2.3907e-05)\n",
            "19376 Traning Loss: tensor(2.2623e-05)\n",
            "19377 Traning Loss: tensor(2.2045e-05)\n",
            "19378 Traning Loss: tensor(2.3475e-05)\n",
            "19379 Traning Loss: tensor(2.2599e-05)\n",
            "19380 Traning Loss: tensor(2.3410e-05)\n",
            "19381 Traning Loss: tensor(2.3148e-05)\n",
            "19382 Traning Loss: tensor(2.3681e-05)\n",
            "19383 Traning Loss: tensor(2.2043e-05)\n",
            "19384 Traning Loss: tensor(2.3204e-05)\n",
            "19385 Traning Loss: tensor(2.2465e-05)\n",
            "19386 Traning Loss: tensor(2.3537e-05)\n",
            "19387 Traning Loss: tensor(2.2519e-05)\n",
            "19388 Traning Loss: tensor(2.2156e-05)\n",
            "19389 Traning Loss: tensor(2.4521e-05)\n",
            "19390 Traning Loss: tensor(2.1272e-05)\n",
            "19391 Traning Loss: tensor(2.4798e-05)\n",
            "19392 Traning Loss: tensor(2.3261e-05)\n",
            "19393 Traning Loss: tensor(2.3675e-05)\n",
            "19394 Traning Loss: tensor(2.2647e-05)\n",
            "19395 Traning Loss: tensor(2.2411e-05)\n",
            "19396 Traning Loss: tensor(2.3040e-05)\n",
            "19397 Traning Loss: tensor(2.3406e-05)\n",
            "19398 Traning Loss: tensor(2.1458e-05)\n",
            "19399 Traning Loss: tensor(2.6256e-05)\n",
            "19400 Traning Loss: tensor(2.3431e-05)\n",
            "19401 Traning Loss: tensor(2.1949e-05)\n",
            "19402 Traning Loss: tensor(2.4250e-05)\n",
            "19403 Traning Loss: tensor(2.4237e-05)\n",
            "19404 Traning Loss: tensor(2.4735e-05)\n",
            "19405 Traning Loss: tensor(2.3300e-05)\n",
            "19406 Traning Loss: tensor(2.3327e-05)\n",
            "19407 Traning Loss: tensor(2.4808e-05)\n",
            "19408 Traning Loss: tensor(2.8801e-05)\n",
            "19409 Traning Loss: tensor(2.7008e-05)\n",
            "19410 Traning Loss: tensor(2.5841e-05)\n",
            "19411 Traning Loss: tensor(2.3308e-05)\n",
            "19412 Traning Loss: tensor(2.3723e-05)\n",
            "19413 Traning Loss: tensor(2.7577e-05)\n",
            "19414 Traning Loss: tensor(2.7546e-05)\n",
            "19415 Traning Loss: tensor(2.3128e-05)\n",
            "19416 Traning Loss: tensor(2.1873e-05)\n",
            "19417 Traning Loss: tensor(2.2246e-05)\n",
            "19418 Traning Loss: tensor(2.3820e-05)\n",
            "19419 Traning Loss: tensor(2.6033e-05)\n",
            "19420 Traning Loss: tensor(2.4807e-05)\n",
            "19421 Traning Loss: tensor(2.3007e-05)\n",
            "19422 Traning Loss: tensor(2.1164e-05)\n",
            "19423 Traning Loss: tensor(2.4250e-05)\n",
            "19424 Traning Loss: tensor(2.3093e-05)\n",
            "19425 Traning Loss: tensor(2.4718e-05)\n",
            "19426 Traning Loss: tensor(2.4500e-05)\n",
            "19427 Traning Loss: tensor(2.2276e-05)\n",
            "19428 Traning Loss: tensor(2.3477e-05)\n",
            "19429 Traning Loss: tensor(2.3101e-05)\n",
            "19430 Traning Loss: tensor(2.3244e-05)\n",
            "19431 Traning Loss: tensor(2.4441e-05)\n",
            "19432 Traning Loss: tensor(2.1991e-05)\n",
            "19433 Traning Loss: tensor(2.3229e-05)\n",
            "19434 Traning Loss: tensor(2.4162e-05)\n",
            "19435 Traning Loss: tensor(2.4580e-05)\n",
            "19436 Traning Loss: tensor(2.3158e-05)\n",
            "19437 Traning Loss: tensor(2.3165e-05)\n",
            "19438 Traning Loss: tensor(2.3700e-05)\n",
            "19439 Traning Loss: tensor(2.1240e-05)\n",
            "19440 Traning Loss: tensor(2.4518e-05)\n",
            "19441 Traning Loss: tensor(2.4450e-05)\n",
            "19442 Traning Loss: tensor(2.4109e-05)\n",
            "19443 Traning Loss: tensor(2.2852e-05)\n",
            "19444 Traning Loss: tensor(2.1683e-05)\n",
            "19445 Traning Loss: tensor(2.3136e-05)\n",
            "19446 Traning Loss: tensor(2.2925e-05)\n",
            "19447 Traning Loss: tensor(2.4193e-05)\n",
            "19448 Traning Loss: tensor(2.1388e-05)\n",
            "19449 Traning Loss: tensor(2.2282e-05)\n",
            "19450 Traning Loss: tensor(2.2229e-05)\n",
            "19451 Traning Loss: tensor(2.3647e-05)\n",
            "19452 Traning Loss: tensor(2.2381e-05)\n",
            "19453 Traning Loss: tensor(2.3302e-05)\n",
            "19454 Traning Loss: tensor(2.5281e-05)\n",
            "19455 Traning Loss: tensor(2.0740e-05)\n",
            "19456 Traning Loss: tensor(2.2733e-05)\n",
            "19457 Traning Loss: tensor(2.3406e-05)\n",
            "19458 Traning Loss: tensor(2.5417e-05)\n",
            "19459 Traning Loss: tensor(2.3019e-05)\n",
            "19460 Traning Loss: tensor(2.3478e-05)\n",
            "19461 Traning Loss: tensor(2.3203e-05)\n",
            "19462 Traning Loss: tensor(2.4395e-05)\n",
            "19463 Traning Loss: tensor(2.4654e-05)\n",
            "19464 Traning Loss: tensor(2.1860e-05)\n",
            "19465 Traning Loss: tensor(2.3023e-05)\n",
            "19466 Traning Loss: tensor(2.2652e-05)\n",
            "19467 Traning Loss: tensor(1.9814e-05)\n",
            "19468 Traning Loss: tensor(2.5239e-05)\n",
            "19469 Traning Loss: tensor(2.2838e-05)\n",
            "19470 Traning Loss: tensor(2.5921e-05)\n",
            "19471 Traning Loss: tensor(2.2377e-05)\n",
            "19472 Traning Loss: tensor(2.3695e-05)\n",
            "19473 Traning Loss: tensor(2.3363e-05)\n",
            "19474 Traning Loss: tensor(2.2677e-05)\n",
            "19475 Traning Loss: tensor(2.5414e-05)\n",
            "19476 Traning Loss: tensor(2.1940e-05)\n",
            "19477 Traning Loss: tensor(3.0157e-05)\n",
            "19478 Traning Loss: tensor(2.1871e-05)\n",
            "19479 Traning Loss: tensor(2.4842e-05)\n",
            "19480 Traning Loss: tensor(2.4311e-05)\n",
            "19481 Traning Loss: tensor(2.2271e-05)\n",
            "19482 Traning Loss: tensor(2.1495e-05)\n",
            "19483 Traning Loss: tensor(2.5479e-05)\n",
            "19484 Traning Loss: tensor(2.4741e-05)\n",
            "19485 Traning Loss: tensor(2.5355e-05)\n",
            "19486 Traning Loss: tensor(2.0724e-05)\n",
            "19487 Traning Loss: tensor(2.3419e-05)\n",
            "19488 Traning Loss: tensor(2.2118e-05)\n",
            "19489 Traning Loss: tensor(2.4016e-05)\n",
            "19490 Traning Loss: tensor(2.3590e-05)\n",
            "19491 Traning Loss: tensor(2.7215e-05)\n",
            "19492 Traning Loss: tensor(2.3563e-05)\n",
            "19493 Traning Loss: tensor(2.1518e-05)\n",
            "19494 Traning Loss: tensor(2.3367e-05)\n",
            "19495 Traning Loss: tensor(2.1458e-05)\n",
            "19496 Traning Loss: tensor(2.2808e-05)\n",
            "19497 Traning Loss: tensor(2.2851e-05)\n",
            "19498 Traning Loss: tensor(2.3004e-05)\n",
            "19499 Traning Loss: tensor(2.3102e-05)\n",
            "19500 Traning Loss: tensor(2.3141e-05)\n",
            "19501 Traning Loss: tensor(2.1795e-05)\n",
            "19502 Traning Loss: tensor(2.3875e-05)\n",
            "19503 Traning Loss: tensor(2.3029e-05)\n",
            "19504 Traning Loss: tensor(2.2999e-05)\n",
            "19505 Traning Loss: tensor(2.1029e-05)\n",
            "19506 Traning Loss: tensor(2.1651e-05)\n",
            "19507 Traning Loss: tensor(2.2636e-05)\n",
            "19508 Traning Loss: tensor(2.3182e-05)\n",
            "19509 Traning Loss: tensor(2.1231e-05)\n",
            "19510 Traning Loss: tensor(2.2931e-05)\n",
            "19511 Traning Loss: tensor(2.3325e-05)\n",
            "19512 Traning Loss: tensor(2.4404e-05)\n",
            "19513 Traning Loss: tensor(2.2402e-05)\n",
            "19514 Traning Loss: tensor(2.2989e-05)\n",
            "19515 Traning Loss: tensor(2.1465e-05)\n",
            "19516 Traning Loss: tensor(2.2165e-05)\n",
            "19517 Traning Loss: tensor(2.2477e-05)\n",
            "19518 Traning Loss: tensor(2.1544e-05)\n",
            "19519 Traning Loss: tensor(2.3597e-05)\n",
            "19520 Traning Loss: tensor(2.3366e-05)\n",
            "19521 Traning Loss: tensor(2.5311e-05)\n",
            "19522 Traning Loss: tensor(2.4750e-05)\n",
            "19523 Traning Loss: tensor(2.4423e-05)\n",
            "19524 Traning Loss: tensor(2.1838e-05)\n",
            "19525 Traning Loss: tensor(2.3289e-05)\n",
            "19526 Traning Loss: tensor(2.1959e-05)\n",
            "19527 Traning Loss: tensor(2.4070e-05)\n",
            "19528 Traning Loss: tensor(2.2003e-05)\n",
            "19529 Traning Loss: tensor(2.2642e-05)\n",
            "19530 Traning Loss: tensor(2.1843e-05)\n",
            "19531 Traning Loss: tensor(2.2456e-05)\n",
            "19532 Traning Loss: tensor(2.2739e-05)\n",
            "19533 Traning Loss: tensor(2.2252e-05)\n",
            "19534 Traning Loss: tensor(2.4750e-05)\n",
            "19535 Traning Loss: tensor(2.5249e-05)\n",
            "19536 Traning Loss: tensor(2.4946e-05)\n",
            "19537 Traning Loss: tensor(2.2780e-05)\n",
            "19538 Traning Loss: tensor(2.3492e-05)\n",
            "19539 Traning Loss: tensor(2.3578e-05)\n",
            "19540 Traning Loss: tensor(2.4689e-05)\n",
            "19541 Traning Loss: tensor(2.5135e-05)\n",
            "19542 Traning Loss: tensor(2.2688e-05)\n",
            "19543 Traning Loss: tensor(2.2658e-05)\n",
            "19544 Traning Loss: tensor(2.4816e-05)\n",
            "19545 Traning Loss: tensor(2.1332e-05)\n",
            "19546 Traning Loss: tensor(2.2866e-05)\n",
            "19547 Traning Loss: tensor(2.6668e-05)\n",
            "19548 Traning Loss: tensor(2.1017e-05)\n",
            "19549 Traning Loss: tensor(2.4657e-05)\n",
            "19550 Traning Loss: tensor(2.2432e-05)\n",
            "19551 Traning Loss: tensor(2.4081e-05)\n",
            "19552 Traning Loss: tensor(2.3449e-05)\n",
            "19553 Traning Loss: tensor(2.2995e-05)\n",
            "19554 Traning Loss: tensor(2.3272e-05)\n",
            "19555 Traning Loss: tensor(2.2561e-05)\n",
            "19556 Traning Loss: tensor(2.2887e-05)\n",
            "19557 Traning Loss: tensor(2.1249e-05)\n",
            "19558 Traning Loss: tensor(2.6080e-05)\n",
            "19559 Traning Loss: tensor(2.0806e-05)\n",
            "19560 Traning Loss: tensor(2.4207e-05)\n",
            "19561 Traning Loss: tensor(2.2614e-05)\n",
            "19562 Traning Loss: tensor(2.2225e-05)\n",
            "19563 Traning Loss: tensor(2.5500e-05)\n",
            "19564 Traning Loss: tensor(2.2333e-05)\n",
            "19565 Traning Loss: tensor(2.4013e-05)\n",
            "19566 Traning Loss: tensor(1.9976e-05)\n",
            "19567 Traning Loss: tensor(2.2884e-05)\n",
            "19568 Traning Loss: tensor(2.2402e-05)\n",
            "19569 Traning Loss: tensor(2.2567e-05)\n",
            "19570 Traning Loss: tensor(2.2075e-05)\n",
            "19571 Traning Loss: tensor(2.0406e-05)\n",
            "19572 Traning Loss: tensor(3.1185e-05)\n",
            "19573 Traning Loss: tensor(2.3414e-05)\n",
            "19574 Traning Loss: tensor(2.2533e-05)\n",
            "19575 Traning Loss: tensor(2.2992e-05)\n",
            "19576 Traning Loss: tensor(2.2546e-05)\n",
            "19577 Traning Loss: tensor(2.1966e-05)\n",
            "19578 Traning Loss: tensor(2.3314e-05)\n",
            "19579 Traning Loss: tensor(2.4947e-05)\n",
            "19580 Traning Loss: tensor(2.1072e-05)\n",
            "19581 Traning Loss: tensor(2.2265e-05)\n",
            "19582 Traning Loss: tensor(2.4366e-05)\n",
            "19583 Traning Loss: tensor(2.6144e-05)\n",
            "19584 Traning Loss: tensor(2.8845e-05)\n",
            "19585 Traning Loss: tensor(2.3645e-05)\n",
            "19586 Traning Loss: tensor(2.1183e-05)\n",
            "19587 Traning Loss: tensor(2.1481e-05)\n",
            "19588 Traning Loss: tensor(2.6417e-05)\n",
            "19589 Traning Loss: tensor(2.3359e-05)\n",
            "19590 Traning Loss: tensor(2.3987e-05)\n",
            "19591 Traning Loss: tensor(2.1186e-05)\n",
            "19592 Traning Loss: tensor(2.7640e-05)\n",
            "19593 Traning Loss: tensor(2.1994e-05)\n",
            "19594 Traning Loss: tensor(2.4923e-05)\n",
            "19595 Traning Loss: tensor(2.2626e-05)\n",
            "19596 Traning Loss: tensor(2.1922e-05)\n",
            "19597 Traning Loss: tensor(2.5851e-05)\n",
            "19598 Traning Loss: tensor(2.2582e-05)\n",
            "19599 Traning Loss: tensor(2.4411e-05)\n",
            "19600 Traning Loss: tensor(2.1898e-05)\n",
            "19601 Traning Loss: tensor(2.2676e-05)\n",
            "19602 Traning Loss: tensor(2.3624e-05)\n",
            "19603 Traning Loss: tensor(2.2784e-05)\n",
            "19604 Traning Loss: tensor(2.7991e-05)\n",
            "19605 Traning Loss: tensor(2.2118e-05)\n",
            "19606 Traning Loss: tensor(2.3015e-05)\n",
            "19607 Traning Loss: tensor(2.2664e-05)\n",
            "19608 Traning Loss: tensor(2.4926e-05)\n",
            "19609 Traning Loss: tensor(2.5360e-05)\n",
            "19610 Traning Loss: tensor(2.1948e-05)\n",
            "19611 Traning Loss: tensor(2.2322e-05)\n",
            "19612 Traning Loss: tensor(2.3557e-05)\n",
            "19613 Traning Loss: tensor(2.3844e-05)\n",
            "19614 Traning Loss: tensor(2.2528e-05)\n",
            "19615 Traning Loss: tensor(2.2496e-05)\n",
            "19616 Traning Loss: tensor(2.1543e-05)\n",
            "19617 Traning Loss: tensor(2.1159e-05)\n",
            "19618 Traning Loss: tensor(2.3143e-05)\n",
            "19619 Traning Loss: tensor(2.0926e-05)\n",
            "19620 Traning Loss: tensor(2.2060e-05)\n",
            "19621 Traning Loss: tensor(2.3330e-05)\n",
            "19622 Traning Loss: tensor(2.2680e-05)\n",
            "19623 Traning Loss: tensor(2.2172e-05)\n",
            "19624 Traning Loss: tensor(2.1659e-05)\n",
            "19625 Traning Loss: tensor(2.2524e-05)\n",
            "19626 Traning Loss: tensor(2.2403e-05)\n",
            "19627 Traning Loss: tensor(2.3047e-05)\n",
            "19628 Traning Loss: tensor(2.2125e-05)\n",
            "19629 Traning Loss: tensor(2.2168e-05)\n",
            "19630 Traning Loss: tensor(2.2120e-05)\n",
            "19631 Traning Loss: tensor(2.1557e-05)\n",
            "19632 Traning Loss: tensor(2.2109e-05)\n",
            "19633 Traning Loss: tensor(2.1322e-05)\n",
            "19634 Traning Loss: tensor(2.2898e-05)\n",
            "19635 Traning Loss: tensor(2.1386e-05)\n",
            "19636 Traning Loss: tensor(2.2951e-05)\n",
            "19637 Traning Loss: tensor(2.0526e-05)\n",
            "19638 Traning Loss: tensor(2.2102e-05)\n",
            "19639 Traning Loss: tensor(2.2838e-05)\n",
            "19640 Traning Loss: tensor(2.2624e-05)\n",
            "19641 Traning Loss: tensor(2.1843e-05)\n",
            "19642 Traning Loss: tensor(2.2855e-05)\n",
            "19643 Traning Loss: tensor(2.4509e-05)\n",
            "19644 Traning Loss: tensor(2.4378e-05)\n",
            "19645 Traning Loss: tensor(2.2976e-05)\n",
            "19646 Traning Loss: tensor(2.2405e-05)\n",
            "19647 Traning Loss: tensor(2.3170e-05)\n",
            "19648 Traning Loss: tensor(2.5299e-05)\n",
            "19649 Traning Loss: tensor(2.6628e-05)\n",
            "19650 Traning Loss: tensor(2.6370e-05)\n",
            "19651 Traning Loss: tensor(2.2972e-05)\n",
            "19652 Traning Loss: tensor(2.1591e-05)\n",
            "19653 Traning Loss: tensor(2.1645e-05)\n",
            "19654 Traning Loss: tensor(2.0743e-05)\n",
            "19655 Traning Loss: tensor(2.2299e-05)\n",
            "19656 Traning Loss: tensor(2.2398e-05)\n",
            "19657 Traning Loss: tensor(2.1498e-05)\n",
            "19658 Traning Loss: tensor(2.2973e-05)\n",
            "19659 Traning Loss: tensor(2.6370e-05)\n",
            "19660 Traning Loss: tensor(2.2325e-05)\n",
            "19661 Traning Loss: tensor(2.3435e-05)\n",
            "19662 Traning Loss: tensor(2.2384e-05)\n",
            "19663 Traning Loss: tensor(2.5532e-05)\n",
            "19664 Traning Loss: tensor(2.5026e-05)\n",
            "19665 Traning Loss: tensor(2.3621e-05)\n",
            "19666 Traning Loss: tensor(2.0591e-05)\n",
            "19667 Traning Loss: tensor(2.1401e-05)\n",
            "19668 Traning Loss: tensor(2.6130e-05)\n",
            "19669 Traning Loss: tensor(2.4276e-05)\n",
            "19670 Traning Loss: tensor(2.1840e-05)\n",
            "19671 Traning Loss: tensor(2.1981e-05)\n",
            "19672 Traning Loss: tensor(2.4851e-05)\n",
            "19673 Traning Loss: tensor(2.3680e-05)\n",
            "19674 Traning Loss: tensor(2.4370e-05)\n",
            "19675 Traning Loss: tensor(2.3158e-05)\n",
            "19676 Traning Loss: tensor(2.1836e-05)\n",
            "19677 Traning Loss: tensor(2.3575e-05)\n",
            "19678 Traning Loss: tensor(2.5592e-05)\n",
            "19679 Traning Loss: tensor(2.2895e-05)\n",
            "19680 Traning Loss: tensor(2.0221e-05)\n",
            "19681 Traning Loss: tensor(2.4592e-05)\n",
            "19682 Traning Loss: tensor(2.3224e-05)\n",
            "19683 Traning Loss: tensor(2.4163e-05)\n",
            "19684 Traning Loss: tensor(2.0828e-05)\n",
            "19685 Traning Loss: tensor(2.2846e-05)\n",
            "19686 Traning Loss: tensor(2.1939e-05)\n",
            "19687 Traning Loss: tensor(2.2918e-05)\n",
            "19688 Traning Loss: tensor(2.0814e-05)\n",
            "19689 Traning Loss: tensor(2.1923e-05)\n",
            "19690 Traning Loss: tensor(2.1270e-05)\n",
            "19691 Traning Loss: tensor(2.2988e-05)\n",
            "19692 Traning Loss: tensor(2.2610e-05)\n",
            "19693 Traning Loss: tensor(2.3079e-05)\n",
            "19694 Traning Loss: tensor(2.1471e-05)\n",
            "19695 Traning Loss: tensor(2.1227e-05)\n",
            "19696 Traning Loss: tensor(2.1793e-05)\n",
            "19697 Traning Loss: tensor(2.1199e-05)\n",
            "19698 Traning Loss: tensor(2.1855e-05)\n",
            "19699 Traning Loss: tensor(2.1698e-05)\n",
            "19700 Traning Loss: tensor(2.2214e-05)\n",
            "19701 Traning Loss: tensor(2.2657e-05)\n",
            "19702 Traning Loss: tensor(2.0821e-05)\n",
            "19703 Traning Loss: tensor(2.2096e-05)\n",
            "19704 Traning Loss: tensor(2.2312e-05)\n",
            "19705 Traning Loss: tensor(2.3989e-05)\n",
            "19706 Traning Loss: tensor(2.2360e-05)\n",
            "19707 Traning Loss: tensor(2.4140e-05)\n",
            "19708 Traning Loss: tensor(2.5198e-05)\n",
            "19709 Traning Loss: tensor(2.5484e-05)\n",
            "19710 Traning Loss: tensor(2.3714e-05)\n",
            "19711 Traning Loss: tensor(2.0123e-05)\n",
            "19712 Traning Loss: tensor(2.1804e-05)\n",
            "19713 Traning Loss: tensor(2.5919e-05)\n",
            "19714 Traning Loss: tensor(2.3084e-05)\n",
            "19715 Traning Loss: tensor(2.1789e-05)\n",
            "19716 Traning Loss: tensor(2.3074e-05)\n",
            "19717 Traning Loss: tensor(2.0642e-05)\n",
            "19718 Traning Loss: tensor(2.1870e-05)\n",
            "19719 Traning Loss: tensor(2.2224e-05)\n",
            "19720 Traning Loss: tensor(2.2974e-05)\n",
            "19721 Traning Loss: tensor(2.5607e-05)\n",
            "19722 Traning Loss: tensor(2.4744e-05)\n",
            "19723 Traning Loss: tensor(2.3212e-05)\n",
            "19724 Traning Loss: tensor(2.1387e-05)\n",
            "19725 Traning Loss: tensor(2.3849e-05)\n",
            "19726 Traning Loss: tensor(2.3022e-05)\n",
            "19727 Traning Loss: tensor(2.2905e-05)\n",
            "19728 Traning Loss: tensor(2.1816e-05)\n",
            "19729 Traning Loss: tensor(2.3119e-05)\n",
            "19730 Traning Loss: tensor(2.1764e-05)\n",
            "19731 Traning Loss: tensor(2.4645e-05)\n",
            "19732 Traning Loss: tensor(2.0803e-05)\n",
            "19733 Traning Loss: tensor(2.3168e-05)\n",
            "19734 Traning Loss: tensor(2.2916e-05)\n",
            "19735 Traning Loss: tensor(2.5228e-05)\n",
            "19736 Traning Loss: tensor(2.4452e-05)\n",
            "19737 Traning Loss: tensor(2.1094e-05)\n",
            "19738 Traning Loss: tensor(2.5165e-05)\n",
            "19739 Traning Loss: tensor(2.2574e-05)\n",
            "19740 Traning Loss: tensor(2.1856e-05)\n",
            "19741 Traning Loss: tensor(2.3468e-05)\n",
            "19742 Traning Loss: tensor(2.1806e-05)\n",
            "19743 Traning Loss: tensor(2.3759e-05)\n",
            "19744 Traning Loss: tensor(2.1988e-05)\n",
            "19745 Traning Loss: tensor(2.6735e-05)\n",
            "19746 Traning Loss: tensor(2.1939e-05)\n",
            "19747 Traning Loss: tensor(2.2860e-05)\n",
            "19748 Traning Loss: tensor(2.3448e-05)\n",
            "19749 Traning Loss: tensor(2.2790e-05)\n",
            "19750 Traning Loss: tensor(2.4156e-05)\n",
            "19751 Traning Loss: tensor(2.3502e-05)\n",
            "19752 Traning Loss: tensor(2.4752e-05)\n",
            "19753 Traning Loss: tensor(2.1812e-05)\n",
            "19754 Traning Loss: tensor(2.2463e-05)\n",
            "19755 Traning Loss: tensor(2.2788e-05)\n",
            "19756 Traning Loss: tensor(2.1620e-05)\n",
            "19757 Traning Loss: tensor(2.2326e-05)\n",
            "19758 Traning Loss: tensor(3.1174e-05)\n",
            "19759 Traning Loss: tensor(2.2854e-05)\n",
            "19760 Traning Loss: tensor(2.1144e-05)\n",
            "19761 Traning Loss: tensor(2.4327e-05)\n",
            "19762 Traning Loss: tensor(3.1779e-05)\n",
            "19763 Traning Loss: tensor(2.2185e-05)\n",
            "19764 Traning Loss: tensor(2.4510e-05)\n",
            "19765 Traning Loss: tensor(2.1574e-05)\n",
            "19766 Traning Loss: tensor(2.4290e-05)\n",
            "19767 Traning Loss: tensor(2.6001e-05)\n",
            "19768 Traning Loss: tensor(2.2945e-05)\n",
            "19769 Traning Loss: tensor(2.3295e-05)\n",
            "19770 Traning Loss: tensor(2.2321e-05)\n",
            "19771 Traning Loss: tensor(2.4578e-05)\n",
            "19772 Traning Loss: tensor(2.2687e-05)\n",
            "19773 Traning Loss: tensor(2.3672e-05)\n",
            "19774 Traning Loss: tensor(2.2801e-05)\n",
            "19775 Traning Loss: tensor(2.4265e-05)\n",
            "19776 Traning Loss: tensor(2.1558e-05)\n",
            "19777 Traning Loss: tensor(2.2833e-05)\n",
            "19778 Traning Loss: tensor(2.2138e-05)\n",
            "19779 Traning Loss: tensor(2.3924e-05)\n",
            "19780 Traning Loss: tensor(2.2798e-05)\n",
            "19781 Traning Loss: tensor(1.9745e-05)\n",
            "19782 Traning Loss: tensor(2.0288e-05)\n",
            "19783 Traning Loss: tensor(2.4763e-05)\n",
            "19784 Traning Loss: tensor(2.1536e-05)\n",
            "19785 Traning Loss: tensor(2.1516e-05)\n",
            "19786 Traning Loss: tensor(2.3320e-05)\n",
            "19787 Traning Loss: tensor(2.4326e-05)\n",
            "19788 Traning Loss: tensor(2.2784e-05)\n",
            "19789 Traning Loss: tensor(2.3396e-05)\n",
            "19790 Traning Loss: tensor(2.3810e-05)\n",
            "19791 Traning Loss: tensor(2.5617e-05)\n",
            "19792 Traning Loss: tensor(2.8344e-05)\n",
            "19793 Traning Loss: tensor(2.2482e-05)\n",
            "19794 Traning Loss: tensor(2.2156e-05)\n",
            "19795 Traning Loss: tensor(2.2486e-05)\n",
            "19796 Traning Loss: tensor(2.5249e-05)\n",
            "19797 Traning Loss: tensor(2.2500e-05)\n",
            "19798 Traning Loss: tensor(2.3775e-05)\n",
            "19799 Traning Loss: tensor(2.4260e-05)\n",
            "19800 Traning Loss: tensor(2.4632e-05)\n",
            "19801 Traning Loss: tensor(2.2664e-05)\n",
            "19802 Traning Loss: tensor(2.2235e-05)\n",
            "19803 Traning Loss: tensor(2.1398e-05)\n",
            "19804 Traning Loss: tensor(2.2092e-05)\n",
            "19805 Traning Loss: tensor(2.2687e-05)\n",
            "19806 Traning Loss: tensor(2.2335e-05)\n",
            "19807 Traning Loss: tensor(2.0976e-05)\n",
            "19808 Traning Loss: tensor(2.2089e-05)\n",
            "19809 Traning Loss: tensor(2.2641e-05)\n",
            "19810 Traning Loss: tensor(2.1677e-05)\n",
            "19811 Traning Loss: tensor(2.1306e-05)\n",
            "19812 Traning Loss: tensor(2.1799e-05)\n",
            "19813 Traning Loss: tensor(2.1541e-05)\n",
            "19814 Traning Loss: tensor(2.0993e-05)\n",
            "19815 Traning Loss: tensor(2.2072e-05)\n",
            "19816 Traning Loss: tensor(2.0290e-05)\n",
            "19817 Traning Loss: tensor(2.1921e-05)\n",
            "19818 Traning Loss: tensor(3.1794e-05)\n",
            "19819 Traning Loss: tensor(2.2214e-05)\n",
            "19820 Traning Loss: tensor(2.6998e-05)\n",
            "19821 Traning Loss: tensor(2.3120e-05)\n",
            "19822 Traning Loss: tensor(2.3169e-05)\n",
            "19823 Traning Loss: tensor(2.0103e-05)\n",
            "19824 Traning Loss: tensor(2.4416e-05)\n",
            "19825 Traning Loss: tensor(2.1990e-05)\n",
            "19826 Traning Loss: tensor(2.2245e-05)\n",
            "19827 Traning Loss: tensor(2.3539e-05)\n",
            "19828 Traning Loss: tensor(2.1799e-05)\n",
            "19829 Traning Loss: tensor(2.3606e-05)\n",
            "19830 Traning Loss: tensor(2.6687e-05)\n",
            "19831 Traning Loss: tensor(2.2558e-05)\n",
            "19832 Traning Loss: tensor(2.1946e-05)\n",
            "19833 Traning Loss: tensor(2.1632e-05)\n",
            "19834 Traning Loss: tensor(2.4216e-05)\n",
            "19835 Traning Loss: tensor(2.3489e-05)\n",
            "19836 Traning Loss: tensor(2.2747e-05)\n",
            "19837 Traning Loss: tensor(2.3059e-05)\n",
            "19838 Traning Loss: tensor(2.1235e-05)\n",
            "19839 Traning Loss: tensor(2.5956e-05)\n",
            "19840 Traning Loss: tensor(2.1266e-05)\n",
            "19841 Traning Loss: tensor(2.2563e-05)\n",
            "19842 Traning Loss: tensor(2.2887e-05)\n",
            "19843 Traning Loss: tensor(2.3023e-05)\n",
            "19844 Traning Loss: tensor(2.4031e-05)\n",
            "19845 Traning Loss: tensor(2.1498e-05)\n",
            "19846 Traning Loss: tensor(2.1484e-05)\n",
            "19847 Traning Loss: tensor(2.2870e-05)\n",
            "19848 Traning Loss: tensor(2.3977e-05)\n",
            "19849 Traning Loss: tensor(2.7548e-05)\n",
            "19850 Traning Loss: tensor(2.3144e-05)\n",
            "19851 Traning Loss: tensor(2.6705e-05)\n",
            "19852 Traning Loss: tensor(2.2660e-05)\n",
            "19853 Traning Loss: tensor(2.1509e-05)\n",
            "19854 Traning Loss: tensor(2.3263e-05)\n",
            "19855 Traning Loss: tensor(2.5054e-05)\n",
            "19856 Traning Loss: tensor(2.2641e-05)\n",
            "19857 Traning Loss: tensor(2.3537e-05)\n",
            "19858 Traning Loss: tensor(2.1384e-05)\n",
            "19859 Traning Loss: tensor(2.2847e-05)\n",
            "19860 Traning Loss: tensor(2.4531e-05)\n",
            "19861 Traning Loss: tensor(2.3006e-05)\n",
            "19862 Traning Loss: tensor(2.0906e-05)\n",
            "19863 Traning Loss: tensor(2.3214e-05)\n",
            "19864 Traning Loss: tensor(2.1464e-05)\n",
            "19865 Traning Loss: tensor(2.2547e-05)\n",
            "19866 Traning Loss: tensor(2.1660e-05)\n",
            "19867 Traning Loss: tensor(2.2361e-05)\n",
            "19868 Traning Loss: tensor(2.1734e-05)\n",
            "19869 Traning Loss: tensor(2.2000e-05)\n",
            "19870 Traning Loss: tensor(2.6368e-05)\n",
            "19871 Traning Loss: tensor(2.0541e-05)\n",
            "19872 Traning Loss: tensor(2.0250e-05)\n",
            "19873 Traning Loss: tensor(2.1760e-05)\n",
            "19874 Traning Loss: tensor(2.0800e-05)\n",
            "19875 Traning Loss: tensor(2.1797e-05)\n",
            "19876 Traning Loss: tensor(2.2370e-05)\n",
            "19877 Traning Loss: tensor(2.3970e-05)\n",
            "19878 Traning Loss: tensor(2.3610e-05)\n",
            "19879 Traning Loss: tensor(2.1562e-05)\n",
            "19880 Traning Loss: tensor(2.4810e-05)\n",
            "19881 Traning Loss: tensor(2.3064e-05)\n",
            "19882 Traning Loss: tensor(2.1422e-05)\n",
            "19883 Traning Loss: tensor(2.1185e-05)\n",
            "19884 Traning Loss: tensor(2.1561e-05)\n",
            "19885 Traning Loss: tensor(2.4263e-05)\n",
            "19886 Traning Loss: tensor(2.3197e-05)\n",
            "19887 Traning Loss: tensor(2.3100e-05)\n",
            "19888 Traning Loss: tensor(2.0836e-05)\n",
            "19889 Traning Loss: tensor(2.1324e-05)\n",
            "19890 Traning Loss: tensor(2.5691e-05)\n",
            "19891 Traning Loss: tensor(2.2538e-05)\n",
            "19892 Traning Loss: tensor(2.1890e-05)\n",
            "19893 Traning Loss: tensor(2.2034e-05)\n",
            "19894 Traning Loss: tensor(2.0702e-05)\n",
            "19895 Traning Loss: tensor(2.1906e-05)\n",
            "19896 Traning Loss: tensor(2.3949e-05)\n",
            "19897 Traning Loss: tensor(2.4787e-05)\n",
            "19898 Traning Loss: tensor(2.2305e-05)\n",
            "19899 Traning Loss: tensor(2.7131e-05)\n",
            "19900 Traning Loss: tensor(2.2395e-05)\n",
            "19901 Traning Loss: tensor(2.4119e-05)\n",
            "19902 Traning Loss: tensor(2.2475e-05)\n",
            "19903 Traning Loss: tensor(2.2881e-05)\n",
            "19904 Traning Loss: tensor(2.0785e-05)\n",
            "19905 Traning Loss: tensor(2.5745e-05)\n",
            "19906 Traning Loss: tensor(2.0985e-05)\n",
            "19907 Traning Loss: tensor(1.9880e-05)\n",
            "19908 Traning Loss: tensor(2.4698e-05)\n",
            "19909 Traning Loss: tensor(2.5265e-05)\n",
            "19910 Traning Loss: tensor(2.2571e-05)\n",
            "19911 Traning Loss: tensor(2.0133e-05)\n",
            "19912 Traning Loss: tensor(2.3677e-05)\n",
            "19913 Traning Loss: tensor(2.1251e-05)\n",
            "19914 Traning Loss: tensor(2.1717e-05)\n",
            "19915 Traning Loss: tensor(2.1699e-05)\n",
            "19916 Traning Loss: tensor(2.2492e-05)\n",
            "19917 Traning Loss: tensor(2.2240e-05)\n",
            "19918 Traning Loss: tensor(2.0687e-05)\n",
            "19919 Traning Loss: tensor(2.2434e-05)\n",
            "19920 Traning Loss: tensor(2.1483e-05)\n",
            "19921 Traning Loss: tensor(2.2708e-05)\n",
            "19922 Traning Loss: tensor(2.3103e-05)\n",
            "19923 Traning Loss: tensor(2.1240e-05)\n",
            "19924 Traning Loss: tensor(2.2336e-05)\n",
            "19925 Traning Loss: tensor(2.2500e-05)\n",
            "19926 Traning Loss: tensor(2.1081e-05)\n",
            "19927 Traning Loss: tensor(2.0436e-05)\n",
            "19928 Traning Loss: tensor(2.2039e-05)\n",
            "19929 Traning Loss: tensor(2.3519e-05)\n",
            "19930 Traning Loss: tensor(2.1678e-05)\n",
            "19931 Traning Loss: tensor(2.2605e-05)\n",
            "19932 Traning Loss: tensor(2.2744e-05)\n",
            "19933 Traning Loss: tensor(2.2097e-05)\n",
            "19934 Traning Loss: tensor(2.0261e-05)\n",
            "19935 Traning Loss: tensor(2.0863e-05)\n",
            "19936 Traning Loss: tensor(2.3344e-05)\n",
            "19937 Traning Loss: tensor(2.0524e-05)\n",
            "19938 Traning Loss: tensor(2.3786e-05)\n",
            "19939 Traning Loss: tensor(2.0880e-05)\n",
            "19940 Traning Loss: tensor(2.3567e-05)\n",
            "19941 Traning Loss: tensor(2.1459e-05)\n",
            "19942 Traning Loss: tensor(2.1270e-05)\n",
            "19943 Traning Loss: tensor(2.1623e-05)\n",
            "19944 Traning Loss: tensor(2.1080e-05)\n",
            "19945 Traning Loss: tensor(2.1119e-05)\n",
            "19946 Traning Loss: tensor(2.2420e-05)\n",
            "19947 Traning Loss: tensor(2.1355e-05)\n",
            "19948 Traning Loss: tensor(2.0936e-05)\n",
            "19949 Traning Loss: tensor(2.1836e-05)\n",
            "19950 Traning Loss: tensor(1.8807e-05)\n",
            "19951 Traning Loss: tensor(2.2089e-05)\n",
            "19952 Traning Loss: tensor(2.0658e-05)\n",
            "19953 Traning Loss: tensor(2.0604e-05)\n",
            "19954 Traning Loss: tensor(1.9457e-05)\n",
            "19955 Traning Loss: tensor(2.2996e-05)\n",
            "19956 Traning Loss: tensor(2.1142e-05)\n",
            "19957 Traning Loss: tensor(2.2465e-05)\n",
            "19958 Traning Loss: tensor(2.3544e-05)\n",
            "19959 Traning Loss: tensor(2.2879e-05)\n",
            "19960 Traning Loss: tensor(2.2273e-05)\n",
            "19961 Traning Loss: tensor(2.1510e-05)\n",
            "19962 Traning Loss: tensor(2.1421e-05)\n",
            "19963 Traning Loss: tensor(2.2227e-05)\n",
            "19964 Traning Loss: tensor(2.0924e-05)\n",
            "19965 Traning Loss: tensor(2.1380e-05)\n",
            "19966 Traning Loss: tensor(2.2224e-05)\n",
            "19967 Traning Loss: tensor(2.2169e-05)\n",
            "19968 Traning Loss: tensor(2.0540e-05)\n",
            "19969 Traning Loss: tensor(2.0934e-05)\n",
            "19970 Traning Loss: tensor(2.0805e-05)\n",
            "19971 Traning Loss: tensor(2.1356e-05)\n",
            "19972 Traning Loss: tensor(2.0802e-05)\n",
            "19973 Traning Loss: tensor(1.9750e-05)\n",
            "19974 Traning Loss: tensor(2.2320e-05)\n",
            "19975 Traning Loss: tensor(2.3647e-05)\n",
            "19976 Traning Loss: tensor(2.1847e-05)\n",
            "19977 Traning Loss: tensor(2.1464e-05)\n",
            "19978 Traning Loss: tensor(2.0332e-05)\n",
            "19979 Traning Loss: tensor(2.1468e-05)\n",
            "19980 Traning Loss: tensor(2.0445e-05)\n",
            "19981 Traning Loss: tensor(2.0812e-05)\n",
            "19982 Traning Loss: tensor(2.1748e-05)\n",
            "19983 Traning Loss: tensor(2.2472e-05)\n",
            "19984 Traning Loss: tensor(2.1509e-05)\n",
            "19985 Traning Loss: tensor(2.0884e-05)\n",
            "19986 Traning Loss: tensor(1.9219e-05)\n",
            "19987 Traning Loss: tensor(2.0954e-05)\n",
            "19988 Traning Loss: tensor(2.0004e-05)\n",
            "19989 Traning Loss: tensor(1.9715e-05)\n",
            "19990 Traning Loss: tensor(2.1910e-05)\n",
            "19991 Traning Loss: tensor(2.2595e-05)\n",
            "19992 Traning Loss: tensor(2.1207e-05)\n",
            "19993 Traning Loss: tensor(2.0700e-05)\n",
            "19994 Traning Loss: tensor(2.0418e-05)\n",
            "19995 Traning Loss: tensor(2.0706e-05)\n",
            "19996 Traning Loss: tensor(2.1389e-05)\n",
            "19997 Traning Loss: tensor(2.1188e-05)\n",
            "19998 Traning Loss: tensor(2.1763e-05)\n",
            "19999 Traning Loss: tensor(2.2554e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D  # 確保導入 3D 模組\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x=np.arange(0,2,0.02)\n",
        "t=np.arange(0,1,0.02)\n",
        "ms_x, ms_t = np.meshgrid(x, t)\n",
        "## Just because meshgrid is used, we need to do the following adjustment\n",
        "x = np.ravel(ms_x).reshape(-1,1)\n",
        "t = np.ravel(ms_t).reshape(-1,1)\n",
        "\n",
        "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
        "pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)\n",
        "pt_u = net(pt_x,pt_t)\n",
        "u=pt_u.data.cpu().numpy()\n",
        "ms_u = u.reshape(ms_x.shape)\n",
        "\n",
        "surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
        "\n",
        "\n",
        "\n",
        "ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
        "\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Iz7bvjXy1uJV",
        "outputId": "8908a167-92f7-41ee-af2d-327059843fdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAGPCAYAAAAKrEpmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAADx9klEQVR4nOz9eXwb9Z0/jj91+r4dH/EVJ3ac+75s0xZ+haYUumX3U46WT0u7PZZyQw8a2gJlKZRld0u7sNB0u0C3y6fdUmj5ci4N5Q4QEt/3GR/xKdmWdUsz8/tDeY/fM5qRZiTZGpN5Ph55JJFGb73HluY5r+v5NHAcx0GHDh06dOjQETeMyd6ADh06dOjQ8VGBTqo6dOjQoUNHgqCTqg4dOnTo0JEg6KSqQ4cOHTp0JAg6qerQoUOHDh0Jgk6qOnTo0KFDR4Kgk6oOHTp06NCRIOikqkOHDh06dCQIOqnq0KFDhw4dCYJOqjp06NChQ0eCoJOqDh06dOjQkSDopKpDhw4dOnQkCDqp6tChQ4cOHQmCTqo6dOjQoUNHgqCTqg4dOnTo0JEg6KSqQ4cOHTp0JAg6qerQoUOHDh0Jgk6qOnTo0KFDR4Kgk6oOHTp06NCRIOikqkOHDh06dCQIOqnq0KFDhw4dCYJOqjp06NChQ0eCoJOqDh06dOjQkSDopKpDhw4dOnQkCDqp6tChQ4cOHQmCTqo6dOjQoUNHgqCTqg4dOnTo0JEg6KSqQ4cOHTp0JAg6qerQoUOHDh0Jgk6qOnTo0KFDR4Kgk6oOHTp06NCRIOikqkOHDh06dCQIOqnq0KFDhw4dCYJOqjp06NChQ0eCoJOqDh06dOjQkSDopKpDhw4dOnQkCOZkb0DHuQeWZcEwDAwGA0wmEwwGQ7K3pEOHDh0JgU6qOlYMHMeBZVkEAgG43W4YDAYYjUaYzWaYzWaYTCadZHXo0LGqYeA4jkv2JnR89MFxHAKBABiGAcdx8Pv9MBgMPNEC4EnWYrHAZDLBbDbDaDTqJKtDh45VA51UdSw7SHTKMAyMRiMCgQBsNhuys7NhsVgAhEiX/JEiWRLJ6iSrQ4cOLUMnVR3LBo7jwDAMgsEgWJaF0WiE0+lEc3Mz/H4/AoEAsrKykJeXh7y8POTk5MBsNvOvpUm2t7cX5eXlyM7OFqSLdZLVoUOHlqDXVHUsC0i6t7W1FVlZWaisrMTIyAh6e3uxbt06lJeXIxAIYH5+HnNzc+jp6YHP5wsjWVJnXVhYQGlpKViWhdfrBQC+0UknWR06dGgFeqSqI+FgGAaBQAAsy6KtrQ2ZmZlwOBxYWFjAzp07kZuby9dUaQL0eDw8yc7NzcHv9yM7Oxt5eXk4c+YMNm/ejIKCAgDCSJb8MRqNYY1POsnq0KFjJaGTqo6EgeM4BINBBINBnuQ+/PBDLCwsIC8vD9u3b4fVagXLspKkKl6LJtmpqSkYDAbk5OTwkWx2djaMRiN/PF2PBSDZXayTrA4dOpYTevpXR0LAsiyCwSAYhgEQIrSBgQHY7XYUFhZiz549qsjMYDAgPT0d6enpWLt2LRwOB6qqqsCyLObn5zE2NgaGYQQkm5WVFVaTJTVdQuDidHEkYtehQ4cOtdBJVUdcoGdPOY6DwWCAz+dDa2srvF4vCgsLkZOTEzdxGQwGpKSkoKCgAOXl5eA4Di6XC3Nzc5ifn8fIyAg4jkNubi5yc3NlSTYYDCIQCOgkq0OHjmWBTqo6YgY9ewqEiG9mZgZtbW0oKirCnj170N3dDXGFIRbSEr/GYDAgMzMTmZmZqKioAMdxcDqdPMkODw/DYDDwBJubm4vMzExFJEvmZEm6WIcOHTqUQidVHTFBPHvKcRy6u7sxNjaGrVu3Yu3atQDACzyIEQuxRir/GwwGZGVl8Z3GLMvyJGuz2TA4OAij0Sgg2YyMDFmSBSCp9qSTrA4dOiJBJ1UdqiA1e+p2u9HS0gIAaGhoQEZGBn+8HKmqhVoSNhqNyM7ORnZ2Nl+LXVxcxNzcHGZmZtDf3w+TycTXY3Nzc5Genh5GsoFAAH6/n19TJ1kdOnREgk6qOhRDnO41Go04c+YMOjs7UVFRgY0bN4aRTKJIlbx/rDAajcjJyUFOTg7WrVsHlmXhcDj4zuK+vj6YzWaeZPPy8pCamiogWRKd053LOsnq0KGDhk6qOhSBjMGQ6JRhGHR2dmJmZga7du3CmjVrZF+bjEg1GkgqODc3F9XV1WAYBgsLC5ifn8fExAR6enpgtVrDSJaAJlmSLhaTrNls1puedOg4x6CTqo6IIOle0t1rNBrhcDjQ0tKC1NRUNDY2CshGDELA8SKREa8UTCYT8vPzkZ+fDyAkYDE/P4/5+XmMj4+ju7sbKSkpApJNSUnhX0+T7NzcHKanp1FTUxPW+KSTrA4dH23opKpDFlLdvadPn0ZfXx/Wr1+P9evXKyKJ1agvYjKZUFBQwCs4BYNBnmRHR0fR2dmJ9PR0vh6bl5cHq9UKIETIdrtdUJPVbe506Dg3oJOqjjBIzZ4GAgG0tbVhcXER+/btQ15enqK1EtmolExyNpvNKCwsRGFhIQAIdItPnz6Njo4OZGRkIC8vDyaTCQD4v8m+SQrd5/PpJKtDx0cUOqnqEICWGgRCZGa329Ha2orc3Fw0Njbydm1KoJVGpUTDYrFgzZo1fC3Z7/fzJDs5OQmfz4cPPviAj2Rzc3MFNneATrI6dHwUoZOqDh4kOqX1c/v7+zE8PIy6ujpUVFSovsjLkSqJgNWso2VYrVYUFRWhqKgIBQUF6O/vR1VVFebm5tDf3w+PxxPR5g4I/fx9Pl/EER6t/xx06DjXoZOqDj7de/r0aaxZswYWiwVerxetra3w+/04dOgQsrKyYlpbTAIcx/F12fT0dOTn5wts3qLtc7XAYDCguLgYxcXFAACfz8e770SzuaOdd8QkS5qezGazbg6gQ4cGoZPqOQ66GamtrQ0f//jHMTc3h/b2dhQXF2Pfvn1RyS4SDAYDH/mSuqzD4cCWLVsQDAYxNzeHrq4u+P1+WQcass5qgni/KSkpKCkpQUlJCQChzR05f2JzR85fimRpL1lCsroDjw4d2oFOqucwxFKDBoMBfX19mJmZwdatW1FaWhr3e5CL/Pz8PJqbm5GVlYX6+no+LVxaWsrbvJFIbmxsDCzLCkiWkMpqgJJ9pqWlIS0tTXD+hGTPnDmDYDCok6wOHasQOqmeg5CaPXW5XLwofUNDA9LT0xP2fk6nEydOnEBNTQ3WrVsHALxgAiC0eSsrKxM40MzNzWF4eFhgLZeXl4eMjIyPDGGIbe44joPb7ebNAeRs7qKRrNiBRydZHTqWH7pJ+TkG8ewpAJw5cwZdXV3gOA4HDhxAbm5uQt7L7/fjgw8+gNvtFqxL9qC0WYllWZw6dQpWq5VXPpKSFNQKYczMzGBoaAgHDhxIyHpim7u5uTlJmzty/jTJkj9Go1E3bNehYwWgR6rnEEh0SqQGg8EgOjo6YLfbsXv3brS2tibsImu329HS0sJL/cVD1CStWVBQgLKyMrAsi4WFBczNzfGSgpHUjpKBRJJVrDZ3pCZNG7YzDCM7wqOTrA4d8UMn1XMA4tlTo9GIhYUFtLS0ID09HY2NjUhJSUnITCnHcRgcHMTg4CA2btwIg8GA6enpRJwGvzej0ciTJ7AkKTg3NxemdkT+qJmtTdQ+lwuJsrmjSXZ+fh7p6el87ZboFuskq0OHOuik+hGH1Ozp8PAw+vv7+RonuXDSnbqxwOfzobW1FR6PBwcOHEBOTg7GxsaWXVBfLClIqx0NDQ2hvb0dmZmZAps3QjAfBai1ucvLy0NaWpqAZMfGxlBQUACLxSIwbKcjWZ1kdeiIjo/OlUWHALTUIEn3+v1+tLW1weVyYf/+/WEpWWI2HgtsNhtaW1uRl5eH3bt38xfsZCgqSakdkaanvr4+eL1eyRnRRCKZ5BOLzR3Lsrz4v9iwnSZZ2hxAt7nToSMcOql+BCHle0pILz8/Hw0NDZLp0FgiVY7jeNWlTZs2oby8XEAoyTIpp2G1WgVCDPT4Smdnp2B8JT8/H1lZWXERhtZ6/5TY3AFLKlek8UtckxWTrO4lq0NHOHRS/YhBPHvKcRx6e3sxMjKCzZs3o6ysTJag1EaqRHXJ5/PJqi5pUVBfPCNKxlfoGVlSj8zLy0NmZuZHKu0pZXN34sQJmM3miDZ3YpIlDjyAtKSiTrI6zkXopPoRAWk8CQaDfLrX4/GgpaUFDMOgvr4emZmZEddQQ1wzMzNoa2tDYWEh9uzZI1ujjLSmWv3f5YDBYEBGRgYyMjJQXl4u6KwlNVkSvZE/6enpSd93IkFqp6WlpSgqKlJscydHsnokq+Nchk6qHwFIpXsnJyfR0dGBtWvXoq6uTlHNUEn6l2VZ9PX1YWRkBFu2bEFZWZmi/cWLlbJ+k+qsFTf9iOuRaWlpgjW0cLOgFvSe1djcEZK1WCwCkiUZEyLyISZZ0l2sQ8dHDTqprnKIZ09ZlkVnZyempqawfft2vo6oBNHSvyTyDQaDiiJfsmaikIxapbjph2EYvulHbkZ2NSLSjUAkmzsl3dU0ydKG7VLdxTp0rHbopLpKQc+eEsUcp9OJlpYWmM1mNDQ0hEVQ0RApGpyenkZbWxuKi4uxefNmVd2y8Yzp0HvTAujRFAAIBoO8EAVJlaakpIDjOMzMzAh8VLUMNdE1bXMHhHdXR7K5I+8lR7J0d7FWfuc6dKiBTqqrELQOLhAinNHRUfT09GDdunXYsGFDTBGiVPqXZVn09vZidHQUW7duxdq1a1WvmShorasWCKVKxTOyQ0NDmJ6exuDgIFwuV0SC0QriSVmLu6uV2tyR9wUgSbK6l6yO1Qjtfbt1yILc4Y+PjyM/Px9msxnBYBDt7e2Yn5/Hnj17+It7LBCnf91uN1paWsCyLBoaGpCRkaF6Tano1+l0oqOjgyckUpeMdNFcLRdUi8WCrKwsOJ1O7NmzR5JgaPeZnJwcTTTwsCybsJ9xrDZ3gJBk/X6/rKSiTrI6tAqdVFcJ6HRvc3MzzjvvPD7dm5WVhcbGRlit1rjeg45UJycn0d7erqrRSW5NmlTHx8fR2dmJsrIymEwmTE1Nobe3l9cIJqblUtq9WoxUo0GKYAjJEos3sftMMkh2OZurYrW5I/sClkg20giPTrI6tACdVFcBpHxPR0ZGMDY2htraWlRVVSXkgmI0GsEwDDo7O3HmzBls27aNJ4NYQUiVrDs9PY2dO3ciNzcXLMsKxAjsdjtflyTdpeTPR+WCSQhGbPE2NzeHkZER3n2G3GCslMXdSnUsx2NzR/ZJ/jgcDnR1dWH37t286QJpftLNAXQkCzqpahhSs6c+nw8cx2FqaorX100UGIbByMgIUlJSEuapajAYwDAMjh8/Lmigov1UxWIEgUCAJ5qBgQF4PB6YzWakpaUhNzd3WWQFkwElM7K0eYCSNHmsSNYYkNTPgLa5E99o5Obm8tG8wWCA2+3myxa6YbsOLUAnVY1CavZ0dnaWt2fbsWNHQgl1YmICs7OzyM7OxsGDBxOWgrTZbPD7/SgrK0Ntba2idS0Wi6C71Ov1orOzE4FAgJcVJJEMkRXU0gUzVoKSm5G12+28Zq/FYgnzkU3mnhONaDZ3RIwjNzeXv8Ggu4flDNt1ktWxUtBJVYMg9SMSnXIch56eHoyOjmLLli3o6+tL2AWBYRh0dXVhamoK+fn5yM3NTQihMgyD7u5unDlzBmazGXV1dTGvlZqaioyMDJhMJmzYsCEsZQoAubm5fD32o6J4RM/I0mnyubk5Xk4wNTVVQLKx1tW1QqpiRLK5m5mZQTAYxNtvvx1mcxeNZMUzsjrJ6kgUdFLVEEi6NxAI8LOnpAMXAN+BOzAwkJDZT6fTiebmZj4tOzg4mJBmILfbjebmZhgMBuzcuRNtbW1xr0nb04nThSSaoxWPCMEmMppLNsRpciInKKV0RAhG6YysVklVDNrmLjs7G52dndi2bVtUmzsxybIsy5Os0WjUDdt1JAw6qWoEUuneiYkJdHR0oLy8HHV1dXwESZST4gHpwq2srOTTsvH6qQJLXcNlZWWoq6uDy+VaVus3g8HAX2CJ4pE4mktLS0uaYflyQiwnSCsdDQwMwO12C+ZDc3NzZWvRq4VUabAsC7PZrNrmjjjwiEnW5/PB6/XqJKsjLuikmmTQ6jLkwkY6ZWdmZrBz506+tkgQD6kGg0F0dXVhenoau3bt4qXnyLqxEiDLsujp6cH4+Liga1hOpUntBUrp8VLRHN34097erphoYkWyCEqsdETPyHZ3d/PzoSSKz87O5n/nJDOymiA1W6vE5o6Mb0llMsjPgmEYMAwjmJOlu4t1w3YdctBJNYmgZ0+BEHEsLi6iubkZKSkpaGxslExdxhpRkrWtVqvk2rGuS1LUHMeFdQ3Lkapa8o51b2azWaBbK0U04qan1UYucqBnZElN0W638xZ3DMPwBLQaQXoOIkHK5o448KixuSNd+LRhu1i3WCdZHYBOqkkDiU4JUZDZ097eXlRXV2PDhg0J8z3lOA5jY2Po7u6OKGNoNBoFoy5KQDSBS0pKsGnTprCob6XcZZRCTDREjIHMyJLxDRLNrdSc6HLDYDAgLS0NZWVlKCsrE4yuzM3NAQDef/99Ablo/dyVkKoYJpNJICsZq82d2LBdrFv8Ubkx06EeOqmuMOh0L7koBAIBtLe3w+FwYO/evfxdtRzUpH+DwSA6Ojpgs9mwe/duvv4mBTUESFvAbdu2DaWlpbLHatX6jRYiIERDOkttNhsGBgai2rxFWlvLoEdXSktL8eabb2L79u1YXFzkz12q4UdL5xULqYqRCJs7nWR10NBJdQUh1Yw0NzeHlpYW5OTkoKGhQdFIhFJSdTgcaG5uRlpaGhobGyWl/2goJS6v14vm5mZFFnD0xUdLF2QpSI1vkKYnUo9TMsKipchcCch+Sb21qqoqrOGHlpIkqfJon6flRiJIVYxYbe7kSBaQllTUSfajC51UVwhiqUEA6O/vx/DwMDZu3IjKykrFpBONVDmO411r1q9fj/Xr1ytaWwlZz8zMoLW1VbUFXLykmow0Mq1mBEiPsJALbH5+vmYdaKKB/Fzp349cww/pqu7q6uLTpIRc4tWeVovlIFUxYrW5E5Ms7cCjR7Ifbay+K8Aqg9Tsqc/nQ0tLC/x+Pw4ePIjs7GxVa0Zq2iGp5Pn5eUWpZPG6csTFsiz6+/tx+vRpbNmyBWVlZYrXTBSSHQFKjbCQeiztQGMymQTSklqHFKmKEWlGdnh4GE6nM6JR+XIgGT/fWG3uaJIldoocx/G9E3QkS7qLdaxO6KS6jJBK987MzKCtrQ1FRUXYu3dvTBceuUalhYUFNDc3IyMjIybXGjmy9nq9/E3AoUOHkJWVpWpNIH5C1OJFRnyBJU1P4+PjcDqdeOutt/g6XF5eHjIzMzV5HkpIVQy5GwwSwXm9XlkP1URBCzctsdrccRzHR6liL1maZOnuYh2rAzqpLhNIdEq++CzL8rJ9W7ZsUW32TUOcpuU4DqdPn0ZfXx82bNiA6urqmL6EUmRts9nQ0tKCwsLCmG4CpEjV7/ejo6MDwWBQVZdtsiPVaCAONAzDwG63Y8OGDfwIy0qK46tFLKQqhvgGw+v18iRLyIV2niEzsvHuO9mkKoZSm7tgMIisrCyeXAnoRkbasP1Xv/oVzjvvPBw6dCiJZ6dDCXRSTTBIk0J3dzeysrJQUlICl8uFlpYWGI3GhLi/0KTq9/v5zuF9+/bx9b9YQKd/OY7DwMAAhoaGsGnTJpSXl8csEk/WA4D5+Xk0NzcjKysLubm5gi5bQrBSTTBaIB81oLtrpcTxe3t7BfOR+fn5K16TJFgOckpNTUVpaamAXAjJjo2NgWXZuKN4LUSqkRDJ5u706dOYnp7G1NSUrM0dsESyv//977F27VqdVFcBdFJNIOhRGZfLBZPJxDd1VFRUYOPGjQm5CBBSJZ3D2dnZijuHlazr8/nQ2toKj8cTU82XBrlQsiyL06dPo7e3FzU1NSgvLwfDMHynKfFTpZtgSA2PiBNoPVKNBClxfBLBiD1kyTmvVNPTcndmS40u0TOyxHmGjuKVmCIQmcLVAlq32mazoaCgALm5uVFt7kwmE1wuFzIyMlS/5/j4OG6//Xa89NJLcLvdqKmpweOPP459+/bJvub111/Hbbfdho6ODlRUVOCHP/whvvKVrwiOeeSRR/Dggw9icnISO3fuxL/927/hwIEDqvf3UcTq+URqGFKzpwaDAZOTk/D5fGFygPHCYDDwd/yJNCk3GAwIBAJ49913kZeXh927d8d90SL76ujowMLCAh9NExk4ILzLlswK2u129Pf3w+PxICUlBSaTCfPz8wlJHS43ov0+xCIEtIcsXZMk0XtOTs6ynfNKjzuJ7d2I84zYFCHafLDWI9VIYBgGJpMpqs3db3/7W5w5cwZnzpzB9PS0qnOem5tDY2MjLrjgArz00ktYs2YN+vr6ImazhoaGcMkll+Daa6/Ff//3f+PYsWP4+te/jtLSUhw+fBgA8Pvf/x633XYbHnvsMRw8eBAPPfQQDh8+jJ6enjBJ1XMRBm413/5rAOJmJIPBAIfDgRMnTsBiseDgwYMJdUnx+/1477334PP5EmpSznEcWltbMTExgS1btqCioiIhF9rFxUW88847yM3Nxe7du/m0Lqk5K7lAeL1e9PX1weFwgGVZPnWoVdWjkZERLCwsYPv27TGvQUsKzs3N8R6y5JwT6SFL5Cs/9rGPJWS9eEHPB8/NzcHhcEhKCba3tyM7OxuVlZXJ3rJqnDhxAlVVVRFJiGVZfPjhh3jhhRfwb//2b7BYLMjIyMAnPvEJXHXVVfj85z8f8T2+//3v45133sFbb72leF+33347XnjhBbS3t/OPXXXVVZifn8fLL78MADh48CD279+Phx9+mN9nRUUFbrzxRnz/+99X/F4fVeiRahyQmj0dHh5Gf38/srKykJOTk1BCtdvtaGlp4QfUE0Wofr8fra2tcDgcSE9PT9hF6syZM+jo6AAA7NixI2axgNTUVOTk5MBgMGDr1q1wuVyw2+1hqkckXZxsUYJEIDU1FWvXrhXU4gjJDg8PC9Kl+fn5cTU9aU2YQ5y5kEqVp6eng2VZmEwmBAKBVec8RCLVSDAajThw4AD279+Pxx57DO+++y48Hg/++te/wmazRX2P5557DocPH8bll1+ON954A2VlZbjuuuvwjW98Q/Y1x48fx4UXXih47PDhw7jlllsAhK4VJ0+exJEjRwT7vPDCC3H8+PGoezoXoJNqDKAFtkk6xu/3o62tDU6nE/v27cP09DQfvSbi/UjTUF1dHYLBIBwOR0LWphWdNm/ejIGBgbjXJJ3OExMT2LlzJ5qamsKOIUPxakAu/uIGIClRAtpPNRl1t0SSFF2LI+nSxcVFgYeoxWKJWe1Ia6QqhlSqfH5+Hr29vbDZbJiYmFjxGdl4QW4IlIBhGHg8HuTl5WHXrl2or69X9LrBwUE8+uijuO2223DHHXfgxIkTuOmmm2C1WnHNNddIvmZycpLv4CYoLi6Gw+Hgm80YhpE8pru7W9G+PurQ9idPg5CaPbXb7WhtbUVeXh4aGxthsVgwOzurWpxeClJNQ6dPn47b95TjOD6qJnVZm80W97oejwfNzc3gOA719fV8w8lyzanSUc369esF9diBgQFe8YZEsauhHhsNdNOT2EN2bGws7MYimlm5lIWalkEyNaOjoygtLUVBQUFYPVpqNlRLUBKpEjidTgBQNR8OhH6v+/btw3333QcA2L17N9rb2/HYY4/JkqqO+KGTqgqIZ085jkNfXx9Onz4dNnZiMpnijlRnZ2fR2tqKgoICQdNQvCblgUAAbW1tcDgc2L9/P99dG4+fKrAkYVhSUoLNmzfz5JUoUlWyhli7la5NtrW1CUY58vPzl6Ueu9JtCrTa0YYNGwSi8LRZOd30JB7bWE2kSkC+h1IiHOTGisyGisdWkn1jRZeMosHtdgNARI1tKZSWlmLLli2CxzZv3ow//vGPsq8pKSnB1NSU4LGpqSlkZ2cjLS2NF6yQOoYIYJzr0ElVAWjfUzLTR1SGgsGgpMpQPMTHsiwGBgYwPDyMzZs3o6ysTHDRi9VbFFhSXcrMzAwbw4mV/DiO43WMt27dGiZskUz7N3FtktRj5+bmMDg4KKjHig2rVyvENxY+n48/566uLgQCAQHJrLZIlUCuE1YswEBmQ0lNNhEzsvGATAsojVRdLhdSUlJUp7QbGxvR09MjeKy3txdVVVWyr6mvr8eLL74oeOzVV1/lU85WqxV79+7FsWPHcNlllwEI/R6OHTuGG264QdX+PqrQSTUKWJZFMBgUpHunpqbQ3t6O0tJSSQ9RclwsxKdEEjCWiJLjON6vVU51KZY9k/S01+uV3e9KRqrR1ohUj+3u7kZaWlpC6rFaIqmUlJQwIQZCsiMjIzypjo6OIj8/X9GMqBagZLyErkeXl5cLxlZopSuaZJf7/Mm1RE36N5aMyq233oqGhgbcd999uOKKK/DBBx/g6NGjOHr0KH/MkSNHMD4+jt/85jcAgGuvvRYPP/wwvve97+Hv//7v8dprr+F//ud/8MILL/Cvue2223DNNddg3759OHDgAB566CG4XC589atfVbW/jyp0UpUBPXtK0mMsy6KrqwsTExPYtm1bxHSH0WhUnf4l6dNousBqyY8W2Y+kuqSWuObm5tDc3KxoplWLk1ty9ViSNqXrsWpmRbV4rgS0EAMhmdHRUYyMjGB2djasm1rL0Xssc6pS9n7ipq9YPXTV7BtQT6pqsX//fjz77LM4cuQI7rnnHlRXV+Ohhx7C1VdfzR8zMTGBkZER/v/V1dV44YUXcOutt+LnP/85ysvL8R//8R/8jCoAXHnllZiZmcGdd96JyclJ7Nq1Cy+//HJY89K5Cp1UJUCne4HQF9HpdKKlpQVms1mR1CARylYC2vBbiQOMGlIVe6pGUl1SSqq01rAS27p4a7Vq9hYPpOqxxIXmzJkzYBhm2euxKw2DwYDU1FSkpKRg9+7dktF7amqqIHrXyvhKIsQfpJq+iI8s8dAlM7LkZxCvchnDMLxwvhK43e6YP2uXXnopLr30Utnnn3jiibDHzj//fMmOfRo33HCDZLrX6/XC7/er3udywmq1ruiNoU6qIkjNno6NjaG7uxtVVVWoqalR9EVWSnwej4evzUYz/FazdiyeqkpqtcFgEG1tbVhYWBA0OUWDmBBjuUCsdAQo1q+l67FDQ0MwmUyrIqKLBrpRSRy9E4s3u93Om3TT7jO5ublJ66xdDkUl8julPXTJTcbIyAg6Ojp4OUklndVSINcWpd+BWCUKVxperxdr0zIxh8SMEiYKJSUlGBoaWrHvp06qZyE1exoMBtHR0YG5uTns2bOHn5NTAiXENz09jba2NpSUlMjWZmNZm+zbZrOp2ne0iHJxcRFNTU1IS0tTpTWcKAnFZEJpPTYvLy8ho1QriUjdv1IWb+TGgviHkqan/Pz8Fe2sXQmXGrPZLCsnSXdWq7nJUDNOA4D3qtU6/H4/5sDgNxkbkG7Qxtiam2Px5ckB+P1+nVRXElKzpwsLC2hpaUFGRgYaGhpUq/REGqlhWRY9PT0YHx/H1q1bUVpaqmrtSBElkZxLSUlBY2Ojqn2TFKvURXZ8fBydnZ2orq7mjZUTsV810FKtUiqiIxfbubk5BAIBfPjhhyui3Rsv1JCT1Wrl/UOjuc8sd4o8Gdq/FosFRUVFvLyglEk5PSMr9XtXS6ok/btakGE1I8OgjblgA8cArpV9z3OeVFmWhd/vF3xBh4aGeFGEdevWxexNKkUkbrcbzc3NAELt67F8WeQiSjL4v27dOtTU1KjeNzl/mlQZhkFXVxempqZiNgYQ74Oek83Ly0NBQUHUWl2yI9VoMJvNfD3WYrHA6XSisLBQMCtJKx5pqR4b65yqlPuMVGctnSJPVNMPy7Ka8FOVMikn5y+ekc3Pz0dmZqaqcRpg9aR/CYwmA4xGbXy2jezK7+OcJVWS7iXdvURqsLW1FW63GwcPHoxLW1eKVCcnJ9He3o61a9di06ZNMV8QxGszDIPOzk5MT09j9+7dfKpOLcTep+QGwGAwoKGhIeYLIt1k5HA40NTUhIyMDNTU1GBhYUFQqyMiBlJ3+FqKVKPBZDKF1WNJ0xMhG9o/Npn12ESJP0h11jocDtjtdkHTD930FGvTD/ksJJtUxSAzsmL/VFKT5TgO6enpCAQCikdlVkv6l8CUZoRJI78XE7vy14xzklTJRa6zsxPbt2+H0WjE7Ows2trawtSLYgU9UsMwDK+FG20UR+nahFSdTieam5thsVjQ2NgY18WZJtXp6Wm0trairKwMdXV1cV28CKmSFPL69euxbt06BAKBsDSa3W5HR0dHWGS3miAmKSmrM0I2Z86cQU9PD1+PJV6qK9lhu1ziD2T+kzSzkaYfu92O06dPo6OjI2bNXvL51xqp0pCbkR0eHobH48HJkycFZQQSyYt/F6stUjVYDDBoJFI16JHq8oNEp8FgEBMTE9i6dSsGBgYwMjIiqV4UK8hIDRnFMRqNikZxlICQKiGpyspK1NbWJmS8AAiproyPj2Pbtm2q671yOH36NBYWFvhIWhzF02k0utPWbrdjcHBQ4FFLrL9WK2iyEddj6flYQrLLXY9dKZlCcdOP3++PqNkb6bzJ50crKXQlIJF8bm4uOI7Dtm3bsLi4CLvdjqmpKfT29sJqtQpuMtLS0uB2u1V/D++++278+Mc/FjxWV1cnK3r/zDPP4L777kN/fz8CgQBqa2vx7W9/G1/60pcEa/7ud7/D6Ogor6z0k5/8BAcPHhSsZbIaYTJp42bHxOiR6rJBPHtK7og/+OADXvw9kSkWcjE4fvw4KioqsHHjxoRdGMkFpbu7Gzt37kyYMTCZL7PZbAn7eXg8HrjdbrAsqziFLNVpOzw8jDNnzvDWXyTCIZGd1gTT1YCuxwLC+VgStRP/2OWoxyZL+1dKs5eQ7Pj4OD8XTNLFtJwgia61HKnKgTQq0TOy1dXVAmOE8fFx/Pu//zuefvppvvwyPT2t6ru+detW/OUvf+H/HykLkJ+fjx/84AfYtGkTrFYrnn/+eXz1q19FUVERL/ywceNGPPzww1i/fj08Hg9+9rOf4VOf+hT6+/sFvRZGkwFGkzZudozQI9VlAZk9pVNGExMTAELOD1u2bEnoRZlhGF5zM5HRHhBKBZHB7IMHDybsRsBut/MNVLt3707IurOzs7xgRk1NTcw1WaPRiMzMTKSkpGD//v0IBAJ8FNvd3c1r2RLSWWkt10RDaj52OeuxWhHUF9cjpc6bkGxqaqom9hwL5BqVaGMEANiwYQO2b9+Ou+++G++88w5KS0uxdetWfOYzn8FPf/rTqO9jNpsVl5rOP/98wf9vvvlmPPnkk3j77bd5Uv3iF78oOOZf//Vf8etf/xqtra345Cc/yT9uMGoo/cvppJpQ0FKDpLuXYRh0dHRgenoaQOiDm0hCJTVOcleYyHog3ejkdDoTkgLlOA5DQ0MYGBhAXV0dOjs747775zgOg4ODGBwcxObNmzE2Nhb3BZB+vcVi4SMcWsvWbrdjeHhY0HGajCagRJJUpHosaf6Jtx6rFVKlIXXedKp0YWEBHMehq6uLT5eulpKA0pGavLw8XHHFFXjiiSdwzTXX4LLLLsPrr7+O0dFRRe/T19eHtWvXIjU1FfX19bj//vtRWVkZ9XUcx+G1115DT08PHnjgAclj/H4/jh49ipycHOzcuVPwnMlq0kzmyJQEHYqPLKlKzZ4SwrNarWhoaMA777yTMCNxYGmkpaqqChs2bMCrr76akPlMYvp95swZbN++HWvWrOGF0OMBPdpy4MAB5OTkoLu7OyG2couLi/ya4+PjyyZTKNaylSMdQrCrwcA6EqSaf2j/WLfbzdclldZjtUiqYohTpcTKz2Kx8CUBWukoWeb0SsAwjKobH7fbjczMTBQWFuLzn/+8otccPHgQTzzxBOrq6jAxMYEf//jH+NjHPsZ32UthYWEBZWVl8Pl8MJlM+Pd//3dcdNFFgmOef/55XHXVVXyd99VXXw2bNjAYNBSp6o1KiYGU1CCR7Fu3bh02bNgAo9GYEM9TIHRh6+zsxOzsrGCWMxZRfTHouVbS6ETIJR7yk7OAi0enlygupaeno76+nl9zJa3fpEiHpBDpZhhCslrw1owHYsUj2uZNXI+VsznTwrxnLCBlBSBc6Ujc7KUlo3KGYVRlT2Lp/r344ov5f+/YsQMHDx5EVVUV/ud//gdf+9rXJF+TlZWF5uZmOJ1OHDt2DLfddhvWr18vSA1fcMEFaG5uxuzsLH71q1/hiiuuwPvvvy+o9RqtBpjM2vg8GY3xBzVq8ZEiVSmpwUAggI6ODszPz2Pv3r2CdGwiSJVWMGpoaBB8WdSI6kthamoKbW1tYXOtpEEjlrU5juO1jKU0gWMlwDNnzqCjo0NScSmZ1m/iJiDa9oyk0ejRnUR0Z5P9JgNimzepuqQ4Nb4aIlUxxGpKYqUj0uxF31yI5RSTdc5qFJXI71AuulSK3NxcbNy4Ef39/bLHGI1G/iZl165d6Orqwv333y8gVTJfXlNTg0OHDqG2tha//vWvceTIEf4Yg9EIg0Zu0pKxj48MqUqle+fn59HS0oKsrCxJh5Z4jMRpwXo5BaNY12dZFr29vRgdHZVtdIpF+o/Uk2dnZ2U1gdWSF0lNT0xMRFRc0opwQ1paGsrKyngFIFKnm56eRl9fHy9OQCK7WOZFtXKuSuqxqampMBqNSE1NRSAQ0IwDTTREkygUN3sREQa73c5bndGp4pX0kE2GTKHT6cTAwIBgRCYaWJaFz+dTfYymGpWSsI+PBKmS2VP6izYwMIChoSHU1taiqqpK8gsTa6QaDAbR3t4eVWg/FlL1eDxobm7mR1Dkvkxq07Ska9hisYRF1LHu2ev1oqmpiR9JkovytGr9ZjAYkJ2djezsbN72S+zIorY+qWXI1WMHBwfhcDjw1ltvrZrzVaP7KyXCQG6miIeqxWIRZCyWs+lJrWaxy+VS3Y3/ne98B5/97GdRVVWFM2fO4K677oLJZMIXvvAFAMCXv/xllJWV4f777wcA3H///di3bx82bNgAn8+HF198Ef/1X/+FRx99lN/DT37yE/zN3/wNSktLMTs7i0ceeQTj4+O4/PLLBe9tshg1k/41qRT2VzvfK4VVTari2VOj0Qifz4fW1lZ4vV4cPHgQ2dnZsq+PhVSJ0D5xaon05VO7PjEpLy4uxubNmyPezaohP9I1XF5eHnVeVil52Ww2tLS0YM2aNYpGkrQSvUWCyWQSiBOQ+iSZF6XnJkmqeLWlTWmQeqzNZoPZbEZ5ebnqemyyEI+YvtTNFD0f2tXVhfT09GVTuFITqQYCAfh8PtWkOjY2hi984Quw2WxYs2YNzjvvPLz33nt8JmlkZETw83O5XLjuuuswNjaGtLQ0bNq0Cb/97W9x5ZVXAgh9N7q7u/Hkk09idnYWBQUF2L9/P9566y1s3bpV8N4Gg4bSvzG45aiZ75XCqiVV8eypwWDAzMwM2trasGbNGuzZsyfqD0MN6XEch5GREfT29ir2J1VKfLRJ+datW7F27dqor1GyNu2Go1QeMVpamR7B2bRpEyoqKhStqcVINRqk6pN2ux02mw0DAwN8dENIli4vaIV8lIAIKYjP1+12C/xjl0scP9Y9JyqKFs+HBgIBzM/PY25uDoODg3xNk47g42l6UkOqTqcTAFTXVH/3u99FfP71118X/P/ee+/FvffeK3t8amoqnnnmGUXvvdrTv2rmeyVfH/MrkwSp2VOO49Dd3Y2xsTHFpAQoJ71AIID29nbJZqd41/d6vWhpaUEgEFClYhRtba/Xi+bmZjAMo8oNJ1KqljYoJ+MySpAoQkxmtCtWeSLRjd1uF6g85efnw+PxxCwUnwxINSrRKVO6Hjs3Nyeox9Li+CtZj13OjmWLxSJobqN1qbu6uuD3+3mxkby8PNUd5GpI1e12A8Cq0v41WYwwWbQRqZoQ2ofD4RA8npKSIptljHW+l2BVkapUM5Lb7UZLSwsARKxBSkFJpEqanTIzMyWbnSIhGvHNzs6itbUVhYWF2Lt3r6o0Q6SIkqyrNDUrXleKvGI1KCdrisGyLF8rUhLVaS3yE0c3RMeWpIsZhoHb7V4VKk9Kun/pemx1dbVgPlbsMkR0e5dzhGUlvVTFutRETpE0PXEcJ2h6iiYjqcb6zeVyIS0tTTPjQEqgxe5fcUbtrrvuwt133x12fCzzvWKsGlIVz54aDAZeUL68vDwmJ5VIpMpxHIaHh9Hf34+ampqYfFXl1uc4Dv39/RgeHsbmzZtRXl6ual1AmrDFSkaJWndiYgLt7e0x+7SKidrj8eDUqVNYXFyExWLhiSdag4iW67K0jq3JZEIwGERWVhbm5uZ4lSel57nSiGWkJtJ8bGdnJ1+PpX1EE3lTkQyDckDeQ5YuC5jN5jDnGRr0/Hw0KLWH0xKMZiOMGmlUMnJLOgV0f43c9y+W+V4xNE+qUrOnxD90ZmYmZuNsQF6cwe/386pA+/fv57slY1lfTFA+nw8tLS3w+Xw4dOhQzPNn4jSt2As2UoNWJNAESNdk4xHup9ckesAlJSXYtWsXX6ckDSIZGRnIz89HQUGBINpZTRcVg8EAi8WCioqKsFGW8fFxdHd3a0rlKRFzqpHqsWLpyETUY5NFqmLQHrJVVVVgWZZveqLT5LQIBcdxqiLV1ZT6BbRZUyWNaWqhZL5XDE2TqlS61+FwoKWlBampqXH7h5pMJt6ZhWBubg4tLS3Izs5GY2NjXHUiManabDa0trYiPz9fUSOV0rXn5+fR3NyM7Oxs1NfXx7VnQoB0TTZeyzqSqh4cHMTAwAA2b96MtWvXwu/3CyzQiCqOzWZDV1cXAoEA331qtVoTIvm4EhCTlNjqjVb/ISpPRJigoKBgxYUJEi3+IFWPJSMsiarHaoVUxaD9UYGlsSXS7OVyuQCErBALCgqiOiwRUl1VN5VGg4bSv/H93GKZ79UsqUrNnp4+fRp9fX2Ku2+jgVY8olOnGzduRGVlZdzrk0iYXruurg4VFRUJW3tkZAQ9PT0xp6il1l1cXER3dzcKCwuxdevWuOs5HMdhcnISDMPwDU5SBEmr4tDRDol4WJZFZ2enZLftaoJY/Yc2BKBVnsh5LneX7XIrKol1e8VEE0s9VqukKoY4Te5wOPDhhx/yTlY+n4+XzczLy0N2dnbYqMtqi1SNZu00KpH0r1JEm+9VAs2RKpk9PXPmDIaGhnDo0CGBSPu+ffv4u8B4QWqeZLbV7Xar6mpVsn4gEMDJkyfjTstKYWxsDD6fT1VHciTQji+bN29OCPm7XC5MT0/zohNKiVAc7ZCGsZSUFL7bllyItShUoObnJqfyRIyrl7vLdqVlCqXqsaTxh9Rjo1n5qWn20RLMZjOMRiM2b94MQCibOTY2BpZlkZubi5ycHExOTsLhcMRtw/jTn/4UR44cwc0334yHHnpI9rj5+Xn84Ac/wDPPPAO73Y6qqio89NBD+MxnPsMfMz4+jttvvx0vvfQS3G43ampq8Pjjj2Pfvn38MVpsVFKKaPO9SqApUmVZFsFgkC/kB4NB2O12tLa2Ijc3N+50rBgmkwkejwfvvvsu8vLy0NDQkND1/X4/ZmZmUFhYGHdalobT6cTc3BzvtpOIphcyLuP1elFVVaWqhVwO09PTaG1tRVpaGgoKCuKKLImp84YNG7Bhwwb4/f6PpDCDWJiA7rIdHBzkheLJeYojm1iQbO1fcXetXD2WRO9paWmrJlIVQzxOI76hIj0G7e3t+MpXvgIgVNc7evQoLrzwQqxfv17V+504cQK//OUvsWPHjojH+f1+XHTRRSgqKsLTTz+NsrIynD59WtBPMjc3h8bGRlxwwQV46aWXsGbNGvT19YUFOZpqVGLV7SPafK8SaIJU6dlT8gU3m83wer04depUwlKm4vecnZ2Fw+HAli1bEro+6RyemppCTk4Odu3albC1SSduamoq1q5dmxBCdTqdaGpqQmpqKgoKChJiek26m7dv3465ubm49yjuILZarYILsZQwA2l4SsYMZaIgjuqIULzdbkdbWxtYlg0zBFD7WdOSS02keuzk5CQfuXMcx/dcrBa9YiBy5694Fnp0dBQ33XQTWltb8dvf/hY33HADysrKwlxh5OB0OnH11VfjV7/6VURhBwD4z//8T9jtdrz77rv8z3PdunWCYx544AFUVFTg8ccf5x+rrq4OP49VHKkmAkknVbHUoMFggNfrRVdXFxiGQWNjY9wODWJ4vV60trbyc5KJiMoI6M7h8vJyMAyTEEKlPVV37tyJqamphFy8Jycn0dbWhqqqKtTW1qKlpSWudQOBAP+zJd3N8/Pzsl6oiYiSpIQZpDR8ExndJQtioXgyzjE7Oyu4mSDpYiXZgWRHqpEgV4/t7e3F3Nwc3nrrrRWdj40XaoQfSEd1eno6/uM//gMulwvvvvuu4lTk9ddfj0suuQQXXnhhVFJ97rnnUF9fj+uvvx5//vOfsWbNGnzxi1/E7bffzu/3ueeew+HDh3H55ZfjjTfeQFlZGa677jp84xvfEKylxe7flURSSVVq9nRqagrt7e0oKCjA4uJiwgmVFlyorKxU1SodDaQLNysrCw0NDRgfH8fCwkLc6xKRfY7j+E7cmZmZuLphiRPO2NgYduzYgeLiYgCxud8QEIGIjIwMQbpbbk01F3M1qkyRNHzp6I5OFScaK0FS4nEOWuXp9OnT6Ojo4FWeIsnraZlUxSCR+9jYGNasWYPCwkKB2lEgEIhaj00m1DrU0GL6GRkZYabhcvjd736HU6dO4cSJE4qOHxwcxGuvvYarr74aL774Ivr7+3HdddchEAjgrrvu4o959NFHcdttt+GOO+7AiRMncNNNN8FqteKaa67h1zKYTDBo5MbGYDpH/FSlZk/JTOSZM2ewdetW5OfnY3JyMmG1E5Zl0d/fj9OnT2Pz5s0oKyvD3NxcQkzKOY7jO5PpLtx4rOUIiMh+SUkJNm3axH8h41nb5/OhubmZl0akuwtjdZQhaWkpP1UgucIN4hlKp9MJm83G272RRiAS7SRzZjQeSKk8kZsJ8YhSfn4+P6pBtH9XE8h1QaoeS0hWrh6bTMRCqmqbEEdHR3HzzTfj1VdfVVzKYVkWRUVFOHr0KEwmE/bu3Yvx8XE8+OCDPKmyLIt9+/bhvvvuAwDs3r0b7e3teOyxx4SkatBQpJqEz3VSrh4Gg4EfNTEajXC5XGhpaYHRaOQjMZIOVqM+Igc5fd1EmJQTXeCFhYWwzmQ5cQkloOuSW7ZsQVlZmeD5WCPKubk5NDc3Iz8/X1IaMRY/VRLxyglEaMn6jY7uxI1AAwMD8Hg8YaniWGqUWoC47kyPKA0NDcFkMiEvLw9+v5//vq0WSN1siy3e6Hos6aROhF9uvPuONVJVipMnT2J6ehp79uzhH2MYBm+++SYefvhh+Hy+sD2UlpbCYrEIHt+8eTMmJyfh9/thtVpRWlqKLVu2CF63efNm/PGPfxQ8ptdUkwRyoSVKOpWVlaitreW/KOSXyzCMqg/+XzJ34EJnK///6elptLW1SdqpxUN6QMgGrrm5GRkZGZLjIvQcrBr4/X60tLTA4/HIqi6R7miloKPpSHO4aiJgog7l9/ujivZrhWjEkGoEEs+M0vKC8TZxJQtSDUAkVRwMBtHV1YWRkRGBypOWa5NKMlhK52MT5T6jBGqDBOKQowaf/OQn0dbWJnjsq1/9KjZt2iSokdJobGzEU089Jfi59vb2orS0lL+uNTY2oqenR/C63t5eVFVVCR4zmk0wmrXx2TEy50j6FwiNcLS2tsJut2P37t38RY3AYDDwGqpq8ZfMHfj/OZrR29uL0dFRWeeaWEmP4ziMjo6ip6cnohBFLClaEknm5uaioaFBNhWpJvqjTdWjzfkqjQhJ/Tg3NzeqOtRqsn4jXdVr167lZ0ZtNhuvBKRUXlDr6VQ6LTo9PY0NGzYAAOx2Oy9KQNcmV1rlKRpi6ViONB8rrscS95lEn7Pa9K/b7VYt/pCVlYVt27YJHsvIyEBBQQH/uNik/Fvf+hYefvhh3HzzzbjxxhvR19eH++67DzfddBO/xq233oqGhgbcd999uOKKK/DBBx/g6NGjOHr0qOC9DMbkNAhJIQY71biRNFLt7e1FIBBAY2Oj7FhIPOnZ17J3Ie2VRyM615hMJn6cR+kXlCaoaKILakiVjiRra2tRVVUV8QutdG2n04nm5mbFM61K0spjY2Po6upSrOIk9byWLtByoGdGSaRDLsK0vCAZ3SFNMVqNyuXAcRzvDUvS9/Ss6MjICADwZKOF2mQiei2i1WMNBkPCzzmeRqVEQmxSXlFRgVdeeQW33norduzYgbKyMtx88824/fbb+WP279+PZ599FkeOHME999yD6upqPPTQQ7j66qsFaxvMJhg0EqkamJXfR9JIdfPmzVEbJGIh1e0Dr6JtQ6hD7tChQxE/wHSKWckXVGx/Fo2glO6fCC/Mz88rVoxSQqqTk5Nob29HRUWFILUebV25PROZQFKvId210SBH1GqIVQtkZTabBT6b5CJss9lw+vRp3onG6/VqOnUqhlT3L3FiEdcm6VnRZDZ3JVr8YaXqsWprqsSlJl6ITcnF/weA+vp6vPfeexHXufTSS3HppZdGPMZgMGjmpvmcaVQClqLEaMcoTf/Sc5wEf83ZLaivikG+lNHqtnTtV667VW79aMRHE7Uav9ZIa7Msi76+PoyOjmLbtm2qXOzlyMvr9aKpqQlA6Mun5q5d6mflcrkwNDSErKwsFBQUKBax0NL4B20BRjvR2O123qlkNdQoo6VS5WqT4uYu2ollueeAl1tRSeqcSQ16eHg45noswzCKb0CIqEmixwqXG3qjkoahNNIj3cNAyKj8Xeq5Ewfqsf+D45KvI7OxkYgvGAyis7MTs7OzkrXfSIhGqsQPNhafUrl9081Dhw4dUp06klrXbrejubkZRUVFYc1eStekiZrIF+bn5/NWaPQ8ZW5urmRnp5ZBO9F4vV5YrVZkZ2fDbreju7ubr9cVFBQIxlm0ALU3KpGau8bHx8PmgNPS0hJ+ristU2g2mwWzz2RcaW5uTlCPJectV4+Npaa6HHPUywmDSUPp3+A5lP5V8iUzm81RSXViYgIdHR0oKytLuFE5qUcSMXi1nZ9yqVSGYdDV1YWpqamY/WClGpVIk1NeXl7M1nL0unSdd9OmTSgvL4/p4khIleM4DAwMYGhoCFu3buUvULTlW2dnJ6/jSwiIjoq1FKlGgslkknXcIeMsdFdxMh134v2Zipu7iMrTzMwM+vr6YLVaBeeaiDGWZM/WiseVaKchUoOm/WMJMcZSU11tkSo0pKiEc01RKRoipX8ZhkF3dzcmJiawfft2XhEIAC50tuIvmUsC0pGiVTlSJVFkVVUVampqYrorluoudrvdaG5uhsFgQENDQ8zND3QUzHEcRkZG0Nvbq6jJKRIIATIMg/b2dtjt9riM2smaDMOgqakJi4uLOHjwILKysngvW6vViuLiYhQXF/MpL5vNxl+UU1JS+PcPBoOrzvIt0jgLcdwhkToxZ1/JKCyRNypSKk8kVUxUnuJ1FyI3aFpJpxsMBska9NzcXFg91uPxKO4N8Pv9CAQCq45UtaWodA5FqkogR3hOp5MXi2hsbIxITAudLuRsyUDXZy/E5v/vL1HfIxFRJAGJ+shFi6Q9165di02bNsV14SSkGgwG0dHRAbvdnhBbPIPBgEAggPfeew9mszkhLjh+vx/z8/PIzc1FfX09rFar7IWF1vElF+W5uTnMzMwAAN555x3JjtvVBHqcRcpxJxgMhskoLuc5Lmf0LyUZSTpsyblKqTxFArmZ1Orvna7Hrlu3TnBj4fV60dfXh4mJiajykcTQfDm6f5cTek01SYg1/XvmzBl0dHSEiUVEg3PKJfk4HfG5XC40NzfDZDLFFUXSawOh6GpwcBAjIyPYtm0bSktL41qXrB0MBvHee+/x6elEONaQNGVlZWVM6XQxZmZmMDg4CKvVir1798aUni8sLEROTg4mJiawf/9+LCwshHXcklSxFqJYtSSl1HEnkenTePYbD8RjLORc5+bmMDg4CLPZLBhjkfpMk+/rajFFoG8siE8pgKj1WKfTCQBx1VSVeqn+4Q9/wI9+9CMMDw+jtrYWDzzwgMBHleM43HXXXfjVr36F+fl5NDY24tFHH0VtbW3YWrqgvoZBR5FE9WV6ejrmCFIqWiXvQbRrKyoqsHHjxoR8Ycnd58mTJ8MkEuPF/Pw83G431q1bl5D9chyHwcFBjI2NISMjgzdRjne9wcFBlJWVweVyxbVHctFPTU1FZmamoOPWZrMJ0qiEYLVmXK4EUo47YpH8RDru0JmUlYb4XOm0OJmFzsjICOugXm2kSoNhGKSkpCAvLy+sHkvPBP+///f/kJ2djdTU1Jh/N0q9VN9991184QtfwP33349LL70UTz31FC677DKcOnWKF4v4p3/6J/ziF7/Ak08+ierqavzoRz/C4cOH0dnZGdZrYjAZNZT+PYciVSUwmUzw+XxYXFxES0sLLBYLGhsbFTUMieuqBGJiNRqNGBkZgcPhELi1JALz8/MAQpHIvn37EjLLx3Ec+vr6MDw8DIvFgk2bNsW9JlG3WlxcxPr162G32+Ner62tDQsLCzh48CCcTid/151I0B23csbl5O6/oKAg6YIFsUAskp9oxx2ShtdCKlWcFqcb2IjKU25u7qqrMdIQNyrJ1WP/93//Fy+//DK8Xi9qampw4YUX4sILL8SnPvUpRSUeNV6qP//5z/HpT38a3/3udwEA//iP/4hXX30VDz/8MB577DFwHIeHHnoIP/zhD/G5z30OAPCb3/wGxcXF+NOf/oSrrrpKuKDRGPqjBZxL6V8lMJlMcDgceO+997Bu3Tps2LAhprtTUlclOP2Vz6HqiT/D7XZjcXGRrx0mqnWdmJQTW7m6urqEECrRBPZ6vdi+fTu6u7vjXpMYlKelpaG+vh6zs7Ow2Wwxr+dyudDU1MQrOFmtVrhcroTIFAKRNYTFaVQ5N5qCgoKIEoNahpTjDt1pm5KSwkfqSkQZtESqYlgsFkEHNYno6Po6fUOxGnSZo3X/knrsvffei4suugjXX389Hn30UfzlL3/B/fffj/z8fEX2b2q8VI8fP47bbrtN8Njhw4fxpz/9CQAwNDSEyclJXHjhhfzzOTk5OHjwII4fPx5Gqrr4Q5IQ7WSDwSCmpqawuLiIvXv3qpoPlYNzyoXM4hC5Tk1Noa2tDRaLBeXl5Qkj1EAggLa2NjgcDhw4cAAffPBBQpSAiNZuTk4O6uvr4Xa747aVExuUky9DrOvOzMygpaUlbLwpGWpIcm40NptNIDFICCiRDU8rlU6V67QltVgljjtaJlUadESXk5ODU6dOYceOHbDb7WG6zCTa1dpNE5FEVdq17HQ6kZWVhU9/+tP49Kc/rfh91HqpTk5OhmXoiouLMTk5yT9PHpM7hoam5lTPte5fuYutw+Hgx05ycnISQqg03DYXcPs3se2nv8T09HTC1iX7Tk9P56O0eO3laPH+RHm1chzHmw2IU96x2LRxHIehoSEMDAxI2tSRY8RQQ7ZKItVIEAsW0HOjw8PDmpobjRXiTlspxx2xKMNqIVUaZJxGrHhEuor7+/vh9XoFNxRZWVlJr8GS76tSUo1F9zcWL9WEI6Son5z3FiMJ+9DUrRxNINXV1cjIyMDQ0NCyvZ/v+/8A03fvS4hROWmsELvWxEN+DMOgo6MDs7OzYeL9sa5Lp5ClFJfURpXEYGB+fh4HDhxATk5O2DGJ8FNNNMR1LNIgMzIygs7OTn6WsqCgYEVk95YDco47RL83LS2N/33Fm/VYSUhpdYt1mT0eD0+y5IaCHt1ZDpUnJfsGlDdYuVwu1bq/sXiplpSUYGpqSvDY1NQUL29K/p6amhJMLpCxQzH0OVWNIBAIoKOjA3Nzc7xY++zsbFyERzcrieuqBEX/eT9mv/6DmN+DYRh0dnZiZmZGUsYwVvJzu91oamrix3vCOuwolSKlFweHw4GmpiZkZ2ejvr5eMj2mJv3rdrtx6tQpWCwW1NfXRxzpWYmaaqwQN8hINQPRUayShietRX5yjjtS879atHqjocT2LS0tDWlpaYIbCrvdztfXk2FWvhKkGouXan19PY4dO4ZbbrmFf+zVV19FfX09AKC6uholJSU4duwYT6IOhwPvv/8+vvWtb4VvwmhIipKRJM61kRpCDGKzb3Jxjjd1KgW6rkoQ63uI51ql0i2xnAMRiYgkvUgeU0qqRCEqkv8rWVcJcc3OzqKlpQWlpaVRhSy0enGWg7gZSOxWQnuq5uXlhV2otBaVS4FEdllZWZicnMTBgwf5yI7M/+bl5fE150TMQCcKanV/6RsKsRgDbVYej8qTEpAmJaXfh1jSv7F4qd588834xCc+gX/5l3/BJZdcgt/97nf48MMPeZ9Ug8GAW265Bffeey9qa2v5kZq1a9fisssuC9vDR0n8QemcL42kkirpku3r68OGDRtQXV0t+MDFS6per1f2ObfNhfSCELkWP/5T4F9+q2ptYqtWXl4ecU5Uradqf38/hoeHZY3V6XWB6BcY4t4zMTGhyBAgWqRKdzZv3rwZ5eXlUc5KXfQbaQ3y/isJ8QWZrt319vbyYx60UP5qArkpEzvukFQxMTxIT0/XjONOvGL6UipP4lEskirOy8tLmPmBWtu3lfJSbWhowFNPPYUf/vCHuOOOO1BbW4s//elPAnL+3ve+B5fLhW9+85uYn5/Heeedh5dfflm6bmsyAhpJ/yKOOVWlc75iJJVUW1tbYbPZZOX1zGazYus3MWZmZtDa2oraz1ej72n5uqzbtoj0giw4/vFaZP/osajrsiyL3t5ejI2NKbJVU0qqpNbp8Xhw6NChqLN4NKnKwefzobm5GcFgEPX19Yo6nCPVVGk9YLn6qdyacvszm82qLjTJjgLp2h0Z87DZbLDb7bwikNFohMlkQiAQWJG0YjyQynTQMnvr16/n50Xtdjt6enrg9/uT6riTaIcacWZCrGhFVJ7ibWJT6ttMEEv6VwpKvFQvv/xyXH755bJrGAwG3HPPPbjnnnuiv6HBEPqjBcS4DzVzvmIklVTLy8tRV1cn+yElnqtqvkR0tLdlyxaUPfFn9D0dua4KAL55Z1Ri9Xq9aGlp4dWRlHzglZDqwsICmpqakJOTg4aGBkWjANFIlTjWFBQUYOvWrYqJSy79S2q8segBi4mauNX09/cLum4LCgoidixqLY1MR3hEKH9+fh49PT2w2+146623+A7UgoICTXSgiqGkfCA3L5osx53ldKiJpGglNj/Iy8tTFbXH4lBTVFQU66kkDQaTGQaTNtp1yD4cDofg8ZSUlIjXMDVzvmIk9cwLCwsjpnfJB1DpHZ7X60Vrayt8Pp8g2hNHq1J1VQI5YrXZbGhpaUFhYSH27dun+MsRLYU9OjqK7u5uwbiMEpDjxKRKd1Bv3LgRlZWVqn1axaRqs9nQ3NysqH4abU2ituRwOLB//35wHAe73c53pJI0IxFoEL9XsiPVSCA6xBkZGcjLy0NRUREf8YyPj4PjOEGdUgtiBWoJSkoBSI505LxxE7HnlUo/ixWt/H4/H7XTPrnkmEjzzrGQ6moT0wcQig610qh09ndRUVEhePiuu+7C3XffLfkStXO+YmjjdkIG5AMYDAajptEI6RUUFEh6iUqlgem6qhxoDdtYPEXlIlW6a5h0O6uFeG16TfEITixr0n6qSuunUiCkSqJdi8WCQ4cO8Y/T9UpCQkRonCYhrUWq0SDV8GSz2QRiBeTcklWnjFeoQspxh5BOZ2fnsjjuJNNLVWxT6Ha7+fMdHh7mb6zIOdM3TucKqRoMRhg0MqdK9jE6Oors7Gz+cbkoNRFzvpomVYPBEDXSo42vo5Fe7eerMd0ZLvZA6qoE3l98B6k3/TP8fj/a2trgdDpV1RBpSJGq2FM11l8enar1eDxoamqKe83l9FM9fvw4H+0Sizn6om42mwVpRlLbIhJ8RGSiuLhYsutWSxB/BsUjLXSdkkQ8pDmmoKBg2e3eCBKt/iTljZtox51E11RjBe2TS6J2h8MBu92OM2fOCFSe8vPzEQwGVX1m3W73qmt8AxBqUtLKd/PsPsh3LxpimfMVI+kjNdEQiVR9Ph9aW1vh8Xhw8OBB2R9a1RN/xumvfC7qe/nmnUjJDd0Zen/xHby341JkZ2ejoaEh5oYTo9Eo2D9poIo1lSpem2VZPj1bUlKCzZs3x+0Gw7Is3n//fRiNxrgt5TiOw+TkJILBILZu3cqnYaKlccW1rWAwiLfffhsA0NvbK2iWWUkSShTEdUqi8GSz2TA4OMiTT0FBwbLOUS6npKISx51YRlm0Qqpi0AYPpMGLjO709fXB4/HAYrFgaGhIVjaSxmqNVFdzo1Isc75iaDpSBUKkKtUBbLfb0dLSgry8POzevVuxzmfRliLJaJWGf8EJa04mGodeh/krd8V10SFWVXREHW1cRikMBgPGx8cxPj4eV3qWxsLCAoCQYHa8BM2yLDo7OzE1NQWTyRRW11AD0lVbVVWFjIyMsK5bi8XCE2yydV/V1n3piKeiooInH5vNhqGhIZ58SKo42sVY7V5X6mYkkuNOe3t7mOOOnOqRVklVDIvFIlB56unp4R2b5GQjCUiUvypJdRW71CiZ842GVUGqdKRH68zG0ogDAOs+tgGzvSEh6Eh1VWbRCTzxY1i+enfM+zcajfD7/Th16hRcLpeicRklCAaDCAaDmJycjDk1TYPUT3t7ewEgbkL1+XxoamoCy7LYtWsXTp06Fdf+aIi7bulhfiIkT5pHCgoKEiqWvxIQk4+Uhi/dMR1vJiFZPxsljjtSqkerhVTFMBqNyMrKwsaNGyVFRcj5Op1OVFZWwuVyqbpWPProo3j00UcxPDwMANi6dSvuvPNOXHzxxZLHP/PMM7jvvvvQ39+PQCCA2tpafPvb38aXvvQl/pi7774bv/vd7zA6Ogqr1Yq9e/fiJz/5CQ4ePCi/EQ2mf1cSmk//ms1mnlQTUeMkKNxYwhMrsFRXpVPABIHH746ZWAOBACYnJ1FQUID6+vqEpPGIvRrHcdi0aVPchEo0hm02G3bv3o2TJ0/GdeEiI0J5eXnYtm0bvF5vQrp25WZo6WH+2tpafuTDZrPh9OnTPEmRSG8lZkcTSVRiDV9izE7qdnTHdE5Ojuq5Xy0QlJzjDhnboc3ZvV5vwlylVhIMw/CfPSlREXK+P/vZz/DCCy8AAJ544gmwLItDhw5FHVUqLy/HT3/6U9TW1oLjODz55JP43Oc+h6amJmzdujXs+Pz8fPzgBz/Apk2bYLVa8fzzz+OrX/0qioqKcPjwYQDAxo0b8fDDD2P9+vXweDz42c9+hk996lPo7+/nI/AwGAwaEtSP/3soNdcb8S25JM4osCyLQCAQ8ZgTJ06gtLQUmZmZaG5uRnZ2NrZv3x7ThZGuq6YXZCElNxPjH/TzkSppViKkas3JhCkr9G9TZhaMl39b1fuNjY2ho6MDmZmZaGhoSMiFlkgYlpeXY3Z2Fhs3boxrlo1ucCJp9L/85S/45Cc/GdPP+MyZM+jo6BCMCLndbrz11lv8FxUIXcwDgYAq8n7rrbewa9cuVXfvZOSDpIrJ3T8h4eXQuCWjV1JuPYkGbeRtt9v5jmml3bazs7MYHBzEgQMHln2v8YCO1mdmZmAwGPibJKV6zMlGR0cHMjIysG7duqjHDg4OYvfu3bj00kvx/vvvw+12495778XNN9+s6j3z8/Px4IMP4mtf+5qi4/fs2YNLLrkE//iP/yj5vMPhQE5ODn+NkHpu6vf/gux0bfw+HG4Piq/8NhYWFhQ1KiUCmk//Go1GTE9Po6urC7W1taiqqkrYRdA370TZgRrMDUzIHsMsOnliZf/wL4qIlWEYdHV1YWpqCuXl5fD7/XHvmRa12LZtG0pLS3H8+PG45P/sdjuam5tRVFSELVu2CJqqYrF/I3Zyu3btCruLXc5INRLokQ8glJYmBDQ6OgqDwSBIpSZCuGAl71PFDU+k23Z2dhYDAwOwWq2CbltxrTmZ6V81oKP1trY2WK1WpKSkSOoxa9WAXs18bUlJCTiOw9GjR1FUVMSfs1IwDIM//OEPcLlcvDB+JHAch9deew09PT144IEHJI/x+/04evQocnJysHPnTvnFVnGjUiKg6fRvIBDA4uIigsFg3GMdSiAeraHBOBdhyswC98JjMFxyrewa4tGWmZkZ3gkkVgQCAbS2tobVZGN1wOE4DiMjI+jt7UVdXR0qKioEVnWAOiuwQCDASyxKKU2pFf9fTqSkpPAXZ1rjllj3rWbLN6luW5JSHBwcFKRQSZSuhd9JLCD19EgG9Fpz3FEjU+h2uwGEmmSMRmNkEqPQ1taG+vp6eL1eZGZm4tlnn8WWLVtkj19YWEBZWRk/KvLv//7vuOiiiwTHPP/887jqqqvgdrtRWlqKV199NbKGuO6nqk0Q5xoAKCsrW1ZCzdtQKohWpeqqBJzbBcgQK3FuoUdbxCM1arG4uIimpiZkZGSE1WRjIVVaIEJKc1mtcL3T6cSpU6eQkZGBQ4cORUwZ0xdwg8Gg+kIXS6QaCWKNW7/fz9di29ra4lJA0sJFXFxr9nq9YVF6WloaAoEAfD6fppxoIkFcMhAb0NMyiiMjIzAYDIKUeLKUrNSIPzidTr4hTw3q6urQ3NyMhYUFPP3007jmmmvwxhtvyBJrVlYWmpub4XQ6cezYMdx2221Yv349zj//fP6YCy64AM3NzZidncWvfvUrXHHFFXj//ffly05GDTUqGc+xRiUp0FEU8bZMFCof/xNGvnoZgPDGpOI9tVg8PSk4nozWSO6TIlZadWnLli2CWhoZqYkFExMTaG9vx7p161BTUyMpKKBmba/Xi6amJgCIKBChdF1S362srERtba0skSSSYJYztWq1WlFSUsKn3sQKSOnp6QIFJLmoQ6tSiqmpqSgrK+OdaBwOB0ZGRuByufDOO+8su7xgohCtDp+WliY4T9JlK27sWmklKzWkSsZp1H53rFYrampqAAB79+7FiRMn8POf/xy//OUvJY83Go388bt27UJXVxfuv/9+AalmZGSgpqYGNTU1OHToEGpra/HrX/8aR44ckd6EHqkmD+IPTDAYRHt7O+bm5niZPZLOiRfEBUaupce/4ERWVUkYsQJLdVWSAibg/vpfCJ53FVpbW+F0OiUFKGKJJmknnJ07d8reEapZe25uDk1NTVizZg22bNkS8csdzVOVHmsi9d1IoHWK4xWmWCnIKSDZbDZ0dnaCYRg++ikoKFgVjTI0iFCB2+0GwzDYunVr2PnRlnZyM6PJgJrPEZ2NECtZ9fT08NZ9SrR7E7FvNaSaCPcflmVVBSZKjo96jD5Sow04HA40NzcjLS1NoOJDj9TEivn5eTQ3NysaPZEjVhqc2wVDeqhuaH77d9idkgam4XOSqU+1pOr3+9Hc3Ay/3x/VCUeJoXgsAvvR7N/a2towPz+veKxptUSqkSDVEGSz2TA9PY2+vj6+UaagoECzkaoUSEpeSl7QZrMJZkYJwSZbXCOem7NIjjvDw8MwmUyClH8iHXfURqpqU79HjhzBxRdfjMrKSiwuLuKpp57C66+/jldeeQVAuDn5/fffj3379vEZwRdffBH/9V//hUcffZTfw09+8hP8zd/8DUpLSzE7O4tHHnkE4+PjEW3iOIMBnEZuwJKxD02QKnFqqa6uxoYNGwQX4XiMyulUMt85TEkWyjUm5W4LpUM8Y5MRU8Bwu4D0DBh8HphO/Blo+HzYIWr2T+Y7c3NzJU0BxIhG2ETRaHp6WpXAvlz6lzRhmUwm1NfXK67BSdVpiaOQwWBAYWFhVNs3ep1kg24IqqqqEhiX9/T0wOv1IhAIgGGYhInILxekGpXE58cwDH9+WhDXSJT4g1rHHbUzwGKoaVRyOp2qI9Xp6Wl8+ctfxsTEBHJycrBjxw688sorfOOR2Jzc5XLhuuuuw9jYGNLS0rBp0yb89re/xZVXXgkgdO3q7u7Gk08+idnZWRQUFGD//v146623JOdeeXzE5lTVIqmkyrIsWltbMTs7K+vUIidTGA10KlnOBJ0gUmMSAT1aIwf23adhFBGr0kiVdJ+qsYCLtDapn3Ich/r6elXpSakImIzfFBcXq1ZbEpPq/Pw8mpqa+KYRMhahpGapxShQbFz+4YcfIj09nReRt1qtmonyxFDS/WsymSQbgYi4BnFmWY7oTgrL5VITyXGHuCbFY3oQS01VDX79619HfF4sYnDvvfdG9ApNTU3FM888o2oPAMAZTeCS0CAkhWTsI+k11czMTGzcuFE2Sokl/bu4uIjm5makpqZKCsIX76nF1Km+sNeRqJQQaGbdhtDjk1P8MfxoDZUCphE89VJo33tC0mBKosmuri5MTk6qtoCTWztWg3ICcfqXZBLq6upQWVmpai2yHhC6gI+Pj6OzsxO1tbX8WEukmiXpXk1NTdVstEeDOCvl5+ejtLSUH2shBEuiPHJeiaibxYNYRmrEjUBS0R0hWKUi+WqwUn6qUjZvJFVMmx6Qm6VINxNE/3s507+agd6olDwYDAbU1NREJB216V+i6CPXMUsg1e0rBuNywRTNeolKAXMpS9Ggr/11pGw7P+JIDR1NNjQ0qG52kUrTEgKMRyiDkDVN+LH6s5J9AkB/fz8mJiawe/duFBYWCtS0xLUup9MJm80mMC/3+/1YXFzUdGeqGPRYCwCBEQCp4dFGACshoUhjOfxUCfF0dHQsS0NXMvxU5UwPlDrukO/pckaqmoEu/qBtKCVVhmHQ3d2NyclJSUUfGqk3/TO8v/iOoqYkALCWFMOQEaq9BqcjH2/0usCmZsDk98DX/jpMGw7yd6n0hYCkU5V048q+F0XY8US8YhgMBvj9fpw4cQLBYDAmwqdB9jg7Oxu1+Yq8P9GBXbduHR/Fdnd34/Tp0xgeHg6LYrWESESVlpaG8vJyvoYnp2+7XBKKavYaC8RjSeTmiDR0paam8ucXq/KRFgT1pRx3SKpYfDORn5/P7/dcIFXOZAKnke7fZOxD86RqNpuj1lTFpt9qCIAQK6mrRmxMOotoKWAaxoH38bHqfP7iRdxg+vr6wtSM1MJoNPKD+8QRJl4CBJYIOj8/H3v37o2rBuh2u3mHmt27d0cl1O89xuCfrhV+EUgUOzw8jOrqaqSmpoZFsYRglyPduFwgtcj8/HzU1NTw+rY2m00gobictcrl9lOlb47ohi5a+Yicn9KGJy2QqhgpKSlhNxNEp7i/v5/PQMzMzCjKSDidzlVLqnr6N8lQ0iQRKVKdnp5GW1tbXKbfdLcvAT+bKkoBm9eF6qzcLOXJKpMCpuHp+xBBcxpOuzjYbLaozVNKYDQa4fV68e6778ZcPxVjYmICHo8HpaWl2LFjR1wXXGKevnbtWrhcLtm9HflVKDXGMKE6rhSxEshFsbOzs3yEQGv5ai2KjQRa35aIMxCC7ezsDDMCSASxrKRLDd3QBUBQoySpcFqnWOomguM4zTjryEHKcWdiYgJ9fX0Cf1w6IyE+H7fbHbf7VLKgNyppHCaTCRzHhd2dchyHvr4+nD59OmGm35HAuRZDKeCzBBoJdAqYsQpJtirDgLq68OapWEAUf+rq6uI2GiA/z5GREaSnp6O4uDiu9UZGRtDT08Obp4+NjYV17h75DxaAAUDocZPJEJFYpeZnldRiV2sUm5ubi9zcXL5WSWqxra2t4DguIZ6qydT+lRpnsdlsGBkZ4W8ixDrMpDa5Wn6PQOgalpmZCavVioMHDwpMHcbHx3k5TNL1npGRAbfbrcrl6P7778czzzyD7u5uftb/gQceQF1dXcTXzc/P4wc/+AGeeeYZ2O12VFVV4aGHHsJnPvMZ/pjx8XHcfvvteOmll+B2u1FTU4PHH38c+/btk1yTgxGcRiJVDudgpBoNJLoJBoP8navP50NLSwt8Ph/q6+tjSpOQuiqwFJWmlZcIotVIMBQWgUtJAwfAMDcb8VhzwI2gJR3moAdBcxq8Y71YNKegsGqj6n0DofRXd3c3ZmZmkJOTo8hKKhKCwSBaWlp4wf729vaYpRVJ6nhqakoQjdOEePtRod2fwWgAxyoj1kiQimJJOpWOYgnJLpfW7XIQldVqFRh6kxsqIr2XkZEh8FRVSjpaEdSXchMiUWxbWxtYluUbgMjxqwn0OA1t6iA2Kz969Ch+//vfw2AwwGKxwOFwKLIse+ONN3D99ddj//79CAaDuOOOO/CpT30KnZ2dsiUXv9+Piy66CEVFRXj66adRVlaG06dPC3TW5+bm0NjYiAsuuAAvvfQS1qxZg76+vshZNr1RSdsgH0SSAibjInl5eYoEEtSCHqNRMpvKQ0EKWIzZ073wm9KwtrxC8f58Ph+am5sRDAZRXV2NhYUFxa+VgsvlwqlTp5CWlsYL9itRapICUYMKBAJhs7GvnT4fr50GgCBPnCaTEQwTIm8xsXo9IeK99RdB/OymEPmpFdS3WCyCkQgSxYq1fFdbFCsloSi+eaBVgSLV2JPRSasEKSkpYTcRdrsd09OhssuJEycEs78rpd8bK+TGgMRm5dXV1diyZQvuvPNOvPzyyygoKEB9fT0+//nP46abbpJd/+WXXxb8/4knnkBRURFOnjyJj3/845Kv+c///E/Y7Xa8++67fI1XfIP+wAMPoKKiAo8//jj/WHV1dcRz5YxGDaV/z8FIVYlkHhGAGBoaQn9/v2K5vUSB1FX5FLD4+dIqAIBpYSliFaeASbQqhTNjowjAiqry4oj7WFhYwKlTp5Cfn49t27ZhYmICc3NzMZ/XzMwMWlpaUFFRgY0bNwocZNSS6uLiIk6dOoXs7Gz+ZufOJ0OEybGA6eyHmxFFwGJi9boDYXu49RchndGrG2I8UWgnil0OyN080F6jtKgGfXHXen0SEBJPcXExjh8/jg0bNmBubg69vb3w+/2Chqdkz/5KQamaUl5eHq688kr8+te/xrXXXouPf/zjePXVV+F0OlW9H7nZjjQG99xzz6G+vh7XX389/vznP2PNmjX44he/iNtvv53/jDz33HM4fPgwLr/8crzxxhsoKyvDddddh2984xvyb643KmkfRqMRXV1dcLvdCfVVTbnxQfj+7bsAwhWTUjaEmpckR2hEUSkhUP+aClhnRiO+J0kBW4I+BMwpsDIe+E1psMCP02NTCHChO8aaCuGXQUpxKR4/1eHhYfT390vWo9WuS9xqqqqq8NT7tXiuQ/i8wRgiViBErgzLCtK8wQBLRakUyYrI/Ym/bsaPvqzu4iIHMRGJHWliTacSJOuiLtdxa7PZ0N3djUAgIJgbZVlWUwpP0UBuAsT6vaRGOTg4CLPZzBNsfn7+is/+SkGNmhKwNFJTXV2Nb37zm6rei2VZ3HLLLWhsbMS2bdtkjxscHMRrr72Gq6++Gi+++CL6+/tx3XXXIRAI4K677uKPefTRR3HbbbfhjjvuwIkTJ3DTTTfBarXimmuukX5/gwmsQRuRajL2oflvk8PhQDAYBMMwaGhoSNhYQTAYRFtbGzaLHhdHpeaikqizqQQmvwf+NaFUrsURuc4qB4shgABnQf+oHQEu9OvhnGMC0QSCWP1UOzo6YLPZZAXxlUaqHMfh/v8xASgBUILjMxyMJoBlJAiSIlaCYIDhiVWY/hUSKwD4vaEI9h9/k4mHv6vqlKMiUjqV1JfVRLFaklIUSyi63W7YbDbMzs6iv78fRqMR6enpyMnJiXludCUhblik9XsrKioEs79ElEE8+5uMyFwNqRJDg6ys8KyYElx//fVob2/H22+/HfE4lmVRVFSEo0ePwmQyYe/evRgfH8eDDz7IkyrLsti3bx/uu+8+AKGxuPb2djz22GOypKpHqklGpDt6Ep1ZLBasX78+YYRKyxgqAT9GMzMpmwIWw1MQkvNLcdnCUsAkWlWKYHo11myoxmmHCRSnqiZVouBkMBhQX18f0U9Vjhj+5U+hjwxzlgDNFiAYEB5rNBlkidV9Nr0r1ZgkJlYA8Hr8Yc/d8KALD383itJVHJCLYummoOWU4Vsu0KpAlZWVYBgGra2tYBiGnxul7d60mEaNNqNKz/4CSw1PNptN0Gkbi/F8PIglUo020y2FG264Ac8//zzefPNNlJeXRzy2tLQUFotFsK/NmzdjcnISfr+fb44TG5xv3rwZf/zjH2XX1V1qNAiGYfgO0t27d6O/vz/mblQxxMbfvuO/X3rfOBqTSApYDaRSwAFY+WjVYggiwJlhMQYQYC2wGBmcHAyRTIA1ASgAWyjnECsEafAqLCzE1q1bI16YiPzhwy8t3cQEg4TsQvOkJqOBIlYDggHhY2JiJWRKSFau49dgFEamcs1M197nwGN3RO+KjBfRolj6Ik1HsVojIymYTCZYrVZkZGRg3bp1/NyozWYTaNsmS0JRCmqFH6QansTNaithWK5Wr1itohLHcbjxxhvx7LPP4vXXX4/aTAQAjY2NeOqppwQ/097eXpSWlvIBTGNjI3p6egSv6+3tRVVVlfxe9DlVbcHtdvP2YkQdaGhoKG5PVZZl0dPTg/HxcYHxNz1aQyDbmFR+9oMaYYRGHJX6MgoQNKch1WOLa/+R8FYPEGBCXwryNwAE2bMEFTQAKEYw4zBmPUD7+6HHg9SPlIhWBRkOwAEEp4CzwSIYFjCbDTyxEtAkKvWY0WSAhyfTJeIkkOv4DX9uiVgZhhUQK4AVIVcCJVGs3++Hy+VCQUGB5qNYulGJnhsl2rY2m00goUhuHlba7o0gHjUlOeN5u93O15vjcaGJBDW2byT9q4ZUr7/+ejz11FP485//jKysLExOhkpWOTk5fPe32E/1W9/6Fh5++GHcfPPNuPHGG9HX14f77rtP0GV86623oqGhAffddx+uuOIKfPDBBzh69CiOHj0qvxmDduZUz/n079TUFNra2lBWVoa6ujr+Q6hEqjASvF4vWlpaEAwGJbVnUzbUwDfQL7+AKCrl8gqjzqaG7SGtAAFzKIpJ885HbFiKFq0GWNPS3yYGAcYEi4lFgDHyf0eC2cQhyBhgNgmJVXCMeYlolx4LEStNkIRESbRKHvO4g2f/beQ7fuVGaeiOX2CpJkkTK8uwYFlhNLvSUasYUhdpm82G3t5enD59GiMjI5JRrJYgN6cq1rb1er18MxCxe6ObgZbb7o0gkRKFYuGQ5YzUGYZR/Pv3eDzgOE5VTZUYi59//vmCxx9//HF85StfARDup1pRUYFXXnkFt956K3bs2IGysjLcfPPNuP322/lj9u/fj2effRZHjhzBPffcg+rqajz00EO4+uqrZffCwQAO2sjUqN3Ho48+ikcffRTDw8MAgK1bt+LOO+/ExRdfrHiNpJMqEPqi9Pb2YnR0FNu3b0dJSYng+XiMyu12O1paWqLK+EUlVhHoMRq1KeCFjFIAQIZvXvH7xQKzkUOQNcBq5uAPKiNRs8lwNlpdgskYilYFj0lEnoRYvd6gZMcv/TqTyciTKamfSnX9GowGPhVsNBp4YiXPBXyh577+YzsA4D/uis1JJxGwWCwoKSnByMgI1q1bx2sUi2uxtEJQsqFU/CE1NVXS7o2oH9HNQNnZ2csWxS7XXK2cCw0dqROZyPz8fNXnqNZLFYCqmqqS5jixnyoA1NfX47333ov4uksvvRSXXnqp8r2s4jnV8vJy/PSnP0VtbS04jsOTTz6Jz33uc2hqaopszE4h6aTq8/lw4sQJXjBAKuURC6mqEa73XPBlpP31N4IxmmizqQRS3b5yjUniqNSVkosArMhglAk4JCJaJcQaKVolxEpHq4RY6TQwT5BGA5zO8EyCFLH6vMGw+qlc1y8hU5pkjWdfw7IcmEAwjGi//mN7UomVwGg0CqJYYolms9nQ1tbGywySi3SyothYFJXEdm90M9DY2BgALNu5rZSYvlSkThSeRkdDY3NqZCLV1FSdTidMJtOq0q2mwcGYFHlAKajdx2c/+1nB/3/yk5/g0UcfxXvvvbd6SJWkWKqrq2Xb+Yn4g1IEg0G0t7djfn5e0Vyr+MNuXrcBweEB4UEJaEySg8uUE5pP5YB0g0s2BRwLxNGq5DGEaCVTvuGP+QMsrBYj3G5GtjFJPErj8S7VS02m0D7kun6lOn5pYg0GhHcBQqJl8NUfzuDxe+Wt/5IBsSUaqcWOj4+ju7s7aVFsIiI/cTMQMQIYHx9HV1dXQk3Lk+VQQ5sd0OdIshBEnYuco/iaojZSzcjI0EQmIxawRhNYjUSqZB8Oh0PweEpKStQbIYZh8Ic//AEulwv19fWK3zPppGoymVBbWxv1GNrQOhKcTieampqQkpKieK5V6sNuXrcB3Ez0+VQSlQayC3nFpBSXDd60gpiakxbY0NxogLUgx+yQPS6R0argOYk0sM/PIcVqgM/H8iTqD5xN50bo+GUYFj6fkAAjdf1G6/hlgqxoLSM4qiucpXLUX/3hDAAkhVyjpeLEtdhkRrGJ1v41GAzIyclBTk4O1q9fLzAtp2d+Y3UR0oLtm/gcScOTzWZDV1eXQGAjPz8f6enpqhqVYh2n0Qq0WFOtqBBKwd511124++67JV/T1taG+vp6eL1eZGZm4tlnnw0bK4qEpJMqEF1swGw2K0r/Tk5Ooq2tDZWVlaitrVX8ITYajXjFtA6HmeHQAyQqXVOiiFjFcOUszYdFSwFHakxaCIaab0LjM0C62Qt30Ip0s1/RPgixRopW5dLALs/ZqPBsqtftCSdRArmOX8ExCnR+lXT8AuKa69muZ9/SGsBSSvnLR0K/v9/cL6zTawmRotiuri5Jt5ZEYbkF9eVMy2kXIVq5Klo0pwVSFUPc8ORyuWC32zE7O4uBgQFYrVYEg0E4nU4UFBREFdhY9aSqoe5fso/R0VGBMUGkG9W6ujo0NzdjYWEBTz/9NK655hq88cYbiolVE6QaDdFqqqTRaWxsDDt27EBxcWQNXTGMRqP8haW8esmJRmUKeDFrLbIWz6jaSyQs+EPvteALtcjTkWi6NQC334x0azBilOr1G5Bq5eD1G0RjNGf/vUwdv6F/R9f5FROrzytNlgaDAQG/9AgO+X8wsHQyXz4yuaLEGitRqYliCwoK4u66XUmXGikJRXJudIRHIvT09HCtbC2SKg2DwYDMzExkZmbyAhvz8/Nob2/H5OQkRkZGwhSexD9/QqqrYdZZCqzBCFYjpEr2Qb5TSmC1WlFTE+qv2bt3L06cOIGf//zn+OUvf6no9do48yiIVFMljU6zs7Oor69XTaj0e0QCU1rFd/yGvdbvARCKSoGQvi8QEndYzFoLT2ouAMCVkss/DgBW5uxxCEWeFkPg7N+hc7UYz/7fyAj/NpG/l9KdC57QnZfDa4HDa8GCx4wFjxluvxEOjxFunwEOd+jX7fSEvqzkhtlskhipkPhkmCSOM1uWHqO/Rybqwke/zmQyIuBjEPAxfNcvICQhg9GAgD+IgD/I10vp5wAgGAjCYDCEvc5gNIBhGDAMw/+f4P9+7wz+7/fOwO12h5+cRkEiva1bt+K8887Dzp07kZ6ejvHxcbz99ts4ceIEBgcHsbCwEJM8YjKt38xmM4qKirB582Y0NDTwVoEzMzN4//33cfz4cfT29mJ2dpa/qdY6qYphMpn4eeXt27ejvr4eJSUlcDqdaG5uxttvv42Ojg5MTEzA5wtdF9TOqAIhP9X9+/cjKysLRUVFuOyyy8JEG6Twhz/8AZs2bUJqaiq2b9+OF198UfA8x3G48847UVpairS0NFx44YXo6+uLuCaJVLXyJ16wLMv/bpRAE5FqrOlfohKUn5+PvXv3xqVbajKZsNB4FXLe+V3ogRis3CKBjNEsF+JJ9YaeC+/45Z+T6fhlWA4ed2ghKY1fccTqdkVO8yrt+BWDHBuUeI68H0M1N33z7nkA87jz66F0XKKVdJZL+1euXmmz2QTG5WqiWK34qYojvGAwiPn5eX7ulzjRcBwHq9WqmX0rBen+FY8mORwO2O12jI2N4YMPPsCPf/xjFBYWwu/381KBShCLn+q7776LL3zhC7j//vtx6aWX4qmnnsJll12GU6dO8UL8//RP/4Rf/OIXePLJJ1FdXY0f/ehHOHz4MDo7O2Xr4Sw0JKgPdfs4cuQILr74YlRWVmJxcRFPPfUUXn/9dbzyyiuK1zBwGlD/DgQCEWUIZ2dn0dXVhY997GMAhOMyibKBe/PNN7Fp0yasOfHM0oPpoQ8jIVWS6mWsof+TMRrx40FLOq/tSwQf/Kaz/0foSxLgLMhlZyUfD/29dIMQYM8+dra2yv/NkL+FakpESQkAT6yEREljkryakvAx0vvj9jCwWkLrL1LjMyz1GoZqFKLF891uP3UMRx2z9G/S8St+HMBZwmTOPif8nJBxGkKarEi1nwnIlw2YIIMb/s8IAoGAgIziHWV47733UFtbi4KCgrjWUQO6I9Vms2FxcZGfq4w0O/r+++9jw4YNAqMGrYF2ohkdHYXP54PVauXPLS8vT9NGABzH4a9//WuYx7AY8/Pz+OMf/4ijR4/ybjsXXHABDh8+jK9//euqPpczMzMoKirCG2+8IeuneuWVV8LlcuH555/nHzt06BB27dqFxx57DBzHYe3atfj2t7+N73wnpDq3sLCA4uJiPPHEE7jqqqsE6zkcDuTk5KD11ElkKZV7XWYsLjqxY89eLCwsKEr/fu1rX8OxY8cwMTGBnJwc7NixA7fffjsuuugixe+p3U8iBTr9GwwG0dHRAbvdzqeLEoXOzk58QsXxnoJKpNlGJJ9T0pg0bywEuCUCTTd6YhqjEXf8kmhVcl8S0Wok4Qdxx280jV+641euMUlNxy8g7PqV6viljzUajDyxskFWNgvCnD35h/8YMj746c0s7z+aCPPylY6ipKJYooDU0tICAJJR7GqI+GgnGrfbzYvm2+12DAwMwOPxICcnhz+/ZEkoyoHlszWRo6bc3Fx87Wtfw+TkJMbHx/Hd734Xr7zyCl599VVce+21qt5TiZ/q8ePHcdtttwkeO3z4MP70pz8BAIaGhjA5OYkLL7yQfz4nJwcHDx7E8ePHw0iVICSor40UvVpB/V//+tdxv6cmSDXaF4Ckf10uF5qammCxWNDQ0JCwUYPp6Wm43e6QPdbHvwnDS2d1LRU0JnkKKpHiSoyuL93tm2MJ9w2VG6ORghI1JanHfP6ljl9xYxINKWL1+YTp13g6fkktNHB2TTUdv+JIl5Y/JBc48Wu+968cgHz89p+3hwnma0GkQS2Iw4h4dpQ4P5EoNh75z2SAZVlBlFpbWwuPx8P/zk6fPs0LN5DfWbKNAEjpSmmJwe12IysrCzt27MCOHTvw3e+q8zpU6qc6OTkZ1oNSXFzM6waTvyMdI/n+GmxUWkloglSjgUSqx48fR0VFhapxmUjgOA4DAwMYGhpCRkYGSkpKYDQaES0fLieaDwCpHltcYzREMWkhEEqfiFO96RbpcRpPwIQ0C6Os89e3lOqlr6lqpAgBYcdvtNcB6jt+xc+Ja6fROn4FM64y3eMcJ4x0/+93xgEAv//5FtnxlmgpVQ1UVASIFMX6/X60t7ejsLCQJ6KV0vGNBVKNSmlpaWESioRgxUYAUt22yw3y2VN6zSKjN7FCqZ/qcoHjDOA4bWQKkrEPzZMqy7ICcePS0sQ0/AQCAbS2tsLpdOLQoUPo7e3lP/xMaRVME6djWncxay2AkGj+csAdsMrWUBc8oV+noF4qqqE63Wc7f2OQIhRbvQHyjUkEJPoM+BjB/4HIGr8ABAL6HEuRqeh1AAREC1CjN0aDQBSCBnv29y0V6V5x4xAA4P89VInMzEx+vIXULMfGxmAwGJCfn88TUrIjIqWgo9i5uTlUV1fD5/OFRbHLreMbC5T4qRIJRSA0HUBuIEZHR/nf2UreQJAmJaU/R5fLFdFaLRLU+KmWlJRgampK8NjU1BSvvU7+npqaElx3p6amsGvXrggra0emMBkDLpogVbkPm8/nQ0tLC9/OnKjGj8XFRTQ1NSE9PR0NDQ28US8hVfOeixE89RJ/vJRoPolW5aBWNF+pG00kKEn1KpUijGb1Fgiwkh2//LFnydjnDY8qlXT8AkIBfVYUaQplC6V0h4Udv+RCTNK/4vWA8LQxAHzhlhGwHIun/rUCRqMRxcXFKC0t5Ts3SUREROULCgoS5v27UsjKykJZWZkgihXfOJBUarKj2Fj8VIm8IMuyfOaBvoFYLmENAjVqSkBs4g+x+KnW19fj2LFjuOWWW/jHXn31VV6Sr7q6GiUlJTh27BhPog6HA++//z6+9a1vya7LwghWI6SajH1oglSlMD8/j6amJuTl5WH37t04duxY3J6qwJLqEjEpJ4RuNBpjuhgqFc0HgAxmQTYFrAZKGpPUShES0I40YuEHt+dsh20EjV+T0Sjo9qUhRawBfzAsxQsspVBDjUih9xBHsGxQ/vcl1/UrRZyR1iJNT1+8LSSi/tS/VvD7zMrKQnZ2NjZs2MBbo9lsNvh8PnR0dKCwsBCFhYWa7k4VNyrRUSxNQqOjo+js7Ex6FBuPVrHRaJQdSSLCGrT4RKIE7dXo/gKhmqpaUo3FT/Xmm2/GJz7xCfzLv/wLLrnkEvzud7/Dhx9+yHulGgwG3HLLLbj33ntRW1vLj9SsXbsWl112mexetChTuJLQ3Ded4ziMjo6ip6cHtbW1qKqqgsFgiMv+DQh9Gfv6+jA6OiowKSeItL6UE00smDeGxhbS4Yp4XCzRqhrhfOFj4Wlgn49FSooRbg/Dp3lpyInne7xSzUfC+ip5LuAXRpdh9VGDgT9G3PFrNBr4BiZA2PELyBMkQ528wWAEd/Y1HMuFpYDFozkEX7hlqdv7/z1Uyf/bbDajtLQUZWVleOedd1BRUQGfz8d3p+bm5vJklEjz63gRiaSkSCjZUWwixR/k5CEnJibCRPJzc3Njfl+1pBqL+EMsfqoNDQ146qmn8MMf/hB33HEHamtr8ac//UnQ3PS9730PLpcL3/zmNzE/P4/zzjsPL7/8csQbDpYzguU0EqkmYR+aIlWGYdDR0QGbzYa9e/cK2sHjIVW/38+nkQ8dOqTIXo5OAcuJ5iu1eBNHpW4uQzBG42bSkG7yxDVGIwUl0SoNn5/jCdDnC5GKuDFJapTGJ1UvFRErEOomFpOp1GvITKqajl/j2S4/uu4KRO74lYpaxeROQ/z4F24ZAcey+P2/VYNlWUGmIyMjA+Xl5aitreXNr2dnZzE4OAir1YrCwsJlEZ5QCzUjNVJR7OzsLB/FroSn6nIpKkkZzhOR/M7OTjAMw4vkFxQURJw3ldrzcpNqrH6ql19+OS6//HLZ1xgMBtxzzz245557lO9Fj1STD4PBAJfLhebmZpjNZtTX14fdCZnN5pja/xcWFtDU1IScnBzs3r07or2cGimqRIGM0dDdvjnWpUjWHbAi3eKPO1r1+oDUlCWidbrOdsWe5QFxY5JgXQli9XsZRfq+hIhIbZVAKhXMk6ms5VtQsuOXYzn+hkj8vFRKP1Rvlf4siUmaFXUHC48NrX3l2aamp/+9BuPj4wgEAvzn1WAw8HW98vJyMAzDX6x7enrg9/v5lKPai3UiEOucKh3FSnmqLlcUu1IyhVIi+TabDdPT0+jr60NaWhp/ftFujNREqhzH8SM1qxU6qWoAdrsd77//PsrKylBXVyf5pYklUiXNCDU1NVi3bl3Ei4fRaFS1PnGiicXeDYjcmESE8wl5LnhTz/6fFtAP8mM0br8R6dYl8iAav7yK0ln+cLojSUFGbkyiiTXgPxuZKhDOD1INTYD0KI04eqWfA4RpW/IcAFkyFXcEi8Ge1QUma9B/iyH1uFjVieD/XEs0UUvx7Pk5YBgGLMsKPldEuKCwsJC/gIov1oRg40k5KkWixB9oT1W6iWtkZCSsoziesZZkaP/SEopVVVUIBoOYm5uD3W7nb4xIep8YAdDntxKNSloCCw2lf8/VRqWsrCxs3749ohi+GlJlWRZdXV2YnJzE7t27FUmwmUymsKhGKgUsTvV60wp4KcI077ziFLAaSKV6Hd7QGmSMxu03htVL+fOIsTEp9Lx0x+/S68OJlUSldCQKUApLZ4k1IBKLCGtEYhj+3/TrCMSfB0KsZIRGScevwWiQr8Eq/LxJEe/ffrMHHMfi//vPbTyxsiwbmrMNLqWoU1NTUV5ezuvdkmiPpBxpFaREC0/wNzcJTtMajUbk5uYiNzc3LIqlx1piiWK1IKhvNpuxZs2akFjM2Rsjcn7E6o2cW15e3orUVLUEFgawGolUk7EPTZCq1WqN6i4TyamGhtfrRXNzM1iWRX19vaR9lNz68XYXqx2jIUh2YxJNrAQ0sfp8rEDjF5CeT42U4g2tudTxKzhOYpSG1E7DZlbPEgBt+0ZDaiaVJ1eJ368UoZImJdrhhmM5yQhVMpKlmpw++/ft/L8JwdJ/xFFsYWEhn3J0Op2YnZ3FmTNn0NPTg4yMDBQUFKCwsDAhNcvlIlUxIkWx9CiSkihWC6RKw2AwICMjAxkZGaioqOCt3gjBejwepKSkwGKxwOl0RrV0Y1l29ZOq3qi0OqDEqJy41hQUFGDr1q2q7g4TQapyYzRuLgPpBpeskpISLEdjUqT5VDdxnxE1JkmN0Xi8QoIT6/sS4onU8btElrF3/Mo1a5D0sbjjV7wHQL7rV2pP0QiVX/PscZd8pQ0A8MIT288+zvKNVFJRbHp6OqqqqsJ8VYmWLyGiWGuWSy5AK3fhkYpipcQZyLmJBTW0RqpiEKs3MlPv8XjQ3d0Nj8eDkydPCp7Py8sLOz9iS7iaSZVDcmqZUkiGtpl2P50iRCI94lrz4YcfYsOGDdi+fbvqjkqp9aenp/G6felDH8k3VQ5kjGaBzVG0DznvVCmYjaGPjNUc/tExm87WQqkfg5R/qs+/FI26PQzcHgb+wBIxmIzhXw6SzvX5GPipeVA6sqM9VINBBkEqZBb4qPL1TwbBACPyRzXyXb/BQLg/qtFgDJFrcCmtTL+ekBX9GoPBGK4NfHZdKUINRah0Y5TxLLkq/7qKPWEv+UobLvlKGz779x0wmUywWCxISUmB1WqF2WzmP7sMwyAYDMLv98NoNKKoqAhbt27Fxz72MezcuRNpaWkYHR3FO++8gw8//BBDQ0NYXFxULJO4UpFqJJAmrm3btuG8887D9u3bkZqaipGREbz99tv8eTkcDnAcB47jNE2qYqSlpSE9PR3FxcX42Mc+hi1btsBisWBoaAhvv/02Tp48KTg/lyvUpKiGVN9880189rOfxdq1a2EwGHhBfDm8/fbbaGxs5BvjNm3ahJ/97GeCYx599FHs2LGD74iur6/HSy+9JLOiEESmUCt/VhqaiFSVfKnl0r/0GE48rjW0+APHcRgcHMTg4CC2bdsGl3sWGQtjMa1LgxArsXJLN3ngDqYi3ezlU8CREClaVSOc7/IQ3d/Q327P2ZqoTGMSgdFkgMcdnnZVIpwf/tzSaxiGjdjxK0a0jl+DwRCh3iovCkF3/JLHwo+TGc+JEOEC4Wlsgku+0sav+eJvdgrqwBwXOkc6mg2tZURmZiaysrKwfv16PtojKVU6GsrPz5fteNcCqdKQi2LpWizLsrDZbLBYLKtGFpJhGFitVr5JLT8/HzU1NfB6vXz2YWRkBPfccw9SU1NhMpngcDgUd4K7XC7s3LkTf//3f4+/+7u/i3p8RkYGbrjhBuzYsQMZGRl4++238Q//8A/IyMjAN7/5TQBAeXk5fvrTn6K2thYcx+HJJ5/E5z73OTQ1NWHr1q0R1z/X07+a8FMFEHWcpbu7GyzLYsuWLfxjbrcbzc3NMBqN2LVrV1wKKHa7HW1tbWhsbERbWxscDgd2796N7OxsLAy0CY4lHb9kZlWJd2ro79DFLZI/ak6KBwu+NKRb/FjwpiLdGuCfC/0d+pAseMx8xy9RUyK1VacnJJzvdBsiCufT3b70GA0hVppUaeF8Wt9X8DqqnumlyFf8EaPTwWKSoYksGAjKvpaQppj4InX8Sr2HXMQplRWJVlONRqzic5Vb88Xf7JR4rbDZiSZEo9HI/00LyttsNrjdbuTk5PAkS9f0/H4/3n77bZx//vmaj/5YlsX8/Dyam5uRkZHBj50kUyhfKdra2pCTk4PKykrZY1iWxbFjx/Db3/4Wf/7znwEAe/bswac//Wl84xvfiPhaGgaDAc8++2xExSMp/N3f/R0yMjLwX//1X7LH5Ofn48EHH8TXvvY1yeeJn+pfTowgIzO6d+lKwOV04ML9lYr9VBMBTUSqQHizihhmsxkej4f//+zsLFpaWlBaWopNmzbFfVEgkfB7772HlJQU1NfX83WqnA3bBcS6mLUWWYtn4no/QNrKbcEXImQyRrPgWer4FJOnw0NGZ6hUa5zC+aHnQxGrEuF88SgNIVOxvi8AKvpkqHpeuHg+aVIir6U/F5GcaFiGDev4DT0uPZIj1aQkW2JQ0KRkOBvpislVKZkSfObLLfy/CcEajUbJKFaq2YnMj9bU1PDm3jabDUNDQwLbNNLEp1UyomE0GvmL4t69e8EwjCA6NxqNguhcS1Gsku5fo9GIiy66CBkZGXj//fdx8uRJvPrqq3j55Zd5b9TlQlNTE959913ce++9ks8zDIM//OEPcLlcvC5wJJzrkapmSDUaSM2T4zgMDQ1hYGAAW7ZsQVlZWULWX1hYQCAQQFlZGTZu3BiVpBez1sJvSkOOayLsOaUWb5Gw0o1JQLjwg4c0K0kI54uJ1e2K7I8KRBfPj9TxS1u+iSHX8cuyrGy6V67rV5zWDf1bfdcvv6aK2qvUe1z8f5v5/7/0210AlhqLyIU60siO1WoVCE+QztTe3l4+OzQ2NiYgWa2CTn+bzWaBUP7CwgLsdrvA3IA0PCU7ilUzUkM6f0tKSvClL30JX/rSl5ZtX+Xl5ZiZmUEwGMTdd9+Nr3/964Ln29raUF9fD6/Xi8zMTDz77LOCTKEcdOu3VQKTyYRAIIDm5mYsLCzgwIEDyMlR1vwTCaR+OjAwAACoq6uL+gWkZ1DpMRryuBokaoyGEGukMRoaUvOpAHjRfMGxEh2/AOD3M5Iav2Ji9XqWBPbla6eMZMcvy3K8OL644xeQF86X6vgFIgjqi9ZVI1cIqCdUA3XTppS0pQgWUBfFEvWm2tpa2O12tLS0YHZ2Fv39/SsuPKEWvNyk6PtJ272Ja7GkxpxM03I1MoUrKfzw1ltvwel04r333sP3v/991NTU4Atf+AL/fF1dHX+9ffrpp3HNNdfgjTfeiEqsuqKSRhAt/UtUTPLy8tDQ0JAQ2bNgMIi2tjYsLCxgz549+PDDD2UVZsQpYDHmU0IC/QHOglw2XHg/0dGqWps3pcL5cvq+NLH6A4xsYxJNrCQqDRN/EKgliclMSKziKJRuJmKDbFhqGQiXJiRpWTpqpSNSua5fek8AwMqoa0SrpUaCUkIVIxLBAkIxk0hRrNFoxO7du/nvl81mQ1dXF4LBoEA+MVGOLfGAjNNEu+kV273RpuV0FFtYWIjMzMxlj2LVKCqtJKkSe7jt27djamoKd999t4BUrVYrampqAIRS7idOnMDPf/5z/PKXv4y4LsMZwGgkUk3GPjRDqpEwNTWF/v5+mM1m7Nu3LyFfArfbjVOnTsFqtaKhoYH/0KuVFJNK9c4bCwEOAtH8SIglWpV8bhmF831nZ0SjafzSHb9y4g+hdYSaumrE86XqnuKO3zDXmQj+qeIIWK7rN1Edv/SaUogUJUvh4v/bzO/l5f/eQ72/UfC3WHhicXERRqORH9kpKCjgVYJcLhdmZ2cxOTmJ3t5eXnhiOX1HoyEW2zexabm443Yloli16d9kSBSyLBu1WVTJMYCe/tU0qXIch/7+fgwPD2PdunWYmppKCKHOzMygtbUVa9eu5bWGyQWdYRjZLxYdrapJ9RLRfNLlCwDpZi/cQSvSzdLeowSxRqteH4fUFAOcLk6glhSLcD6NSOL5pElJSt936fVG+LzSZBmye5MewSH/FzcpkdfLzzAriyJJBKxEAEQN6altUorlPehz/PTVp/h/0wQLCNPEU1NT6O3tRW1tLf/5p6PYtLQ0VFZWYt26dQgEAmG+o7RAw0oZlydC+CE1NVUyih0eHg5Td0pUFKuGVJ1Op2rhB6fTif7+fv7/Q0NDaG5uRn5+PiorK3HkyBGMj4/jN7/5DQDgkUceQWVlJTZt2gQgNOf6z//8z7jpppv4NY4cOYKLL74YlZWVWFxcxFNPPYXXX38dr7zyStT9cFzojxaQjH1ohlTFH16/34/W1la43W7U19cjEAhgfHw8rvegm5y2bt2KtWvXCt5far4xFsilekkkCoAXzSfdvkKx/IBktEqE890+A9JTQp8Wp4cWWTj791nS8/oIQUrXTwmiCeeHjlHf8UsTKwA+HUxqpQRLlm/BsHSuWBxfspFI5psjl+6VIqtYun5JWjm0B2XkrYRQ6feIelyE95Uj2PHxcXR3d2P79u28rzCpv5KZWHEtds2aNSguLgbxHZ2dneUNK8hoS2Fh4bI2BSVaTYmOYsVzo6dPn05YFKu2pqqWVD/88ENccMEF/P9vu+02AMA111yDJ554AhMTExgZWfIBZlkWR44cwdDQEMxmMzZs2IAHHngA//AP/8AfMz09jS9/+cuYmJhATk4OduzYgVdeeQUXXXRR1P0wnBGMRrp/k7EPzZAqDYfDgaamJmRlZaG+vh4WiwWLi4txEV4wGER7ezvm5+dlm5ykRPUjQU40X9FrRaleOhJVMkZDnGjiaUxSIpwfT8evWDifJlqaWKVs2JQIQIRee7aJiU5xquj4BZa6fqN1/IYej71JaSUJVQyaYG//+gJ2794t8CuO1OwkFp7IyMhAZmZmmHF5c3MzDAbDso22LLdE4XJEseQmRSmput1u1V3Y559/fsR+lCeeeELw/xtvvBE33nhjxDV//etfq9oDjdUcqd5///145pln0N3djbS0NDQ0NOCBBx5AXV2d4jU0R6pnzpxBR0cH1q9fj/Xr1/MfXHqkRu2dsNvtRlNTE+/VKuf2oUT/N1rDkhiRotWwY2Mco5FqTOKPj1E4X67j12Q0wu32h71WiliXdHylO36NRgPvoUr+D1AuNTIkCMh3/XIsF9bxGwl0LXVpflV+7bDHVHT9klpxJHJVQqjxNEYBwAP/kQNgGMBwWIoYiD6yQ3fhmkwmFBcXC8TyZ2dnw4iosLAwqph8NKyk7q9UFEs0ikkUG0nDl4BcT9Q0Kq1ZsyZh55EMrOZGpTfeeAPXX3899u/fj2AwiDvuuAOf+tSn0NnZqbjWrRlS5TgOXV1dGB8fx65du8I+WCaTiVeSUfPFJCIRdP1UDkpIlWVZvtOXIINZiMniLVFjNILHJGzepMjWH2AFiklSHb9iiIXzAWlijeaPGqqd0rVR5eL5EYmWIkNaiEHssyrX8UueE5Of7GhNjHOp4vOl31sJ1Nw0RALHcjj8hZP8/1/5f3slj1MzskO0YmkiIulUs9ksICI5+UQ5JFNMPzU1FWVlZSgrKxNEsUNDQ+jo6JCNYsnPR01Ndf369ct2HisCDTUqQeU+Xn75ZcH/n3jiCRQVFeHkyZP4+Mc/rmgNzZDqwMAAbDYbGhoaJNMf5AsYDAYVNUZwHIfh4WH09/crFomIRqp+vx/Nzc0IBAIor6rhHyei+elwRVw/3mhVjb5v6LHwNLDPx0rWT+n0L4HJZIRbQus3vPkoRKxiMg0Tf6DIMrx2GiIaErmKa6f8KA3CVZr4yElBxy+B1MxruNC+ke/6VSpHGK3jN/Q+8RNiLPXcaFBKsIAy4QmLxYLS0lKeiMSWaLm5uSgsLOSF3aPdLGvFoUYuiqVrsfQoEpGQVAKPx7OqDcoBbaZ/HQ6H4PGUlBRF/sREzYoulUSDZki1uroaFRUVsnev9MhLNDAMg/b2dszNzakSiaBF9cVwOBw4deoUcnJysGfPHoxP2sKOoZ1o0o0euJk0pJs8ES3elnuMxudf6viVqp9GE86P1O1LiCaSjyo9o0qcZqRGaehGpGgdvwKrNonfF+9KI/FRoV8breOXJj8+NRxHXT8amYr9W6OuFyOZKlmbJlhAeRQbySs2NzcX+fn5qK2thdvt5oloYGAAKSkpAuEJqchOK6QqRqQo1u128zf4SmqxxHN1NUOL6d+KigrB43fddRfuvvvuiK9lWRa33HILGhsbsW3bNsXvqRlSjdbQQOo30UhVaf1UCnLrT05Ooq2tLazOy+9dwh+VjNEsBEKdfAHWhByrix+jiTVa9fpDYvl0ZOr1IaJw/tL5STcm8c8bDbxwfqRuX0KswYDwDeRGaSL5qALCtK34eYZhwkdvRB3BYhDiC28+ir/rV07fV/D+cUoTKjpuGQlVCrGmieW8YlNSUlBWVsYbexPhie7ubgQCAYHwBHFr0Sqp0hBHsTMzM+jq6oLD4QiLYqUchFwuF7KyspK0+8RAi3Oqo6OjAkF9Jbxw/fXXo729HW+//baq99QMqSqBnP0bAelAjFVkX0yq9Jzsjh07UFxczD9XVV6M02NTUdekhR0ijdGQf4u7fYEl8iQRqdQYDYFafV8AqoXzgwFWlP6VHqWhhfMBYccvIUZe4EGUDpYUeODt25Y6UYGlaFUqijQYDRHrsFJQKp6/Grp+Y1k/GmJNE0eKYonC0caNG+FyuWCz2TA9PY2+vj6kp6ejoKAgdIO1CsT/aZhMJlitVuzYsYNPgdvtdr4Wm5OTg/z8fKSnp6OwsDBm8YdHHnkEDz74ICYnJ7Fz507827/9Gw4cOCB7/Pz8PH7wgx/gmWeegd1uR1VVFR566CF85jOf4Y8ZHx/H7bffjpdeeglutxs1NTV4/PHHsW/fvoh7YdjwG/pkgeyD1PqV4oYbbsDzzz+PN998E+Xl5arec1WRqtlslrbkOmtS3tfXh82bN6v+IRDQpBoMBtHa2gqn04lDhw5FvXtUI0MoNUYjhlRjEkG8+r5Lz4ei1XiF80PPCYmVFs4HlshySc83KPg/gVgAQgw58fxgQPo1kcZoyHkAZH5VefOQ2q7faMS6El2/UjO+8UIJwQLyzU6k+VAsPFFRUYGqqioEg0F+dnR6ehosy6KtrY2P9tRkopIBWqFN7Kfq8Xj4c7vrrrvw5ptvwuPx4NSpUzj//PMVl61+//vf47bbbsNjjz2GgwcP4qGHHsLhw4fR09PDzyHT8Pv9uOiii1BUVISnn34aZWVlOH36NHJzc/lj5ubm0NjYiAsuuAAvvfQS1qxZg76+PkV+1atZ+5fjONx444149tln8frrr/NSjmqgGT9VlmURkLkwErzzzjuora0VfFBok/Ldu3cLPhhq0draivT0dJSWlqKpqQkpKSnYuXNnxMYoOlqV800N/TvcOzX0d2zRquAxKjJV6p+66Fw6gO72pRuLyDXc4w0I07XU8eILNC2eL34uVDulfU2FJEF3/ALhIvfRxPND7xEuNyiowUbo+hXvSekIDaA85Rtr12+impGUvl9864f2+r+/36/oeLHwhDC7YeT/DA4Owul0IisrCzabDYuLi8jMzBTIJ2otkp2cnMT4+Dj27pW/4QBCDUqvvvoqvv71r6O0tBSjo6NoaGjALbfcgr/927+N+NqDBw9i//79ePjhhwGEfp4VFRW48cYb8f3vfz/s+MceewwPPvgguru7Zctu3//+9/HOO+/grbfeUnimS36qTx6bQ3qG8qhwOeF2OXDNJ/MU+6led911eOqpp/DnP/9ZMJuak5Oj2DReM6TKcRz8/siSfe+99x4qKyt5JSSPx4OmpiZeFDzeu9aOjg4EAgHYbDbFFnBSpBr6d2RDciCcWMWkCiwRKx2ZEmIVPCZSU6Ifc3vChfMBCGqqUsTq8zEikpUmVr9o1EbqNYRMw03HWeHzogs+Ec4XvObsGiTtK0USUqlg+fqpxOtViucrIVUxocZKbvEJ+K8MoYqhhmDpkR3683L69GkEg0F+NM7v9/ORns1mg8FgEMgnasFTdXx8HDMzM9i1a1fUYzmOQ0lJCU6cOIHMzEy89NJLqK6uxuHDh2Vf4/f7kZ6ejqefflpgTH7NNddgfn6eNzyn8ZnPfIZPOf/5z3/GmjVr8MUvfhG33347n67fsmULDh8+jLGxMbzxxhsoKyvDddddh2984xuyeyGk+vir2iLVr16knFTlbsoef/xxfOUrX1H0nqsq/UunZ0n9tKSkBJs3b467gYHjODidTszPz2Pbtm0x+bRGmk0Vi+ZLHnM2HSzV7RvLGA0tnE8s3RIpnE/GaMRpRfFr6OhU7EYUJqwvalISP0bWkBPPl+vO5VhO4HBD7zf82NjF84Fwgo0knE/vb7mRLEIFgE9deYL/dySClRvZIWMra9asQTAY5BsXi4qKUFJSwhsEzM7OYmRkBF1dXcui46sWatSUGIaBx+NBZmYmqqqqcO2110Z9zezsLBiGEfR7AEBxcTG6u7slXzM4OIjXXnsNV199NV588UX09/fjuuuuQyAQwF133cUf8+ijj+K2227DHXfcgRMnTuCmm26C1WrFNddcE/mcOQNYjTQqqd1HImLMVUWqZrMZwWAQw8PD6Ovrw6ZNm8JapWMBSSE7HA6sWbNGFaFWlRejdXjJhSbdFPq3O5iKdLM35tqqWtEHr09aOJ9AbowmFuF8juXCxmjo58hrfF5pcXxycSO1U6nuXnFUSV4fqfubk5QbDP+SGA1GxV2/ZD0lggvxNCkpRbKblOTXV74vpQQLgI9I29rakJWVherqall94qysLOTk5IR5qirpul0uqLV9A6Ba+1ctWJZFUVERjh49CpPJhL1792J8fBwPPvggT6osy2Lfvn247777AAC7d+9Ge3s7HnvssaikqsU51ZWEZkhVyV2k0WjEmTNn4Pf7sW/fPkVF82jwer1oamoCAFRVVcHjiWzTFg1uJi1MNJ840bgDVqRblI3TCPYoGqNxukn6V1g/jSZDGK9wPsOwCPhEZCeKPGkf1UjC+XKvlXOhAUJ3kXKzxFLi+XJNSpLNbgq0fiON05wLXb+RoESCUQo0wQLhJOvxeHDy5Enk5eVhy5YtgutEJOEJs9mMkpISXseXCE8MDg7yXbdEeCI9PX3Zoli1tm+AOlItLCyEyWTC1JRwEmFqagolJSWSryktLYXFYhHsa/PmzZicnITf74fVakVpaWmYGfnmzZvxxz/+MeqeWA11/yb4nlYRNEOqQPhFlobH48Hs7CyMRiPq6+sTYpo8Pz+PpqYmFBYWYsuWLRgdHeU/2GqwY12aIFolIOTpDlr5+umCN7RvmjjFjUlS9VN6jEYMNWM0csL5cqA7foWPC7t9AQg6fuluX4DS82UY4WgN1R0sN3dqMBoETUorIZ4fb9evVgh1tYEm2T89vhUnT55EYWEhNm3aFEZ8sQpPeDwePoodHByE1WoVyCcqJUElUEuqKSkpqmrBVqsVe/fuxbFjx/iaKsuyOHbsGG644QbJ1zQ2NuKpp54SzP329vaitLSUb8psbGxET0+P4HW9vb2oqqqKuictzqmuJDRFqnKw2+1obm5GamoqcnNzE0Ko4+Pj6OzsRG1tLaqqqhSLSyhBMmQICbHKjdHQxEpGaGhIjdGQFG+8wvlAaBRGrO+75FIj/zOP9JxcWlaOqNSI54e9l8oGJQOV8lPjeKP0fdVgpWq3iU51X/bVDgCpAJz4399HvjiqEZ6wWq1Yu3YtysvLBcITvb298Pv9ksITsYJhGMUNlC6XK6ao+bbbbsM111yDffv24cCBA3jooYfgcrnw1a9+FQDw5S9/GWVlZbj//vsBAN/61rfw8MMP4+abb8aNN96Ivr4+3HfffQI/1VtvvRUNDQ247777cMUVV+CDDz7A0aNHcfTo0ejnrKFINRn70DSpchyHkZER9Pb2oq6uDn6/P+70LMuy6OnpwZkzZ7B7924UFhbyz8VCqhzHYWBgAFNDQyiuPU/2OLUyhFJuNPxzVGQabT7VH2BhtRjhdjOSovlSbjRKhfOB8LlR6dopId3owvkABOL5ctkLsXg+LZwftsc4xfNjFc6PhJUWz1fznrGtvbxXr2hpYhpyzU5ytVgiPMFxHC+fODMzg76+PqSlpQnkE9U2RKpRgYpVovDKK6/EzMwM7rzzTkxOTmLXrl14+eWX+ealkZERwR4qKirwyiuv4NZbb8WOHTtQVlaGm2++Gbfffjt/zP79+/Hss8/iyJEjuOeee1BdXY2HHnoIV199ddT9nOs1Vc2M1ABAIBDg62UMw6CzsxMzMzPYvXs38vLyMDQ0hIWFBUXt6VLw+/1oaWmBz+fDnj17woT7p6amMDAwgIaGBkXrMQyDtrY2zM/PY+/evRiyCe9R5GZTQ/9e/jEaQqJKx2h8PqEIBBB+ISbEGvAHRSM24dGiYCZV9DEjSkpy7yOVCqbF88WvidT1G004f+nxcGJVQmhyBBuvxVvYazRcU11uUo0EpeM6QLjwBCu4yTPCYDDwAvjBYBBzc3OYnZ2FzWYDwzCCkR0lEWhrayvy8vIUNVS+8sor+NGPfoSuri7F56MlkJGah59bQJpGRmo8Lgdu+JscxSM1iYAmI1W6eaihoYFP90aTKYyExcVFnDp1CllZWTh06JBk918kQX0xyIysyWRCQ0NDSIosC5K1VYJYo9Voj/n8QIoVcLnJKIrwIhppjMYj4UIjN0YTes4Ar0fanDz03qyATAnEpuNSwvl0d6+4SUlOthCQNjmXEs6XWyd0vERq+BwRz48HySRTArXdxIB8FEt7xRLD9TVr1vAjdzabDRMTE+jp6UFGRoZAeEIqIlVbU13tYvqAnv7VHKnOzc2hqakJa9aswdatWwUfVDmZwmiYmppCa2sr1q1bh5qaGtmahdL07/z8PE6dOoWioiJs2bJFNr0TaTZVSW2VhjgN7HSdJZuzHxqfhG5GpDEaNcL5BqMBXop81QjnAxQxSgjnk+dlZ1JFHcE0WBkCliMStV2/sYrna6VJaSUiVK1BTZoYiOwVS5Os0WhEeno6MjIysG7dOl4kxmazoa2tDRzHCaJY0vBzLpIqyyan61YK53z3LxnarqurQ0VFRRj5qa15knrn0NAQtm/fLttirmZ9qQYnGjvWpeGtHiAnRRixuv0WpFsDktGq229EupWVjFa9foOkDCG/5xjHaJQI55tMRp5Mw7xRFQrnh97LIEj3hgtAyLvUsAwruOgRqBXPV/q5kSKic1k8PxqUNGQlE4mIYunGJwB8epgIT3AcB4fDAZvNhrGxMYHwhN/vVzWnqpNqYnHOkyrLsti7d6+sIaya9G8wGERbWxscDociQXyyvmx6kOPQ09ODsbGxsAYnKRAnGmCpbrrgCdVgpOqnDg81YiOqn0qPzMQ/RqNWOF+KWFdaOB+AKvF8KeH80N+JH6MJvU9ixPPl3lcplkM8PxK0SKhiqCFYIHIUKyc8sX79evh8Pl4+0ev1oqurCzMzMygsLEReXp7syIzL5Vp24YeVAAtAK0mSZHwqNUWq1dXVESMKpZGq2+3GqVOnYLVaUV9fH1EQn4bRaOTrK3QEGgwG0dLSArfbjfr6+qh3kx+rA97qCX880TKEBHJjND7fkuYvgIgdv6F1jHC7/WePkR+jAZaE8+VNx0lqdolk6JlVJsCEdfsSRBPOF6dlaSWmaOL5vEKSiPwSKZ4fSQhhpcXzxT+T5cBqIFQxYkkTA6FrEN1MKSc8UVxcjNLSUrz11ltYv349PB6PwO6NpIkzMjL4a81HhVSJ85AWkIx9aNvxVwQlNVWbzYbjx48jPz8f+/btU0yowFLah/5FuFwuHD9+HBzH4dChQzGlZywm+YuO1Rz+SzebJB4z088bwh4zUb9Jf4CF++wsKiHUSPB4A/wfuoHGZFr6N+9/6g8i4A/y5Eo/R8CIIkY6RRgMMGFRKE+uQZYfoxGn1VlKj5df12AMIwyD0RBKA0sQU8jejU5LG8+Sa+K/eLESKhA6L/In/n0s7xjNaiRUKXzqyhP8n2ggEazFYkFKSgqsVqtAoYhhGASDQfj9frAsi+zsbNTU1ODgwYOor69HcXExFhYW8OGHH+Ldd9/FBx98gN///vew2+1hEwlKwHEc7rzzTpSWliItLQ0XXngh+vr6or7ukUcewbp165CamoqDBw/igw8+EDx/9OhRnH/++bz7z/z8vKL9sAzAaOQPG7/sgGpoKlKNNvRMIlVxJAkIZ1pj9VSlvxRGo5EX7V+7di3vjKEUyxGtRksD+3ysYGQGkBbND52rEW65eqmMcL4Y4mYmYMm+LVyC0Cisq0o0F0UTzhfvTaquSp5bCfF8tTXVlU/Jrt651GQiEWlilmUxMTHBS2v6/X4YDAZYLBaUlpairKwMDMNgfn4eb731Fj9jWlVVhdraWlxyySWoqalRtN9/+qd/wi9+8Qs8+eSTqK6uxo9+9CMcPnwYnZ2dskI5SjxY3W43Pv3pT+PTn/40jhw5omgvgD6nqqk5VXKHJwefz4e//vWvuOiiiwQddSzLorOzE9PT0/xMayzgOA6vvPIKzj//fExPT6Onpycu03MxqYpnU4HYvVNdHk4gmk9StTSpys2n0mM0jGhWlQYtnE9/TMQXa5838sypuHaqaCY1gng+IdMwv9YIJCJeK1LXb6yzqVppUlLzHrGt/dEl1GhQOhM7MTGBrq4u7NixAzk5OVG9YjmOw//5P/8HFosFwWAQb7zxBj7xiU/gf//3fyO+D8dxWLt2Lb797W/jO9/5DgBgYWEBxcXFeOKJJ3DVVVdJvk6NB+vrr7+OCy64AHNzcxH9qsmc6k+emkdqujbmVL1uB37wxVx9TlUOZLaUblP3+XxoamoCy7KCmdZYQNKO3d3dsNvtcYv2R4tWaUiN0Xh9QOrZ+XIyQgNI108JEjVGE0k4n464Av6grHA+cHYmVUI4P9JMKiBfC5ESzhendWnEIp5Pp131rt9waL3jdzmhJIqdnJxEV1cXdu7ciYKCAv7xaCM7RqMRF154IW677TY4nU4MDQ1F3c/Q0BAmJydx4YUX8o/l5OTg4MGDOH78uCSp+v1+nDx5UhB9kvc+fvx49B9CFLAMJxCWSSaSsQ9NkWq09C9JsZAL5cLCAk6dOoX8/Hxs27YtbiFsYpK+uLiI+vr6uHU/Q84RSz6HUmTq9hmQnhL6xdOi+SQidZ4VdKDTvMs5RhMMCOX+xML5hOxoeUFAWjhf3O0LLJGlnJiDnHA+WVMKkibjstKEie/61Qqh6lhZEIJ9+ld1yMrKgsFgwNTUFDo6OsIIFYg8srOwsIB3332XF6zPzMzE9u3bo+5hcnISACT9VMlzYsTiwaoGLKeh7t8k7ENTpBoNRPQ+GAzizJkz6OjoQE1NDdatWxe3dRNRXDIYDNiyZUtchMpxHIaGhjAwMIDt27fj5NSSRJlUmtfhDn3ZlIrmC56XGaMhnb9EPD/WMZrQc0vEGgwwkt2+BEZqJlWq9hhplEau6zcSGUUaoyHnQtZQSqixiucnQpYwXkJdSePzcy1KlcKDd1jx4YcfwmQyITMzE3Nzc9i2bVvUkTtgKTp1OBy48sorsWvXLt7PVA7//d//jX/4h3/g///CCy/EfQ6JBsNwYapuyUIy9rGqSBUI3eUNDg5iZmYGu3btwpo1a+Jec3p6Gi0tLVi3bh3OnDkT11osy6K9vR12ux0HDx4MzcdSVodKTcfViOaLx2hIlOrzLV304hmjIa+nx2gAOl0qP0YD0DOroeeNBqOA+OQEGwBp4XzBe4v2Itf1S/YV+r9881I84vlyM6rJUDbSCXX5QdK/LMtiaGgIg4ODSElJQXt7O8bHx1FYWIjCwsKIEwNOpxOf//znkZGRgeeffz5q9+/f/M3f4ODBg/z/fT4fgFBWrLS0lH98ampKViM9Fg9WNTjXG5U0RarRos1AIMCLXCuZF40GcURZUlKC6enpmO3fSH2XjN9YrVYEg0F8aksQ/9u59GWRsngjxKqm21fsRkM3JkWDwQi++1eq25c8Diz5pMp1CfNkKqi7LhHMEtkuvQ/pzhWYkotSxGSMRmwULqempHhkRY78ZKLESN2+gtcngGii1XO1AJ1QhfVUm82G4eFh7Ny5E0VFRXC73ZidncXs7Cz6+vqQmpqKNWvW8OIPJA3sdrtx+eWXw2g04rnnnlM0TpOVlSUQsuE4DiUlJTh27BhPog6HA++//z6+9a1vSa4RiwerGuikukrgdDr59GxdXV3chMowDNrb2zE3N4cDBw4gJycHgDpRfRqLi4s4efIkcnNzsW3bNsE4SKSbhVhEH3x+DilWA9yes0PoMYzRkP9LCeeLx2giqSqJO4ZpYpUyQKfXYhhGsklJ/POPJJwvfi09SiMZiUoI5yvt+BU3ZEmtKQW1ozRa7foNra8TKk2os7OzaG1txbZt2/hRlPT0dFRWVqKyshIMw8Bms2F2dhYdHR3w+Xx45JFHUF9fj2PHjoFlWbz88ssxiz4YDAbccsstuPfee1FbW8uP1Kxdu5YnTAD45Cc/ib/927/lSTOaBysQqtdOTk6iv78fANDW1oasrCxUVlbKqt4Bevp3VZDq9PQ0WltbUVlZCZvNFrdKBu2CU19fL7BwisVTlaSPq6ursX79+rCuPgD49DYvXm5f6kyOJQ3s84NP8xJCFRwr40YTDLCSTUliEMIjYzRSz4XWMArGaKTmS+m6qRqlo0hjNJJdwrLzp+rIb7WL56t5j9jX1wlVilC3bt0a1vRDYDKZUFRUhKKiInAch+npadTV1eFnP/sZFhcXsW3bNjzwwAO45JJLcPDgwZiaLb/3ve/B5XLhm9/8Jubn53Heeefh5ZdfFkxCDAwMYHZ2lv9/NA9WAHjsscfw4x//mP//xz/+cQDA448/jq985Suy++GgIUUlrPw+NDWnynEc34FL/j84OIjBwUFs27YNpaWl+PDDD1FcXKzIn1AKpGO4oKAAW7duDfsQnzx5EmvWrEFlZaWi/Q4PD6O/vx/btm1DSUkJ381HvBlp0KQKRJ5Ndbo5QeqXBH1Ss6mh55f+TaeByRhN6Bg6wqTXEY7RCPxURR8Pv2gmVUw0YT6poteLZ1Jp4fylNYUXb7muX6larBJrN6n3FzyW4K5fufdR8r6JXD+ROBcJVpzybWlpwebNmwW1zGjw+/340pe+hLGxMfzhD3/AiRMn8MILL+D111/n7eRWK8ic6vcenUFKmjbmVH0eB/7pW2vO3TlVmoRoA/CDBw/yP5BYIkmCiYkJtLe3R+wYVro+y7Lo6OjA7OwsDhw4gOzs7IiECoRHq16/IUzUAUBM3b5SYzRebzBhYzRySkl0SpQJSMys8k1M0qlbvluV+pHHIpwPhCJg6Wg28WM0SrGSXb+6eP7ygSZUu90eE6EGAgF87Wtfw/DwMF577TWsWbMGNTU1+MIXviCpErdasdpHat588008+OCDOHnyJCYmJvDss88KUunRoDntX4PBAI/Hg/fffx8+nw/19fWCO4xYSJXjOPT29qKjowO7du1CdXW17AeYiOpHgt/vx4kTJ7C4uMg74EQjVIJPb/PC4TYKxmgAaW1fGrS2r9ksrc9Lun09bgaBAF07pNcR6vCSVC/RzF1aV3gc/VyYLi/DCuqdRqPo+UjdvQFGcpSGCTKS2rfKm5EMZ4lG+RiNWkIl2sG0GEIse01UQ5JYG1lHYiAm1ObmZmzatEkVoQaDQVx77bXo6urCq6++Gja18FEhVGBJjEUrf9TC5XJh586deOSRR2I6f01FqkDoQ9vU1ITi4mJs3rw5TG/XbDYrtn8DQh/m1tZWOJ1OHDp0KGpDQDTSdjqdOHnyJLKzs7F9+3ZBQ1I0Qo0GNd2+8YzR+P1Ls6biGqvYy1ROE5ic51IzU/gYDbAkEiHW4lUzRgOcJb04xmhCj8k0L8XoRkOvK/f+SiDn16oWek018aAJdW5uDs3Nzairq8PatWsVr8EwDG644QacPHkSr7/+ekLGVrSM1d6odPHFF+Piiy+O+T01RapkxrOmpka2pqkmUqUt4MiISzTQ1k5izMzMoKWlBVVVVdiwYYNkQ5ISXHHAjf/5YKl9Xo3oAxF1CJ2f8ojdJ6qXyo3RkOdiHaMBQhdfqTEaIESucmM0vAm0ioYkKUQ6VryfSGSmZJRGjmjUpmOVNEpFgk6oiQdNqPPz82hqasLGjRtRVlameA2WZXHrrbfinXfewV//+ldVZLxaoUXrN4fDIXg8JSVF0KCaSGiKVI1GI84777yI0Z7JZBI0M8mBRLylpaXYtGmTYtIzmUz8QDUBx3E4ffo0+vr6sHXrVpSWlipO96qFnOiDz79090fs3OS6fekxmmCQFRBmpDEar0e6Y1esqCQGTaxStVNCkuQ5JWM09HFSozSxjtHQ68o1QCmBEpJRM0qj5SYlnVBDhFpbW6vKXINlWXzve9/Dq6++itdff11R8+NHARy7lBVLNsg+xI2td911F+6+++5leU9NkSoQOVIkz0dL/46OjqK7uxt1dXWqP8jiSJh2wNm/fz9ycnJ4+7lYCdXpdKLU9xYmUg7zj9HRqtfHITXFEOZEw+8xgmh+LGM0XiICISN8H1rDKLB/UzJGAyxd8MlMquA1IrIVg5CelE2cFNSM0XAsp4/RKIScWMZHFTShLiwsoKmpCTU1NaomDliWxQ9+8AM899xz+Otf/4rq6url2KomwbCsYNIgmSD7GB0dFfTmLFeUCmiQVKMhUvqXZVn09PTgzJkz2LNnT5igtRLQ4g9+vx/Nzc0IBAL8PGu8EarNZuNnbs9b78IfTiy10Ht9S2M0Lg9J/Srv9iWPK3WjISM0cqL5gDAVHPr5LHX3LnmoLpEygLDXC9K9FEGSMRopwQ051SS5WqzSkoAUCcnVNHVCXYISfeOPAmhCdTgcOHXqFDZs2KCKUDmOwz333IP/+Z//wV//+lfU1tYux1Y1C5bl4uqYTyTIPrKzs8/NkRolMJvNkhfQQCCA5uZmvmNYieSXFAhpEwWnzMxM7N69W9AVHCuhjo6Oore3F1u2bOE7B4kLTdg+ZJqS5Ig1EGBhsRgVjdFIiTuIiRVYmklN9BgNIJxLDa0Z+xgNIBXNLs8YjZKobaXF8/VRmsRATKgnT57E+v9/e2ceF1W5//HPgMgiCAzMoLKKKIsgm2zu3SxTQEi7mZnaetO0q7ZZlmk/by7X7JppWd2ScrluLJaaaSCWSxogJKugIArMAsq+zXJ+f9CcZoYZZs4wwwzwvF8vXy89c855nimYzzzP+X6+H29vRrtdFEVhy5YtSEpKQkZGBvz8/AwxVZNGKjWh6DcjiLvJiaomsVK1UpUJ4LBhwxAdHU3nruqCubk52tvb8dtvv8HDwwM+Pj46FyTJkFl6ampqEBYWppDR+vyMNnyT+VcijjbVvsppNLIfYGUbjbKwKoup8ocyvYJV0Z5QWVilEmm3at+/hFZ143z511Qhb6ORFxxKSqks/OmpM5My+qz67SlPtK9tNF3zMbxHdTAJqiyxavTo0XQUmzZQFIUdO3bg888/R3p6OgIDAw0xVZOHoihITaxQiQnNzc10a0agK7M2NzcXbDZbqy9YJieqmlB+piqryHV3d8e4ceN6XTR0//59NDc3IygoCKNGjaIFVdfVqVgsxo0bN9DW1obIyEiVK2hthVUeWbFST8iEtU2pC5ImG43iPXq20QB/feDKqmXlZyYvrFKxVOUWMaCi36+s+EhNKHlPNhrluTERVG3Rh41Gf3MhgtoblAU1Ozsbnp6e8PLy0voeFEVh165d2LlzJ3766Se1CTGDAV39oYZAl3lkZWXhoYceov/92muvAQCWLl2KpKQkjdf3O1GVbf8qV+T2tlRdKpWiuLgYPB4PVlZWGDVqVK+fn7a1tSE3NxdDhw5FREQELCwstL5WlWdVPommp6b5sm3cjg6JWp+pvmw0Xa+babTRKO8uyO4hL6aK4t5ztqryKliTjUb+HE0ipslK05ONRtNc9ElfjDOYBFXmQffw8GBUWERRFPbu3Ytt27bhxx9/REREhOaLBjD93ac6Y8aMXlmCTE5Utdn+FYvFyM/PR21tLSIiIuDg4NCrMeWfx44fPx4lJSW9FtSGhgbk5uaCw+FoZelRXq3KUE6iYWqjUSeQXa/1nY1G+f6AehuNJkGVIbPSaNvvl8k2KVNB1XUcU/Wldo0x+ATV3d0d3t7eWt+Doih88803+OCDD3Dq1CnExMQYYqr9iv6+Uu0tJieqmpBIJJBIJGhqakJMTIxCEoMutLS0ICcnBzY2NoiKikJbWxs6OjpQVFQELpcLNpvNWFT5fD4KCgowZswYeHh4aH398zPa8Olp1e9Hk42ms/3P5g49VPvK0NZGo2orGJCzySh1rDCGjUaVoGpqnq9LowV9N89nOr4u9+8tA9lK88lGWwiFQrDZbLS3tyM7Oxuurq6MBXX//v1Yt24dvv/+e0ydOtWAM+4/dK1UTePnhkS/aaCxsZGObJs4caJWHZJ6oq6uDrm5uXBzc8PYsWMhlUphbW2N0NBQCAQCFBYWQiKRgMPhgMvlwsnJqcdoJllqTXl5OYKCgrr199QFbWw0os6/hMXQNhrZ6/JpNMrXd9vqVWOjAf5arapbnaqz0Wjb6EE2rrYQG40iA1FY//eZN4RCIYqLi9HZ2QmKosBms+Hm5qb1F2CKonDkyBG88cYbSElJUXgGN9gxxY5KfYnJiaq6H2oej4cbN25g9OjRCpVZulJZWYmSkhL4+/vD1dVVoSDJyckJTk5O8PPzQ2NjIwQCAUpLS3Hjxg04OTmBy+WCw+EoPCOVNYm4f/8+IiIiYGdnp9O8Xp3Trna12hN9ZaORdTfSh42m6z5mvbbRALItJ8PZaLrupV5c+tpGw2TM3jDQBFW25evk5AR3d3f8/vvvGDZsGCQSCS5evAhbW1twOBw4Oztj+PDhaj+PUlNT8eqrr+Lo0aN49NFH+/ItmDxk+9fEoSgKt27dQnl5OSZMmAAul4uysjKIxWKdVqryDSLCw8Ph6Oio9vkpi8WCvb097O3t4ePjg5aWFggEAlRWVqKwsBCOjo7gcrlwcHBAcXExJBIJIiMje70lrSysqlarba3ds0/l6clGI/+DxtRGI498QY8xbTR/vTfFVZW+m+erW7WZygeIvhmoggp0FRHm5ORg5MiRtGugs7MTdXV1EAqFqKyshJmZGZydncHhcBR2qX744Qe8/PLLOHjwIGJjY431dkwWiVRqOtu/RvgZNmlRlc9UlUWsAbpnqopEIuTl5aG9vZ1+HqttQRKLxYKtrS1sbW3h7e2NtrY2CAQCVFdXo7i4GEOGDIGnp6fOWa+akE+ikUddEk1PNhp5sZJvPSiDiY0GUCG2WthoVBYp9dJGIz8/qXJCgR7Qh6Dqy0pDbDTMUBbUrKwscDgcBRve0KFDMXLkSIwcORJSqRT19fUQCoUoLS3Frl27cPnyZXh7eyM1NRXfffcdo4zNwcRgX6maZJ4qAIVM1UmTJilsp6rrqtQTra2t+O233wCAXk32poevtbU17Ozs0NraCnd3d4wdOxYNDQ24cuUKLl++jLKyMjQ2Nuq8p//qnPY/5y2h02jM5bb+zORyVOWzTztFEgVBVcxIZSkcV5efKn+dWCTpylNV+u/DMjOjX1MeB+gS1m6Cx2KBxWJ1s9HIN7hX9WxV3kaj6nj386Xd3p+6vFR6vmYs+o+6e6pCNg7TLFMiqH2HsqBmZ2eDw+HA19e3x1xlNpsNX19fTJ48GYsXL4anpyeOHDkCkUiETZs2Yd26dfj9998NMudffvkF8fHxGDVqFFgsFtLS0jRek5mZibCwMFhaWsLHx0crT6UhkD1TNZU/fY1JrlQfPHiA69evg8vlIiAgoJsdhelKVZZYM2rUKPj6+kIqldLXyz7omVJVVYXi4mL4+fnRUVBubm4Qi8Wora2FQCBAVlYWLCwswOVy6W1iJmO9Oqcd244reltNyUajXKGrzkYjf45a+4uWImNIGw3Qu0IlbSGC2nfIC6qsytfJyalHQVUFn8/HqVOn8PXXXyMhIQHnzp3DyZMncfDgQYP4UmVB2c8//zzmzZun8fzy8nLExsZi2bJlOHjwINLT0/Hiiy9i5MiRmDVrlsbr9YlULIXE3DR+jnrKbTYULMpUyrT+pLm5GZmZmRg3bpxaO8qlS5cwduxYcLlcjfe7d+8eioqK4OfnBzc3t177TymKQllZGe7du4fg4GCw2Wy150qlUvoZjUAgAAC6kpjNZvdYSSyPsrACUGmjARRLyJWfa8hsNLL3ofC+lLaDlcVF9rpMMNVdLxM7VR/+mmw0yvfSp41G4RiD56qDtfJ3IIirsqBmZWWBzWbD39+f0e/+pUuXMH/+fHz00Ud46aWX9Br1qA0sFgupqak9bjevXbsWp06dQn5+Pn3sqaeeQn19Pc6cOdMHs+xyZ9jb2+OJ1YWwsNStUFPfiDqacHxnABoaGgZvQ31bW1vExMTA1tZW7TnaxL9RFIWSkhJUVVUhLCwMbDa714IqkUiQn5+P5uZmREZGYtiwYT2eb2ZmBg6HAw6HA39/f9TX10MgEKC4uBgikQjOzs7gcrlwdnbusV/x2idE3YRVJqjA4LLRqKK3gtrb1elAE1Sg/1tp5AW1o6MD2dnZcHR0ZCyoV69exRNPPIHNmzcbRVC15cqVK5g5c6bCsVmzZmH16tV9PheJWAIzc8PUljBF2UvfF5icqALoUVABzc9UxWIx8vLy0NraiujoaFhbW/daUNvb25Gbm4shQ4YgIiKCceUxi8WCo6MjHB0dMW7cODQ1NUEgEOD27dvIz8+Hk5MTvYrVdG/lJBplmNhoKIqCqEOs8G8Z2thoACjkqCqjzkbT9Vr3/4eabDSAomAb00bT01jajM0EYqXRHlWCam9vj4CAAEa/+zk5OZg3bx42bNiAFStWmKygAl2WQxcXF4VjLi4uaGxsRFtbG6ytu3drMxTEp2qCKH+4K9PTM9XW1lbk5OTA0tISUVFRCgKsq6A2NjYiNzcXTk5O8Pf31ympRh4Wi0Xn+8msOkKhkK4ktre3p5/Dyn4Z1j4hwgcHlIuFVHtTTcFGA6i30si+PSrnmMpsNLK/03PopY0GYCaoPd1f0zjajq0rRFB7Rl5QOzs7kZ2dDTs7O4wfP57R735eXh7mzp2LtWvXYs2aNSYtqKbGYK/+NUlR1YS67V9ZgdOIESPg6+sLAL0uSBIIBMjPz8fo0aPh5eVlkF+uYcOGYdiwYfDy8kJ7ezv9DLa0tBS2tra0wL6/aBj+76Byle5fwtrZKVEST+PZaGT30cVGI5sPoO75qfY2Gk0Vv5qEVR8C019sNF1jDCxBtbW1ZSyoBQUFiI+Px+rVq7F27dp+IagjRowAn89XOMbn8zF8+PA+XaUCgEQqgZmBrIVMkUjJ9q9WqNr+raqqQmFhIXx9feHu7k5v97JYLJ0zUO/cuYPbt29j/Pjx3bZWDIWVlRXc3d3h7u4OkUgEoVAIoVCI8vJyWFpa4ukoFxy6Ok7hmk7RX2k03VelisIq/9xUVZWwbCsYUJ1Go/BcVWksVR/6sg8kiTrvqRZpNMBfq1VNNhr5c7QRsJ4SaXoSGF0SaYigGg5Vgjps2DAEBgYy+v0vLi5GXFwcXn75Zaxfv75fCCoAxMTE4PTp0wrHzp07Z5QG/5TUdBqi6HGTSGtMUlSZbP/KAsDv3r2L0NBQODk59fr5qSwGTigUIjw8HPb29jq/l95gYWGBUaNG0TF0dXV1EAgEmOaagV+q/oaODrliJYUmEN2FVWajUbcVDHTZaLo1wFdho9HUIJ+JjUZbywttpYHmyl/ZPZW3l9Vh6jaavmCgCKpIJKIDMpgKallZGeLi4rBkyRJs2rTJqIKqKSj7nXfeQVVVFb777jsAwLJly7B792689dZbeP7555GRkYGjR4/i1KlTfT53qUSqspbCGBhjHiYpqpowNzdHR0cHxGIx/vjjDzQ3NyM6Opru4dkbQRWJRPjjjz/Q2dmJqKioXrcc1Bfm5ub0NrBUKsWEB7XYkeaodE53YZU9X5UXR2XRk0qk3Sp95YVOlzQa2Xna2Gjkr9W3jUZTIg2x0XShbWGWqaEsqNnZ2bCyskJQUBAjQS0vL0dcXByeeOIJbNu2rdd1E71FU1B2TU0NKisr6ddHjx6NU6dOYc2aNfjkk0/g5uaG//73v33uUQVA91E3BYwxD5PzqQJdvxw9/ccoLy9HXV0dOjo6YGFhgeDgYFhYWCg0xddFUFtbW5Gbmwtra2sEBQX1aHMxFdZ9raJASM5Go+xVVf7f3anUzlBZZOS3e5WvVyeY8jaav+4r/zxW9cq1p8pfZXExhI2mp/tqM47ac/uBqP41nml8GGqDqhXq0KFDERwczEgUKysr8dhjj+Gxxx7DZ599ZnRB7a/IfKqPLrkKi6E9Ozj6ClFnM85+FzW4faqA+qQaGbLG166urvD39weAXlf4PnjwAHl5eQoNtvsDm18wUxBW5crfbs9N9WCj0TWNBoDKRBp92Ghkr3U7xrDq15QSabrm03dWmv4qqGKxGNevX9dJUKurqxEXF4eHH34Ye/bsIYKqB9S1GzUGxpiHSYpqT1RXV+POnTuwsrJCQEAApFIpLQq6/kJUV1ejqKgI48aNg7u7uz6n2ydsfsEMr+/pVDim3H4QgIKNRuFcLWw0MiHqTRpN11imb6NRNYamsQwNEVTViMVi5OTkYMiQIZgwYQKjzwAej4fY2FjExMTgyy+/1LrDGUEDJmSpAWmorx6KolBaWorCwkKMHj0aQ4YMobd7e1PhW1ZWhpKSEoSEhPRLQZWxY0X3hhHyqxxRp1hBLJUbwLNYLKUG+Wb0qg34U1hV2GhkyNJoVK3wlbfyWSyzLnFV8QyWZcbqoXOSYiVyl/BpL6iy96EN+kqkkf3pDURQFZGtUmUrVHNzcwQHBzMSRaFQiPj4eISGhmLfvn1EUPWIlJKa1J++pl+sVMViMW7cuIHGxkZER0ejvb0dlZWVqKurg6Ojo84tBwsKCtDY2IiIiAiNXZz6AztWDFW5YlVno5FfIcon0TCx0ZixzLq3KNRgowHUb8tQUkonG438eZq2XeWFtXuPY802mp7mpHKeJvxMtb8J6k+HJwLo+tm6fv06zMzMEBISwkgU6+rqEB8fD19fX+zfv79f1E70J6QSiUGiF3WBbP/+ibxItre3IycnB+bm5oiOjoaFhQWd/CJrHi2rimWz2VqtWDs6OpCXlwcWi4XIyEidws5NFXlhVdUxCdAtjUYbG41CxbAOaTTKAtKTjaa39IWNBiCCqk/WLW/BhQsX4OTkhKamJlhYWDAW1Pr6eiQkJMDT0xOHDx+GhUX3sApC7xjsHZVMsvpXIpFALBajoaEBOTk5cHZ2RkBAAIC/thJlwvvgwQMIBAIIBAJIJBK6f66Tk5PKX7ampibk5ubCwcEBAQEBA3bb55//ae12TPkHrEOp8lf5dVU9fY2dRtN1vnZbsz0JGpMVqqZx1J5rwoLafTzTFtizRyIglUrx4MEDFBYWorOz64sjm80Gh8OBs7OzRvtbY2Mj5s6dCzabjbS0NJOxyw0UZNW/UxLTMcSi57CRvkIsasHFtIdJ9S8A1NTUID8/Hz4+PvD09FRbkMRms+kw4YaGBggEAty8eROdnZ3dUmBqa2tx48YNeHh4wNvbu99U+OrCrjU23YRVtpqUrWBVJdF0bQX/VRkMKG8Hs9Q2elCXRgOo3oZRl0YDqC5U6i/xbj2Nr+9xBgOyZ6iyLmdWVlaIiYlBR0cHhEIhampqUFxcDFtbW/pLta2trcLvd3NzM+bPnw87OzukpqYSQTUglFRqMl/SdJ3Hnj17sH37dvB4PAQHB+PTTz9FZGSkVteapKjKBDU4OBgcDkcr/ymLxYKDgwMcHBwwduxYhRSYgoIC2NjYoKWlBf7+/nSo+ECmtrYWicF/IC0vmj6mykaj2CBf/zYaVRYa+hoN8W76tNEAvUukGag2mq4xTOMDUBUyQZVKpcjLy4NYLEZYWBiGDBmCIUOG0D2zOzs7UVtbC6FQiDt37sDCwgJtbW1oaWnBtGnT8Mwzz2DIkCE4ceJEn/fCHWz09+3fI0eO4LXXXsPevXsRFRWFnTt3YtasWSgpKdEqw9skq3+5XC5iYmLA4XAgkUggkUgY+U9lKTA+Pj6Ijo4Gl8tFa2srrKysUFRUhJycHNy7d4/eQhpoVFVVIS8vDwEBAdi1xgYAFHr6At29wOrSaIAuq4xULFWo9pV/rac0GlXVr7JfOuUKZE02GmWY+FK1TaRRxUC10XSN0X8EVSQSITQ0VGVh0dChQzFq1CgEBwdjxowZ8Pf3x61bt7Bq1Sp4eXmhoKAAixYtQkdHh8HnvWfPHnh5ecHKygpRUVG4du1aj+fv3LkTvr6+sLa2hru7O9asWYP29naDz9NQyHyqpvKHKR9//DFeeuklPPfccwgICMDevXthY2ODb775RqvrTfKZqlQqRUdHR687JMnaGLa3tyM0NBTW1tZobW2FQCAAn89HU1MTHBwc4OLiAg6H0++3hCiKwu3bt1FZWYng4GCw2Wz6teVbG1Veo+xbVfUhqyzIymk0srEVzpF274IEqN4GpqSUVjYa+h49VBYyXaUqjqVfX6qpbwH3F0H9448/0NHRgbCwMEaFRR0dHXjqqadw9+5dJCQk4OzZs8jLy8OiRYvw7bffGmTeR44cwZIlSxRWOceOHVO7yjl06BCef/55fPPNN5g0aRJu3ryJZ599Fk899RQ+/vhjg8zRUMieqU58+CjMh5jGM1WJuAVZ6U/i7t27Cs9ULS0tYWlp2e38zs5O2NjY4Pjx40hMTKSPL126FPX19Thx4oTGMU1SVJOSktDc3Iy4uDg4Ozvr5EFta2tDbm4uLC0tERQUpPKXsb29nRZY2YNsFxcXhRzT/oJUKkVRURHu37+P0NBQlRYhZWGVVf6q+hGgpFKFymBtCpIoilLZXlIbGw3QfaWqciWqpukDEwHT9bnqQHqm2l8E9caNG2hra0N4eDgjQe3s7MTixYtx7949pKen018w7969i7t372LSpEkGmXtUVBQiIiKwe/du+j24u7vj1Vdfxdtvv93t/JUrV6KoqAjp6en0sddffx1Xr17FxYsXDTJHQ9He3o7Ro0eDx+MZeyoK2Nraorm5WeHYhg0bsHHjxm7nVldXw9XVFZcvX1ZI+Hnrrbdw4cIFXL16VeN4JvlMFQC+++47rFmzBpMnT0ZiYiLmzp0LFxcXrVas9fX1yMvLA5fLha+vr1pRtrKygoeHBzw8POiiBz6fT+eYygR22DDT+NalDtmKvKOjAxEREWpX3J+/PRzLtzZ2s9EoFySpenYqK2JS9p4qdEFS80HNMmPRz2uVUfa8AtpX/TKNX+ttv19tIYKqO/oQVJFIhBdeeAEVFRXIyMhQ2LGRxSoaAlnk3DvvvEMfMzMzw8yZM3HlyhWV10yaNAkHDhzAtWvXEBkZidu3b+P06dNYvHixQeZoSKysrFBeXm5yj9UoiuqmG6pWqfrCJEX12WefxdKlS3Hnzh0kJyfj2LFjeOONNxAdHY2EhAQkJCTA1dVVpcDyeDwUFhbCx8cH7u7uWm8bW1paws3NDW5ubnSOKZ/Px61bt2BjY0MLrHJVobHp6OjA9evXYWFhgYiICI1G9s/f7toCeWnTg26vsVgsBRtNN9FUEdWmyrMqj2x12i3rVM82Gk1pNOogiTSmg7yg5ufno7W1lbGgisVivPzyyygqKsL58+fB4XAMNd1u1NbWQiKRdMtednFxQXFxscprnn76adTW1mLKlCldxYBiMZYtW4Z169b1xZT1jpWVVb9+jObs7Axzc3OVge8jRozQ6h4mWagEdH3Ae3l54fXXX8fFixdRUVGBJ598EidPnsT48ePxt7/9DTt37kR5eTm97bh3717k5+cjKCgIHh4eOoufLMc0NDQUM2bMwOjRo9Hc3Ixr167h0qVLKC0tRUNDQ4+Zr31BS0sLrl27BltbW7UFHOr4ar1ibJxYJO5q9KD030zWOlBVdyTgr+xEVbsB6mw0TMWjt886VRUqMSn7H0iCKo+6wixjIG+bKSgoQHNzM8LDwxk1ZpFIJFi5ciVycnLw888/dxM3UyQzMxObN2/GZ599hpycHKSkpODUqVPYtGmTsac2KBk6dCjCw8MVtuOlUinS09O1Dnw3yWeqPUFRFPh8PlJTU5GcnIxffvkFAQEBaG9vh1AoRHp6OsaNG2eQsSUSCWprayEQCCAUCunOTlwuFw4ODn26gq2vr0dubi5cXV3h4+Oj89gvbXqgPsJNk41GLFX5xUIqleoU76YwNkMbTdd8mRcpaRLWvrTSMB2z9+OYxkpVWVAbGxsRHh7OaItOKpVi1apVyMzMxPnz5+Hh4WGo6apFlyKXqVOnIjo6Gtu3b6ePHThwAP/4xz/Q3NxMUnOMwJEjR7B06VJ88cUXiIyMxM6dO3H06FEUFxdr9UXNJLd/e4LFYmHEiBFYvnw5li1bhpKSEsTHx0MgEKCtrQ2LFy9GQkICEhMT4e/vr1ehMzc3h4uLC1xcXCCVSlFXVweBQEC3PJQJrKOjo0F/Gfh8PgoKCjB27NhePx+SrVife1/Y7TV1Aij/mvLz2J7SaFTeh0Eajbr79Kbit6etUGMIal9hioJaWFiIhoYGTJw4kbGgvvnmm0hPTzeaoAKKqxyZqMpWOStXrlR5TWtra7fPClmXt3623hkwLFiwAEKhEO+//z54PB5CQkJw5swZrXc++t1KVZ7bt2/jkUceQXh4OJKSktDR0YEffvgBycnJOHv2LDw9PWmBDQoKMpjQydqnydolUhSl0C5Rn+NWVlairKwMgYGBWhmRmSATVlW+U1U2GmW0bU8IMLfRUGosOkxaEaoei9lqWOP9eimwg6lQSVlQ6+vrER4ezuiZnFQqxbp165CSkoLMzEz4+PgYarpaoWmVs2TJEri6umLLli0AgI0bN+Ljjz/Gl19+iaioKJSVlWH58uUIDw/HkSNHjPpeCLrRr0W1qakJ+/btw8qVK7sJV2NjI06dOoXk5GT6W4ZMYMPCwgwmsBRFob6+nhZYsVgMZ2dnuLi4qO1HrO19S0tLUV1djZCQEDg4OOh34n/y3PtCtc0cetoKln/mqlDcpAcbTdfx7ok12gpYb6p+GT//NfHnqqYoqDIr2MSJExkL6saNG3Hw4EGcP38efn5+hpouI3bv3k23uAsJCcGuXbsQFRUFAJgxYwa8vLyQlJQEoKuw6sMPP8T+/ftRVVUFDoeD+Ph4fPjhhwb7HScYln4tqtrS0tKCH3/8EcnJyTh9+jQcHBwwd+5cJCYmIjIy0mBN9SmKQmNjI+2F7ejooAVW1o9YG2TVkI2NjQgNDe0Ti8+SdxS9ZupEU121LgCtbDQypJSalagaEWDSKYX0+5WNYXqCWlxcjLq6OsaCSlEUNm/ejK+++grnz5/H+PHjDTVdAoERg0JU5Wlra8PZs2eRnJyMkydPwtraGvHx8UhMTMSkSZMMlq1IURSam5tpgW1rawObzaa7OamzDYhEIuTl5UEikSA0NLRPY+qWvMPrweJCqX1NXRqN8t9lMLHRqLoH0y3gwSiof41lXGGVF9SSkhIIhUJMnDiRUbMViqKwY8cOfPLJJ8jIyEBwcLChpksgMGbQiao8nZ2d+Pnnn5GcnIzvv/8eZmZmiIuLw+OPP46pU6caNGuxpaWFFtjm5mY4OjrSAisr0pBlyVpbW2PChAlGi6lb9FZVt2OyXsHKDR96WkGqeh7bW0EFmBUqDWZBVRy378VVXlBv3rwJgUCgk6Du2rUL27dvx9mzZzFx4kRDTZdA0IlBLaryiEQiXLhwAcePH0daWhpEIhHi4uKQkJCAhx56yKAdONra2sDn8yEQCOj+mfb29qipqQGHw4Gfn5/RS+tlwqoqjQbovY1GcTWrfSWuugb66tDnc9T+aKPpGsu4glpaWgo+n4/w8HDY2NhofQ+KorB3715s2rQJZ86cQXR0tOaLCIQ+hoiqCiQSCS5evEgLbFNTE+bMmYPExEQ8/PDDBu0L3N7ejvLyclRVVYGiKIV+xEw+gAzFwtfvqjwuEUtUioy6GCh1lb+qmuXrS1C739e0bDQD9bmqvKCWlZWhpqYGEydOZCyo33zzDd59912cOnUKU6dONdR0CYReQURVA1KpFL/99huOHz+O1NRU1NbW4rHHHkNCQgJmzZql96KhmpoaFBYWwt/fH87OznQV8f3792Fra0t7YVU1zO8r5IVVokoEqe6VuvL/Vh1Arr2NRn4MefpKUDXNQxcGqqCeOhACCwsLUBSFW7duoaqqChMnTmT0e0NRFPbv348333wTP/zwA2bMmGG4CRMIvYSIKgOkUimys7Npga2qqsIjjzyChIQEzJ49WyFaiCkURaGiogLl5eUIDg6Gk5OTwuuyfsQCgQB1dXWwtrYGl8uFi4uL0foRL3z9rkpRNaSNBtBOyHTdAjaGsA5UQf2/NRI0NzfDwcEBZmZmaGxsREREBGNBPXz4MFatWoWUlBQ8+uijBpwxgdB7iKjqiCznUSawt27dwsMPP4yEhATExsYyalsosxYIBAKEhYXBzs6ux/PFYjHdLrG2thZDhw6lBXb48OF9LrBPrqqg/65JUOVRZ6PpOl/FSrSXNhp191WHNmJHBFU1si3ftrY22ocKAMOGDQOXywWHw4GdnZ3Gn9Xk5GQsW7YMR48eRWxsrMHnTSD0FiKqekBmYD9+/DhSUlJQWFiIGTNmIDExEXFxcXByclL74SGRSHDjxg20trbSQepMkEgkdLtEoVAIc3NzhXaJfSmwT6y8pfK4vhNpNAlZbwV1oDXQ72tRlQkq0NX1rLKykm49WFtbC6FQiNraWlhYWIDD4YDD4ahs7fn999/jhRdewMGDBxV66RIIpgwRVT0jK8aQCWxubi6mTJmChISEbpmwTU1NKCoqAovFQkhISK8tPFKpFPfv36efw7JYLHA4HLi4uBi8H7EMZWFlIqj6aKDfGxuNpvE0ja3P+xsKQwusvKCWl5fjzp07CA8P77b7IpFI6NaeQqEQUqkUzs7OKCkpwcyZM3H58mUsXboUSUlJ+Pvf/27QORMI+oSIqgGRPSdNTk5GSkoKfv/9d0RHR2Pu3Lnw9/fH8uXLsW7dOjzzzDN696BKpVK6XSKfz4dUKqUFls1mG9zz+sTKWxqrfjXZaJTPoY8xbKBvSitUXcbTz1iGX63KC2pFRQUqKipUCqoyFEWhoaEBlZWVeOqpp1BVVQWpVIrnnnsO//rXv7TOsewte/bsodsLBgcH49NPP0VkZKTa8+vr6/Huu+8iJSUF9+/fh6enJ3bu3Ik5c+b0yXwJpgkR1T6Coijcu3cPKSkp+Pbbb3H9+nU4Oztj1apVmDdvHjw9PQ22VSv70JJ5YUUikUK7REMK7PxXyui/M7HRAPoRVMX79b5ASd9WmoES8yYvqHfu3MHt27cRHh7OuHjvwoULmDdvHuLj41FVVYWrV68iMjIS//3vfxEQEKDvadMcOXIES5Yswd69exEVFYWdO3fi2LFjKCkpURlc0dnZicmTJ4PL5WLdunVwdXXFnTt34ODgQDo8DXKIqPYxJ0+exMKFC/HGG2+Aw+HQmbATJkxAQkICEhISepWPqgmKotDU1EQLbHt7O5ydncHlcuHs7GyQLlJtbW145vUqjVYa+Q9+fQuq8v3VzUGr+/SjlWpfr1ArKytx69YthIWFwd7entF9Ll26hPnz52PHjh148cUXwWKxwOfz8cMPP2D+/PlwdHTU99RpoqKiEBERgd27dwPo2ulxd3fHq6++irfffrvb+Xv37sX27dtRXFxs0M5rhP4HEdU+pLW1FYGBgdi2bRv9nIiiKNTW1uLEiRNITk5GRkYGfH19aYHVdyasPBRFoaWlhRbYlpYWODk50dWZ+ugz3NzcjJycHHC5XPj6+mLeK6V/ja8uY7UH76s6NAmrviPe6Ot1FNiBuEK9e/cuysrKdBLUq1evIjExER9++CFWrFjRpwV2uoSLz5kzB2w2GzY2Njhx4gQ4HA6efvpprF271mjtRAmmARHVPqa9vV1tGocsNu77779HcnIyzp07By8vLzqyLjAw0KDFRrJ+xAKBAE1NTXB0dKQriXVp09jQ0IDr16/D3d0d3t7eCh+UiS+XdDtfXSA5U+FSFtjB2O+3r1eo9+7dQ2lpKUJDQxlHlmVnZ2Pu3Ll4//33sXr16j63hFVXV8PV1RWXL19GTEwMffytt97ChQsXcPXq1W7X+Pn5oaKiAosWLcIrr7yCsrIyvPLKK/jnP/+JDRs29OX0CSYGEVUTprGxESdPnqQzYUeOHIm5c+fi8ccfR2hoqEEFtq2tjRbYhoYG2Nvb0wKrje2nrq4OeXl58PHxgYeHh9rzZOLa00qyN7mp+valAqbvTTWGoN68eRNhYWGMBTUvLw+xsbFYu3Yt3nrrLaM0MdFFVMeNG0e3FJWtTD/++GNs374dNTU1fTZ3gulhmJwzgl4YPnw4nn76aTz99NNobm6mM2FjY2PBZrMRHx+Pxx9/HBEREXrfcrK2toanpyc8PT3R0dFBC2xpaSns7OxogVXVHYfP5yM/Px8BAQEYOXJkj+OkfeELAEh4qajbazLxYbH++vLApD3hYFyhAgBL7suWIQRWXlCrqqpw8+ZNnVaoBQUFiI+Px+rVq40mqADoYj0+n69wnM/nq608HjlyJCwsLBR+7/z9/cHj8dDZ2dmnEY0E08K40ScErbG1tcXf//53HD58GDweDzt37kR9fT3mz58Pf39/vP766/j1118hFqsOBu8NlpaWcHd3R3h4OKZNmwY3NzfU19fjypUruHLlCm7duoXm5ma6wrmgoAATJkzQKKjynPjKHye+8qf/rW2hUn8R1IGCvKBWV1ejpKQEISEhjIuIiouLERcXh2XLlmH9+vVGE1QAGDp0KMLDw5Genk4fk0qlSE9PV1i5yjN58mSUlZUpRB/evHkTI0eOJII6yCHbv/2c9vZ2pKenIyUlBSdOnIC5uTm9gp0yZYpBKxNFIpFCu0Rzc3OIxWL4+fnB1dW11x+Uc18opP+uSwN9TcJqTEHtj89V5QW1pqYGRUVFCAkJAZvNZnSf0tJSzJ49G4sWLcK2bduMHmsIdFlqli5dii+++AKRkZHYuXMnjh49iuLiYri4uGDJkiVwdXXFli1bAHQVZY0fPx5Lly7Fq6++itLSUjz//PP45z//iXfffdfI74ZgTIioDiBEIhEyMzORnJyMtLQ0iMViOhN2xowZBsuEpSgKJSUlqKmpgb29Perr62FhYUH3I7a3t++VwMY/n9/tWF8m0hBBVRRUHo+HwsJClcEPmigvL8djjz2GefPm4T//+Y9JCKqM3bt3080fQkJCsGvXLkRFRQEAZsyYAS8vLyQlJdHnX7lyBWvWrEFubi5cXV3xwgsvkOpfgvFFlWkXk2PHjmH9+vWoqKjA2LFjsW3bNtLBRAVisVghE7a5uRmxsbFISEjQayasVCpFUVERHjx4gLCwMNjY2EAikeD+/fvg8/nd+hHLEkt0QSauxoh4o6818SKlrjEMJ6h8Pp/e3nd2dmZ0n8rKSsyaNQtz5szBnj17TEpQCQR9YVRRZdrF5PLly5g2bRq2bNmCuLg4HDp0CNu2bUNOTg4CAwON8A76BxKJhM6ETUtLQ11dHWbNmoXExEQ8+uijOmfCysIA2traEBoaqtIqJJVK8eDBA1pgKYqiBZbNZuv8wRr77A2N5+hz+5e+xsQLlfpCUIOCgsDhcBjdp7q6GrNmzcJDDz2EL774gqzmCAMWo4oq0y4mCxYsQEtLC06ePEkfi46ORkhICPbu3dtn8+7PSKVSZGVl0ZF11dXVeOSRR5CYmIjZs2dr7NMqQywWIzc3F1KpFKGhoVo9u6Uoim6iLhAIIJFIwOFwwOVy4eTkpNMHrTpxNbVCpf4uqAKBADdu3MCECRMYCyqPx8Ps2bMRFRWFffv2EUElDGiMJqq6dDHx8PDAa6+9htWrV9PHNmzYgLS0NOTl5fXBrAcWUqkUeXl5tMCWl5crZMKqexba2dmJnJwcDB06FMHBwTp9SFIUhcbGRrqbU2dnp0K7xCFDmLm95MXV1Bro93UqjT7EVV5QhUIh/vjjDwQFBancQeoJoVCIOXPmICgoCAcOHGD8/5VA6G8Y7Se8trYWEokELi4uCsddXFxQXFys8hoej6fyfB6PZ7B5DmTMzMwQGhqK0NBQ/Otf/0JhYSGOHz+O3bt3Y8WKFXjooYeQmJiI2NhYOhO2tLQU5eXlGDlyJMaPH6/z9i2LxYK9vT3s7e0xduxYNDc3g8/n4/bt2ygoKFBol6jNKvhUUhD99zlLNH/BGqiCCnT5VHsjrMqCeuPGDQQGBjIW1Lq6OsTHx8PPzw/79+8ngkoYFJBKAQKALpEbP348NmzYgNzcXOTn52P69On4+uuvMWbMGMTHx2P9+vX429/+hkuXLum1ZSKLxYKdnR18fHwwadIkREVFYfjw4aisrMSFCxeQk5ODe/fuobOzU6v7nf4uGKe/U58U0peJNCwzFlhmfevB1Jeg1tbW4saNGxg/fny3L7OaqK+vR0JCAjw9PfG///2PNJ0nDBqMJqq6dDEZMWIEo/MJusFisTBu3DisW7cOv//+O0pKSjB+/Hjs2rULDx48wG+//YbPP/8cVVVVMMTTA1tbW3h7eyM6OhqTJk0Cm81GdXU1fvnlF2RlZaGyshLt7e0a7yMTV3mBNVbEW18Ja28E9aN3LcHj8SAWi1FXV4c//vgDAQEBjAW1sbERiYmJ4HK5OHbsGGmGQBhUGE1UdeliEhMTo3A+AJw7d07t+YTew2KxcPv2bXz77bfYvn07KioqMH/+fHz//fcICAjAww8/jE8++QR37twxiMDa2NjAy8sLkZGRmDJlCrhcLvh8Pi5evIhr166hoqICbW1tGu8jE9cfD4RoNS6LZabQHrG3mHqh0vGvfGFjY4Pbt28jMzMTOTk5GDFiBOPGDs3NzZg3bx7s7OyQmpqqNjyCQBioGN1Sw6SLyeXLlzF9+nRs3boVsbGxOHz4MDZv3kwsNQZm5cqViIqKwuLFi+ljFEWhpqYGqampSElJoTNhExMTkZCQgDFjxhi09VxHRweEQiH4fD4ePHgAW1tbuLi4qO1HrIrZz+RqdV5/ea6qq6jKb/nev38f169fh7OzM9rb29HU1AQHBwf6+XZP/uaWlhbMnz8fLBYLp06dgq2trU7zIRD6M0Zv/sC0i8mxY8fw3nvv0c0f/v3vf5PmD0ZGlgkrE9iMjAz4+fnRAuvn52dQgRWJRLTA1tXVwcbGhhZYW1tbrcZWJ7D9QVD19Qz1wYMHuH79Ovz8/DBq1CgAXW0whUIhBAIB/eVFPkxB9t+2ra0NTz75JNra2nDmzBkMHz68d2+KQOinGF1UCQMLmRdVPhPW29ubzoTtTcWwNojFYtTW1oLP56O2thaWlpa0wA4fPlyjwPJ4PDz3Bu/P92L6gto1Tu9XqPX19cjJyYGvry9cXV1Vnt/Z2Un3eq6rq0NjYyMyMjIQHx+Pzz//HA8ePMDZs2cZp9X0BqYd2WQcPnwYCxcuREJCAtLS0gw/UcKggYgqwaA0NDTQmbA//fQTRo0aRWfChoSEGFRgJRIJ6urqaIEdMmSIQrtEZYGtrq5GcXEx3YLvsUU5Oo9tDCtN17jaCayyoF6/fh1jx46Fm5ubVtdLJBJkZWVh69atSE9Ph5mZGZYsWYKFCxdi2rRpfVLty7Qjm4yKigpMmTIF3t7eYLPZRFQJeoWIKqHPaG5uxunTp5GcnIwff/wRTk5OCpmwhhRYqVSKuro6CAQCCIVCsFgsWmAdHR1RXV2Nmzdvqk1dYSKw/UlQGxoakJOTAx8fH7i7uzMaTyQS4bnnnsPNmzexYcMGZGRkIC0tDZ2dnbh165bBV6xMO7IBXV8Gpk2bhueffx6//vor6uvriagS9AoRVYJRaG1txU8//YTk5GScOnUKw4YNw9y5c5GYmIiYmBiDtrKT9SOWtUsUi8WgKApjxoyBp6enRnE3RYHtjaCOGTMGHh4ejMYTi8V46aWXcOPGDZw/f5623ci6dIWGhjK6H1N06cgGdHVg++OPP5Camopnn32WiCpB75AWJwSjYGNjg8cffxyPP/442tvb8fPPPyMlJQULFy6EhYUFvYKdPHmy3rcSzczM4OTkBCcnJ1haWqKiogIcDgf37t1DRUWFQrtEVeJ+5mAY/ffebBHrC10EtbGxETk5OfD29mYsqBKJBCtWrMD169eRmZmp4GOVdekyNLp0ZLt48SK+/vpr5ObmGnx+hMEL6ajEkD179sDLywtWVlaIiorCtWvX1J6blJQEFoul8If49rpjZWWFuLg4fPPNN+DxePjuu+9gZmaG5557DmPGjMErr7yCs2fPat1RSRsoisKtW7dQWVmJiRMnYsKECZgyZQrCwsJgZWWF0tJSZGZmIi8vDzU1NRCLxSrvc+ZgmILIKoxhQpW/8oLa1NSEnJwcjB49Gp6enozGk0qlWLVqFS5fvoyff/6ZrhI2dZqamrB48WJ89dVXjCPrCAQmkO1fBjAtjEhKSsKqVatQUlJCH2OxWIw71AxWZJmwx44dQ1paGlpbWxEbG4u5c+di5syZOn9BoSgKZWVlqK6uRnh4uEo/JUVRaG5uhkAgAJ/PR2trK5ycnODi4qKxH/Fji3JMWlCzs7Ph6emJ0aNHMxpPKpXijTfewJkzZ3D+/HnG1+sTptu/ubm5CA0NVdh5kP7538/MzAwlJSUYM2ZMn8ydMLAhosoApoURSUlJWL16Nerr6/t4pgMPiUSCK1eu0JmwDx48UMiEtbGx0eo+FEWhpKQEAoEA4eHhWjeKaGlpoQW2ubkZjo6OtMBaWlqqvW7Wwmyt7s8UXQS1ubkZWVlZOgvqO++8g9TUVGRmZsLHx4fR9YYgKioKkZGR+PTTTwF0zdHDwwMrV67s9vvY3t6OsrIyhWPvvfcempqa8Mknn2DcuHGknSJBLxBR1RJdCiOSkpLw4osvwtXVFVKpFGFhYdi8eTPGjx/fhzMfeEilUvz+++90ZB2Px6MzYR977DG1mbAURaGoqAh1dXUIDw/XWoiVaWtrowW2sbER9vb2tBe2p9WzvgVWk7AqC2p2djbc3d3h7e3NaBypVIqNGzfi4MGDyMzMhK+vr07z1TdMO7IpQwqVCIaAFCppiS6FEb6+vvjmm28wYcIENDQ04KOPPsKkSZNQUFCgtR+Q0B0zMzNERUUhKioK27ZtQ25uLo4fP46tW7di2bJlmDlzJhISEjBnzhw6E1YsFuPSpUswNzfHxIkTe2y3pwlra2t4enrC09OT7jjE5/Nx8+ZN2NnZ0QKrLNo//S+c/rs+BJYlV6WsLLDygtrS0oLs7Gy4ubkxFlSKorBlyxZ89913OH/+vMkIKgAsWLAAQqEQ77//Pt2R7cyZM/TvaGVlpUFtWgSCKshKVUuqq6vh6uqKy5cvKzTwf+utt3DhwgVcvXpV4z1EIhH8/f2xcOFCbNq0yZDTHZRQFIWCggIcP34cKSkpKCkpwUMPPYTY2FikpKSgra0Np0+fNlixWGdnJ23TuX//PoYNG6bQLlEdvRFYVatVZUHNysqCq6sr437MFEXho48+wq5du5CRkYHgYPVxegQCoQuyUtUSXaLqlLGwsEBoaGi3ZzsE/cBisRAYGIjAwEBs2LABN2/exJEjR7B+/Xo0NDRg0qRJOHDgAOLj48HlcvXej3jo0KFwc3ODm5sb3Y9YIBCgvLwc1tbWdLMJOzs7hbH1uYKVF9TW1lZkZ2dj1KhROgnqrl278Mknn+Ds2bNEUAkELSF7I1qiS1SdMhKJBDdu3MDIkSMNNU3Cn7BYLHh6euLatWsYO3YssrKykJCQgEOHDmHcuHGYPXu2QTNhLSwsMGrUKISEhGD69Onw9vZGa2srsrKycOnSJdy8eRP19fXdxv7pf+EKIqsOSirtcctXNtaIESPg4+PDWFA///xzbNu2DadPn8bEiRO1vpZAGOyQ7V8GMC2M+L//+z9ER0fDx8cH9fX12L59O9LS0pCdnY2AgAAjv5uBz9GjR7Fz5078+OOPsLe3B9AlGHfv3kVycjJSU1Nx5coVTJw4EQkJCUhISICHh4dBE3Vk/Yhl7RLNzc0V2iWqGlt59appy7etrQ1ZWVngcrkYN24cY0H9+uuvsX79epw6dQpTpkxh8O4IBAIRVYYwiapbs2YNUlJSwOPx4OjoiPDwcPzrX//qk44zhC46OzvVWiUoikJ1dTUdWffrr78iODiYjqzz9vY2qMBKpVLcv3+ffg7LYrHA4XDg4uICR0dHlUU2sxZm97hClQkqh8OBr68vY0Hdv38/3nzzTfzwww+YMWOGzu+NQBisEFElENAlKEKhkBbY8+fPw9/fnxZYpgLFFKlUivr6elpgJRIJLbBsNlurXsjt7e3IysqCk5MT4wxbiqJw+PBhrFq1CmlpaZg5c2Zv3g6BMGghokogKCHLhD1x4gSSk5Px888/Y8yYMXRkXUBAgEGtGhRFoaGhgfbCikQiODs7w8XFRW0/Ypmgstls+Pv7M/4CcPz4cbzyyis4evQo5syZo6+3QiAMOoioEggaaGhowA8//EBnwrq5udGh68HBwQYX2KamJlpg29vb6XaJzs7OsLCwQEdHB7KysuDo6KiToH7//fd44YUXcOjQISQkJBjonRAIgwMiqgQCA5qamhQyYZ2dnekV7MSJEw0usC0tLeDz+RAIBGhpaYGDgwNaWlrg6OiIoKAgxoJ6+vRpLF26FN9++y2eeOIJA82cQBg8EFElEHSktbUVZ86cQUpKCk6ePAk7Ozs6EzY6OtqgmbBA1wr6+vXrALrCBxwdHelK4p76Ecs4d+4cnn76afz3v//FwoULDTpXAmGwQHyqA5xffvkF8fHxGDVqFFgsllZ9TjMzMxEWFgZLS0v4+PjQ1cwERWxsbDBv3jwcOHAAPB4Pn332GVpbW7FgwQKMGzcOq1evRmZmJkQikd7H7uzsREFBAZycnDB9+nRMnjwZzs7O4PF4+PXXX/H777/jzp07aGtrU3l9ZmYmFi1ahM8++wxPPfWU3udHIAxWiKgOcFpaWhAcHIw9e/ZodX55eTliY2Px0EMPITc3F6tXr8aLL76In376ycAz7d9YWVkhPj4e+/btA4/Ho7+IPPvss/Dx8cErr7yCc+fO6SUTtrOzE9nZ2bC1tcX48ePBYrHofsQRERGYOnUqRowYgdraWly6dAlXr17F7du3kZ+fD6ArrHvBggX4z3/+gyVLlhi0qlkZJnnEX331FaZOnQpHR0c4Ojpi5syZPZ5PIJgCZPt3EMFisZCamqqQsqPM2rVrcerUKfoDGACeeuop1NfX48yZM30wy4GFWCzGr7/+SmfCtrW1ITY2FgkJCXj44YcZ9yEWiUTIzs6GjY0NAgMDNT7D7ezshFAoRHZ2NhYvXgwul4va2lq88cYb2LRpU582nGeaR7xo0SJMnjwZkyZNgpWVFbZt24bU1FQUFBTA1dW1z+ZNIDCBiOogQhtRnTZtGsLCwrBz50762L59+7B69Wo0NDQYfpIDGIlEgsuXL9OZsPX19XjssceQmJiIRx55RGMUnUxQra2tERQUxFgQMzIy8Pe//x1jx45FWVkZ3N3dMX/+fLz66qt90jqTaR6xMhKJBI6Ojti9ezeWLFli6OkSCDpBtn8JCvB4PJXxdo2NjWqfzxG0w9zcHFOnTsUnn3yC8vJy/PTTT3B3d8f69evh5eWFZ555BsePH0dTU1O3a0UiEXJycmBlZaWToObl5WHJkiX44IMPkJeXB6FQiM2bN6OiogISiURfb1Etsi1r+aYSZmZmmDlzJq5cuaLVPVpbWyESicBmsw01TQKh1xBRJRCMgJmZGaKjo/HRRx/h5s2buHDhAvz8/PDhhx/Cy8sLCxYswP/+9z80NDSgtrYWCxYsQFtbGyZMmMBYUPPz8xEfH481a9bgzTffBIvFwrBhwzB//nwcOnSoT7J9e8oj5vF4Wt1j7dq1GDVqFOn2RDBpiKgSFBgxYoTKeLvhw4f3KtiboB4zMzOEh4dj8+bNKC4uxrVr1+gteE9PTwQGBqK0tJRx2gwAFBUVIT4+HsuXL8d7773Xp0VJ+mTr1q04fPgwUlNTDZaHSyDoAyKqBAViYmIU4u2ALj+jtvF2hN7BYrEQFBSEDz74AJcuXUJgYCAdch4YGIi5c+fi66+/hkAg0BhZV1pairi4OCxduhQffPCBUQW1N3nEH330EbZu3YqzZ89iwoQJhpwmgdBriKgOcJqbm5Gbm4vc3FwAXZaZ3NxcVFZWAgDeeecdhaKPZcuW4fbt23jrrbdQXFyMzz77DEePHsWaNWuMMf1BS3t7O+Li4uDs7IwbN24gNzcXhYWFePTRR3HgwAGMHTsWs2fPxt69e1FdXd1NYMvLyxEXF4cFCxZg69atfVrlqwpd84j//e9/Y9OmTThz5gzJdSX0DyjCgOb8+fMUgG5/li5dSlEURS1dupSaPn16t2tCQkKooUOHUt7e3tS+ffv6fN6DHalUSn3xxRdUS0uLytcqKiqoHTt2UFOmTKGGDBlCxcTEUFu3bqWKioqooqIiysPDg1q+fDklkUiMMHvVHD58mLK0tKSSkpKowsJC6h//+Afl4OBA8Xg8iqIoavHixdTbb79Nn79161Zq6NCh1PHjx6mamhr6T1NTk7HeAoGgEWKpIRD6MdSfmbApKSl0JuyQIUMwf/587N+/3+grVGWY5BF7eXnhzp073e6xYcMGbNy4sQ9nTSBoDxFVAmGAQFEUeDweNm7ciD179mDIkCHGnhKBMOggokogEAgEgp4wrb0hAoFAIBD6MURUCQQCgUDQE0RUCQQCgUDQE0RUCSYJ0xzYzMxMsFisbn+0bYFHIBAI+oCIKsEkYZoDK6OkpAQ1NTX0H1WRYgQCgWAoSM09wSSZPXs2Zs+ezfg6LpcLBwcH/U+IQCAQtICsVAkDipCQEIwcORKPPPIILl26ZOzpEAiEQQYRVcKAYOTIkdi7dy+Sk5ORnJwMd3d3zJgxAzk5OcaeGoFAGESQ5g8Ek4fFYiE1NRWJiYmMrps+fTo8PDywf/9+w0yMQCAQlCArVcKAJTIyEmVlZcaeBoFAGEQQUSUMWHJzczFy5EhjT6PfsmfPHnh5ecHKygpRUVG4du1aj+cfO3YMfn5+sLKyQlBQEE6fPt1HMyUQTAciqgSThGkO7M6dO3HixAmUlZUhPz8fq1evRkZGBlasWGGM6fd7jhw5gtdeew0bNmxATk4OgoODMWvWLAgEApXnX758GQsXLsQLL7yA69evIzExEYmJicjPz+/jmRMIRsZYmXMEQk8wzYHdtm0bNWbMGMrKyopis9nUjBkzqIyMDONMfgAQGRlJrVixgv63RCKhRo0aRW3ZskXl+U8++SQVGxurcCwqKop6+eWXDTpPAsHUIIVKBAJBgc7OTtjY2OD48eMKxWFLly5FfX09Tpw40e0aDw8PvPbaa1i9ejV9bMOGDUhLS0NeXl4fzJpAMA3I9i+BQFCgtrYWEokELi4uCsddXFzUtn3k8XiMzicQBipEVAkEAoFA0BNEVAkEggLOzs4wNzcHn89XOM7n8zFixAiV14wYMYLR+QTCQIWIKoFAUGDo0KEIDw9Heno6fUwqlSI9PR0xMTEqr4mJiVE4HwDOnTun9nwCYaBCRJVA0DNbtmxBREQE7OzswOVykZiYiJKSEo3XmZLP87XXXsNXX32Fb7/9FkVFRVi+fDlaWlrw3HPPAQCWLFmCd955hz5/1apVOHPmDHbs2IHi4mJs3LgRWVlZWLlypbHeAoFgFIioEgh65sKFC1ixYgV+++03nDt3DiKRCI8++ihaWlrUXmNqPs8FCxbgo48+wvvvv4+QkBDk5ubizJkzdDFSZWUlampq6PMnTZqEQ4cO4csvv0RwcDCOHz+OtLQ0BAYGGmX+BIKxIJYaAsHACIVCcLlcXLhwAdOmTVN5zoIFC9DS0oKTJ0/Sx6KjoxESEoK9e/f21VQJBEIvIStVAsHANDQ0AADYbLbac65cuYKZM2cqHJs1axauXLli0LkRCAT9QkSVQDAgUqkUq1evxuTJk3vcCiU+TwJhYDDE2BMgEAYyK1asQH5+Pi5evGjsqRAIhD6AiCqBYCBWrlyJkydP4pdffoGbm1uP5xKfJ4EwMCDbvwSCnqEoCitXrkRqaioyMjIwevRojdcQnyeBMDAgK1UCQc+sWLEChw4dwokTJ2BnZ0c/F7W3t4e1tTWALp+nq6srtmzZAqDL5zl9+nTs2LEDsbGxOHz4MLKysvDll18a7X0QCATmEEsNgaBnWCyWyuP79u3Ds88+CwCYMWMGvLy8kJSURL9+7NgxvPfee6ioqMDYsWPx73//G3PmzOmDGRMIBH1BRJVAIBAIBD1BnqkSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAniKgSCAQCgaAn/h9/LqMZSTONkgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "torch.save(net.state_dict(), \"model_uxt.pt\")"
      ],
      "metadata": {
        "id": "bVlAnLuJ11mW"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}